{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.6048223  |\n",
      "| ent_coef_loss           | -0.7811502 |\n",
      "| entropy                 | 1.1373899  |\n",
      "| episodes                | 10         |\n",
      "| fps                     | 276        |\n",
      "| mean 100 episode reward | -1.46e+03  |\n",
      "| n_updates               | 1700       |\n",
      "| policy_loss             | 48.055183  |\n",
      "| qf1_loss                | 0.23832673 |\n",
      "| qf2_loss                | 0.29020354 |\n",
      "| time_elapsed            | 6          |\n",
      "| total timesteps         | 1800       |\n",
      "| value_loss              | 0.17269486 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.3804809   |\n",
      "| ent_coef_loss           | -0.74201274 |\n",
      "| entropy                 | 0.91518843  |\n",
      "| episodes                | 20          |\n",
      "| fps                     | 272         |\n",
      "| mean 100 episode reward | -1.3e+03    |\n",
      "| n_updates               | 3700        |\n",
      "| policy_loss             | 92.182434   |\n",
      "| qf1_loss                | 95.072556   |\n",
      "| qf2_loss                | 96.95395    |\n",
      "| time_elapsed            | 13          |\n",
      "| total timesteps         | 3800        |\n",
      "| value_loss              | 1.2863626   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.26942784  |\n",
      "| ent_coef_loss           | -0.49914938 |\n",
      "| entropy                 | 0.88054085  |\n",
      "| episodes                | 30          |\n",
      "| fps                     | 277         |\n",
      "| mean 100 episode reward | -1.28e+03   |\n",
      "| n_updates               | 5700        |\n",
      "| policy_loss             | 113.09793   |\n",
      "| qf1_loss                | 4.9038706   |\n",
      "| qf2_loss                | 5.121813    |\n",
      "| time_elapsed            | 20          |\n",
      "| total timesteps         | 5800        |\n",
      "| value_loss              | 1.8451234   |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0003       |\n",
      "| ent_coef                | 0.23033172   |\n",
      "| ent_coef_loss           | -0.036992252 |\n",
      "| entropy                 | 0.79568404   |\n",
      "| episodes                | 40           |\n",
      "| fps                     | 271          |\n",
      "| mean 100 episode reward | -1.18e+03    |\n",
      "| n_updates               | 7700         |\n",
      "| policy_loss             | 144.90085    |\n",
      "| qf1_loss                | 241.26299    |\n",
      "| qf2_loss                | 242.96045    |\n",
      "| time_elapsed            | 28           |\n",
      "| total timesteps         | 7800         |\n",
      "| value_loss              | 1.517252     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.1737346   |\n",
      "| ent_coef_loss           | -0.18186215 |\n",
      "| entropy                 | 0.7761593   |\n",
      "| episodes                | 50          |\n",
      "| fps                     | 259         |\n",
      "| mean 100 episode reward | -1.13e+03   |\n",
      "| n_updates               | 9700        |\n",
      "| policy_loss             | 171.2605    |\n",
      "| qf1_loss                | 8.655359    |\n",
      "| qf2_loss                | 6.7541804   |\n",
      "| time_elapsed            | 37          |\n",
      "| total timesteps         | 9800        |\n",
      "| value_loss              | 0.839789    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.1748444  |\n",
      "| ent_coef_loss           | 0.33751562 |\n",
      "| entropy                 | 0.4403626  |\n",
      "| episodes                | 60         |\n",
      "| fps                     | 258        |\n",
      "| mean 100 episode reward | -960       |\n",
      "| n_updates               | 11700      |\n",
      "| policy_loss             | 176.59425  |\n",
      "| qf1_loss                | 6.4961095  |\n",
      "| qf2_loss                | 7.173324   |\n",
      "| time_elapsed            | 45         |\n",
      "| total timesteps         | 11800      |\n",
      "| value_loss              | 0.91797656 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.20650043 |\n",
      "| ent_coef_loss           | 0.2710486  |\n",
      "| entropy                 | 0.4237244  |\n",
      "| episodes                | 70         |\n",
      "| fps                     | 251        |\n",
      "| mean 100 episode reward | -851       |\n",
      "| n_updates               | 13700      |\n",
      "| policy_loss             | 187.51431  |\n",
      "| qf1_loss                | 7.4816713  |\n",
      "| qf2_loss                | 4.0846715  |\n",
      "| time_elapsed            | 54         |\n",
      "| total timesteps         | 13800      |\n",
      "| value_loss              | 0.82913756 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.20352629 |\n",
      "| ent_coef_loss           | -0.177307  |\n",
      "| entropy                 | 0.2962598  |\n",
      "| episodes                | 80         |\n",
      "| fps                     | 233        |\n",
      "| mean 100 episode reward | -774       |\n",
      "| n_updates               | 15700      |\n",
      "| policy_loss             | 178.47906  |\n",
      "| qf1_loss                | 66.16352   |\n",
      "| qf2_loss                | 63.90026   |\n",
      "| time_elapsed            | 67         |\n",
      "| total timesteps         | 15800      |\n",
      "| value_loss              | 0.65785146 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.21926345  |\n",
      "| ent_coef_loss           | -0.19356811 |\n",
      "| entropy                 | 0.19893461  |\n",
      "| episodes                | 90          |\n",
      "| fps                     | 230         |\n",
      "| mean 100 episode reward | -708        |\n",
      "| n_updates               | 17700       |\n",
      "| policy_loss             | 165.49356   |\n",
      "| qf1_loss                | 436.80212   |\n",
      "| qf2_loss                | 414.1297    |\n",
      "| time_elapsed            | 77          |\n",
      "| total timesteps         | 17800       |\n",
      "| value_loss              | 0.7338748   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.22398956  |\n",
      "| ent_coef_loss           | -0.29202247 |\n",
      "| entropy                 | 0.14649306  |\n",
      "| episodes                | 100         |\n",
      "| fps                     | 234         |\n",
      "| mean 100 episode reward | -652        |\n",
      "| n_updates               | 19700       |\n",
      "| policy_loss             | 152.31631   |\n",
      "| qf1_loss                | 8.033593    |\n",
      "| qf2_loss                | 5.2837157   |\n",
      "| time_elapsed            | 84          |\n",
      "| total timesteps         | 19800       |\n",
      "| value_loss              | 1.0670033   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.22373317  |\n",
      "| ent_coef_loss           | -0.12640902 |\n",
      "| entropy                 | 0.1523597   |\n",
      "| episodes                | 110         |\n",
      "| fps                     | 234         |\n",
      "| mean 100 episode reward | -532        |\n",
      "| n_updates               | 21700       |\n",
      "| policy_loss             | 154.28583   |\n",
      "| qf1_loss                | 8.438166    |\n",
      "| qf2_loss                | 4.0691986   |\n",
      "| time_elapsed            | 92          |\n",
      "| total timesteps         | 21800       |\n",
      "| value_loss              | 0.9304639   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.22128844 |\n",
      "| ent_coef_loss           | 0.392865   |\n",
      "| entropy                 | 0.26374286 |\n",
      "| episodes                | 120        |\n",
      "| fps                     | 235        |\n",
      "| mean 100 episode reward | -441       |\n",
      "| n_updates               | 23700      |\n",
      "| policy_loss             | 147.41446  |\n",
      "| qf1_loss                | 8.209032   |\n",
      "| qf2_loss                | 9.209205   |\n",
      "| time_elapsed            | 101        |\n",
      "| total timesteps         | 23800      |\n",
      "| value_loss              | 0.9501337  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.21119197 |\n",
      "| ent_coef_loss           | 0.40553272 |\n",
      "| entropy                 | 0.30969685 |\n",
      "| episodes                | 130        |\n",
      "| fps                     | 233        |\n",
      "| mean 100 episode reward | -337       |\n",
      "| n_updates               | 25700      |\n",
      "| policy_loss             | 159.11804  |\n",
      "| qf1_loss                | 21.949194  |\n",
      "| qf2_loss                | 21.017756  |\n",
      "| time_elapsed            | 110        |\n",
      "| total timesteps         | 25800      |\n",
      "| value_loss              | 1.1866753  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.21202666 |\n",
      "| ent_coef_loss           | -0.6545413 |\n",
      "| entropy                 | 0.25757438 |\n",
      "| episodes                | 140        |\n",
      "| fps                     | 232        |\n",
      "| mean 100 episode reward | -258       |\n",
      "| n_updates               | 27700      |\n",
      "| policy_loss             | 98.98578   |\n",
      "| qf1_loss                | 26.119415  |\n",
      "| qf2_loss                | 28.336887  |\n",
      "| time_elapsed            | 119        |\n",
      "| total timesteps         | 27800      |\n",
      "| value_loss              | 0.9630506  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.19173689  |\n",
      "| ent_coef_loss           | 0.056136638 |\n",
      "| entropy                 | 0.37302554  |\n",
      "| episodes                | 150         |\n",
      "| fps                     | 231         |\n",
      "| mean 100 episode reward | -179        |\n",
      "| n_updates               | 29700       |\n",
      "| policy_loss             | 99.731064   |\n",
      "| qf1_loss                | 6.591015    |\n",
      "| qf2_loss                | 7.26829     |\n",
      "| time_elapsed            | 128         |\n",
      "| total timesteps         | 29800       |\n",
      "| value_loss              | 1.5269341   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.1707891  |\n",
      "| ent_coef_loss           | 0.61301625 |\n",
      "| entropy                 | 0.36744937 |\n",
      "| episodes                | 160        |\n",
      "| fps                     | 230        |\n",
      "| mean 100 episode reward | -183       |\n",
      "| n_updates               | 31700      |\n",
      "| policy_loss             | 111.72402  |\n",
      "| qf1_loss                | 14.151059  |\n",
      "| qf2_loss                | 12.150604  |\n",
      "| time_elapsed            | 138        |\n",
      "| total timesteps         | 31800      |\n",
      "| value_loss              | 1.174063   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.17355639 |\n",
      "| ent_coef_loss           | 0.13322787 |\n",
      "| entropy                 | 0.32190257 |\n",
      "| episodes                | 170        |\n",
      "| fps                     | 226        |\n",
      "| mean 100 episode reward | -178       |\n",
      "| n_updates               | 33700      |\n",
      "| policy_loss             | 104.09689  |\n",
      "| qf1_loss                | 9.372681   |\n",
      "| qf2_loss                | 9.630014   |\n",
      "| time_elapsed            | 149        |\n",
      "| total timesteps         | 33800      |\n",
      "| value_loss              | 4.139916   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.18185961 |\n",
      "| ent_coef_loss           | 0.18544182 |\n",
      "| entropy                 | 0.38915935 |\n",
      "| episodes                | 180        |\n",
      "| fps                     | 225        |\n",
      "| mean 100 episode reward | -165       |\n",
      "| n_updates               | 35700      |\n",
      "| policy_loss             | 100.31747  |\n",
      "| qf1_loss                | 15.899336  |\n",
      "| qf2_loss                | 14.544376  |\n",
      "| time_elapsed            | 158        |\n",
      "| total timesteps         | 35800      |\n",
      "| value_loss              | 1.4784244  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.18455005 |\n",
      "| ent_coef_loss           | 0.32180262 |\n",
      "| entropy                 | 0.32613295 |\n",
      "| episodes                | 190        |\n",
      "| fps                     | 224        |\n",
      "| mean 100 episode reward | -166       |\n",
      "| n_updates               | 37700      |\n",
      "| policy_loss             | 91.95647   |\n",
      "| qf1_loss                | 32.86918   |\n",
      "| qf2_loss                | 22.13259   |\n",
      "| time_elapsed            | 168        |\n",
      "| total timesteps         | 37800      |\n",
      "| value_loss              | 2.9510405  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.20643914 |\n",
      "| ent_coef_loss           | 0.14033723 |\n",
      "| entropy                 | 0.32292408 |\n",
      "| episodes                | 200        |\n",
      "| fps                     | 223        |\n",
      "| mean 100 episode reward | -167       |\n",
      "| n_updates               | 39700      |\n",
      "| policy_loss             | 74.684235  |\n",
      "| qf1_loss                | 9.680204   |\n",
      "| qf2_loss                | 10.41295   |\n",
      "| time_elapsed            | 178        |\n",
      "| total timesteps         | 39800      |\n",
      "| value_loss              | 1.8940499  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.21723041 |\n",
      "| ent_coef_loss           | 0.3056295  |\n",
      "| entropy                 | 0.27288717 |\n",
      "| episodes                | 210        |\n",
      "| fps                     | 222        |\n",
      "| mean 100 episode reward | -159       |\n",
      "| n_updates               | 41700      |\n",
      "| policy_loss             | 46.84075   |\n",
      "| qf1_loss                | 12.319199  |\n",
      "| qf2_loss                | 6.4157295  |\n",
      "| time_elapsed            | 187        |\n",
      "| total timesteps         | 41800      |\n",
      "| value_loss              | 1.5397971  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.21685757  |\n",
      "| ent_coef_loss           | -0.18637216 |\n",
      "| entropy                 | 0.38626426  |\n",
      "| episodes                | 220         |\n",
      "| fps                     | 222         |\n",
      "| mean 100 episode reward | -150        |\n",
      "| n_updates               | 43700       |\n",
      "| policy_loss             | 56.848297   |\n",
      "| qf1_loss                | 9.157225    |\n",
      "| qf2_loss                | 8.754407    |\n",
      "| time_elapsed            | 197         |\n",
      "| total timesteps         | 43800       |\n",
      "| value_loss              | 2.045458    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.19698077 |\n",
      "| ent_coef_loss           | 0.27113765 |\n",
      "| entropy                 | 0.38780296 |\n",
      "| episodes                | 230        |\n",
      "| fps                     | 220        |\n",
      "| mean 100 episode reward | -143       |\n",
      "| n_updates               | 45700      |\n",
      "| policy_loss             | 53.276512  |\n",
      "| qf1_loss                | 19.928211  |\n",
      "| qf2_loss                | 13.610522  |\n",
      "| time_elapsed            | 207        |\n",
      "| total timesteps         | 45800      |\n",
      "| value_loss              | 0.5472033  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0003      |\n",
      "| ent_coef                | 0.13790666  |\n",
      "| ent_coef_loss           | -0.84160495 |\n",
      "| entropy                 | 0.78103125  |\n",
      "| episodes                | 240         |\n",
      "| fps                     | 220         |\n",
      "| mean 100 episode reward | -146        |\n",
      "| n_updates               | 47700       |\n",
      "| policy_loss             | 58.681633   |\n",
      "| qf1_loss                | 5.784893    |\n",
      "| qf2_loss                | 3.8415089   |\n",
      "| time_elapsed            | 216         |\n",
      "| total timesteps         | 47800       |\n",
      "| value_loss              | 1.1372521   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| ent_coef                | 0.12515622 |\n",
      "| ent_coef_loss           | 0.03917873 |\n",
      "| entropy                 | 0.6121666  |\n",
      "| episodes                | 250        |\n",
      "| fps                     | 215        |\n",
      "| mean 100 episode reward | -149       |\n",
      "| n_updates               | 49700      |\n",
      "| policy_loss             | 42.768757  |\n",
      "| qf1_loss                | 4.7343073  |\n",
      "| qf2_loss                | 2.4630222  |\n",
      "| time_elapsed            | 231        |\n",
      "| total timesteps         | 49800      |\n",
      "| value_loss              | 1.3044034  |\n",
      "----------------------------------------\n",
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'ndim'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b3c324ba1070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.6/site-packages/gym/envs/classic_control/pendulum.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mnewthdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthdot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mnewth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewthdot\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mnewthdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewthdot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_speed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_speed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#pylint: disable=E1111\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnewth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewthdot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m     \"\"\"\n\u001b[0;32m-> 2037\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0musing_deprecated_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_clip_dep_is_scalar_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0musing_deprecated_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip_dep_is_scalar_nan\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# guarded to protect circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromnumeric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3019\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stable/lib/python3.6/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines.sac.policies import MlpPolicy\n",
    "from stable_baselines import SAC\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "model = SAC(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=50000, log_interval=10)\n",
    "model.save(\"sac_pendulum\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = SAC.load(\"sac_pendulum\")\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stable Baselines (3.6)",
   "language": "python",
   "name": "stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
