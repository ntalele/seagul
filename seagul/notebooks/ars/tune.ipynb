{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-05 12:26:42,056\tINFO resource_spec.py:205 -- Starting Ray with 3.96 GiB memory available for workers and up to 2.0 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 9.5/15.7 GiB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 1, 'PENDING': 2})\n",
      "PENDING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tPENDING\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tPENDING\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:46,580\tINFO trainer.py:344 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:46,558\tINFO trainer.py:344 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:46,650\tINFO trainer.py:344 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:48,074\tINFO rollout_worker.py:768 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fdf28368c50>}\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:48,074\tINFO rollout_worker.py:769 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fdf28368908>}\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:48,074\tINFO rollout_worker.py:370 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fdf28347080>}\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:48,113\tINFO multi_gpu_optimizer.py:93 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:48,181\tINFO rollout_worker.py:768 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f20c339bc50>}\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:48,182\tINFO rollout_worker.py:769 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f20c339b908>}\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:48,182\tINFO rollout_worker.py:370 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f20c337a080>}\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:48,222\tINFO multi_gpu_optimizer.py:93 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:48,194\tINFO rollout_worker.py:768 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f7b28fdcc50>}\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:48,194\tINFO rollout_worker.py:769 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f7b28fdc908>}\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:48,194\tINFO rollout_worker.py:370 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f7b28fbb080>}\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:48,242\tINFO multi_gpu_optimizer.py:93 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m 2019-11-05 12:26:53,280\tINFO rollout_worker.py:467 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m 2019-11-05 12:26:53,281\tINFO sampler.py:310 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-0.048, max=0.044, mean=0.005)}}\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m 2019-11-05 12:26:53,281\tINFO sampler.py:311 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m 2019-11-05 12:26:53,281\tINFO sampler.py:409 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-0.048, max=0.044, mean=0.005)\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m 2019-11-05 12:26:53,281\tINFO sampler.py:413 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-0.048, max=0.044, mean=0.005)\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m 2019-11-05 12:26:53,282\tINFO sampler.py:528 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-0.048, max=0.044, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m 2019-11-05 12:26:53,282\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m 2019-11-05 12:26:53,324\tINFO sampler.py:555 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                       { 'action_logp': np.ndarray((1,), dtype=float32, min=-0.693, max=-0.693, mean=-0.693),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.5, max=0.5, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.0, max=-0.0, mean=-0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m 2019-11-05 12:26:53,351\tINFO rollout_worker.py:467 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m 2019-11-05 12:26:53,351\tINFO sampler.py:310 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-0.03, max=0.036, mean=-0.005)}}\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m 2019-11-05 12:26:53,351\tINFO sampler.py:311 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m 2019-11-05 12:26:53,351\tINFO sampler.py:409 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-0.03, max=0.036, mean=-0.005)\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m 2019-11-05 12:26:53,352\tINFO sampler.py:413 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-0.03, max=0.036, mean=-0.005)\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m 2019-11-05 12:26:53,352\tINFO sampler.py:528 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-0.03, max=0.036, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m 2019-11-05 12:26:53,352\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m 2019-11-05 12:26:53,384\tINFO sampler.py:555 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                       { 'action_logp': np.ndarray((1,), dtype=float32, min=-0.693, max=-0.693, mean=-0.693),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.5, max=0.5, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.0, max=0.0, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.0, max=-0.0, mean=-0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m 2019-11-05 12:26:53,378\tINFO rollout_worker.py:467 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m 2019-11-05 12:26:53,378\tINFO sampler.py:310 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-0.044, max=0.015, mean=-0.015)}}\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m 2019-11-05 12:26:53,379\tINFO sampler.py:311 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m 2019-11-05 12:26:53,379\tINFO sampler.py:409 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-0.044, max=0.015, mean=-0.015)\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m 2019-11-05 12:26:53,379\tINFO sampler.py:413 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-0.044, max=0.015, mean=-0.015)\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m 2019-11-05 12:26:53,380\tINFO sampler.py:528 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-0.044, max=0.015, mean=-0.015),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m 2019-11-05 12:26:53,380\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m 2019-11-05 12:26:53,337\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m { 'agent0': { 'data': { 'action_logp': np.ndarray((15,), dtype=float32, min=-0.694, max=-0.692, mean=-0.693),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'action_prob': np.ndarray((15,), dtype=float32, min=0.5, max=0.5, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'actions': np.ndarray((15,), dtype=int64, min=0.0, max=1.0, mean=0.8),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'advantages': np.ndarray((15,), dtype=float32, min=0.998, max=13.995, mean=7.638),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'agent_index': np.ndarray((15,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'behaviour_logits': np.ndarray((15, 2), dtype=float32, min=-0.004, max=0.0, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'dones': np.ndarray((15,), dtype=bool, min=0.0, max=1.0, mean=0.067),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'eps_id': np.ndarray((15,), dtype=int64, min=1390712965.0, max=1390712965.0, mean=1390712965.0),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'infos': np.ndarray((15,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'new_obs': np.ndarray((15, 4), dtype=float32, min=-2.809, max=1.757, mean=-0.092),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'obs': np.ndarray((15, 4), dtype=float32, min=-2.462, max=1.561, mean=-0.074),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'prev_actions': np.ndarray((15,), dtype=int64, min=0.0, max=1.0, mean=0.733),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'prev_rewards': np.ndarray((15,), dtype=float32, min=0.0, max=1.0, mean=0.933),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'rewards': np.ndarray((15,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         't': np.ndarray((15,), dtype=int64, min=0.0, max=14.0, mean=7.0),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'unroll_id': np.ndarray((15,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'value_targets': np.ndarray((15,), dtype=float32, min=1.0, max=13.994, mean=7.639),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m                         'vf_preds': np.ndarray((15,), dtype=float32, min=-0.001, max=0.002, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m 2019-11-05 12:26:53,421\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m { 'agent0': { 'data': { 'action_logp': np.ndarray((22,), dtype=float32, min=-0.696, max=-0.691, mean=-0.693),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'action_prob': np.ndarray((22,), dtype=float32, min=0.499, max=0.501, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'actions': np.ndarray((22,), dtype=int64, min=0.0, max=1.0, mean=0.364),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'advantages': np.ndarray((22,), dtype=float32, min=1.008, max=19.837, mean=10.737),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'agent_index': np.ndarray((22,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'behaviour_logits': np.ndarray((22, 2), dtype=float32, min=-0.006, max=0.01, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'dones': np.ndarray((22,), dtype=bool, min=0.0, max=1.0, mean=0.045),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'eps_id': np.ndarray((22,), dtype=int64, min=859969481.0, max=859969481.0, mean=859969481.0),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'infos': np.ndarray((22,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'new_obs': np.ndarray((22, 4), dtype=float32, min=-1.142, max=1.978, mean=0.053),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'obs': np.ndarray((22, 4), dtype=float32, min=-0.946, max=1.633, mean=0.042),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'prev_actions': np.ndarray((22,), dtype=int64, min=0.0, max=1.0, mean=0.364),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'prev_rewards': np.ndarray((22,), dtype=float32, min=0.0, max=1.0, mean=0.955),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'rewards': np.ndarray((22,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         't': np.ndarray((22,), dtype=int64, min=0.0, max=21.0, mean=10.5),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'unroll_id': np.ndarray((22,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'value_targets': np.ndarray((22,), dtype=float32, min=1.0, max=19.837, mean=10.734),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m                         'vf_preds': np.ndarray((22,), dtype=float32, min=-0.008, max=0.005, mean=-0.004)},\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m 2019-11-05 12:26:53,414\tINFO sampler.py:555 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                       { 'action_logp': np.ndarray((1,), dtype=float32, min=-0.693, max=-0.693, mean=-0.693),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.5, max=0.5, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.001, max=0.0, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.0, max=-0.0, mean=-0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m 2019-11-05 12:26:53,431\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m { 'agent0': { 'data': { 'action_logp': np.ndarray((13,), dtype=float32, min=-0.699, max=-0.688, mean=-0.692),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'action_prob': np.ndarray((13,), dtype=float32, min=0.497, max=0.503, mean=0.501),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'actions': np.ndarray((13,), dtype=int64, min=0.0, max=1.0, mean=0.231),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'advantages': np.ndarray((13,), dtype=float32, min=0.995, max=12.248, mean=6.724),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'agent_index': np.ndarray((13,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'behaviour_logits': np.ndarray((13, 2), dtype=float32, min=-0.002, max=0.009, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'dones': np.ndarray((13,), dtype=bool, min=0.0, max=1.0, mean=0.077),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'eps_id': np.ndarray((13,), dtype=int64, min=115857438.0, max=115857438.0, mean=115857438.0),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'infos': np.ndarray((13,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'new_obs': np.ndarray((13, 4), dtype=float32, min=-1.359, max=2.213, mean=0.092),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'obs': np.ndarray((13, 4), dtype=float32, min=-1.355, max=2.112, mean=0.073),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'prev_actions': np.ndarray((13,), dtype=int64, min=0.0, max=1.0, mean=0.231),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'prev_rewards': np.ndarray((13,), dtype=float32, min=0.0, max=1.0, mean=0.923),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'rewards': np.ndarray((13,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         't': np.ndarray((13,), dtype=int64, min=0.0, max=12.0, mean=6.0),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'unroll_id': np.ndarray((13,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'value_targets': np.ndarray((13,), dtype=float32, min=1.0, max=12.248, mean=6.728),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m                         'vf_preds': np.ndarray((13,), dtype=float32, min=-0.0, max=0.005, mean=0.003)},\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m 2019-11-05 12:26:53,521\tINFO rollout_worker.py:501 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m { 'data': { 'action_logp': np.ndarray((200,), dtype=float32, min=-0.694, max=-0.692, mean=-0.693),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'action_prob': np.ndarray((200,), dtype=float32, min=0.499, max=0.501, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=0.998, max=43.04, mean=14.866),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.004, max=0.004, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.04),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=5399995.0, max=1729089006.0, mean=747483805.935),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.809, max=2.238, mean=0.022),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.462, max=1.897, mean=0.02),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.48),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.955),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=55.0, mean=15.925),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.999, max=43.04, mean=14.866),\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=-0.003, max=0.004, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=32089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m 2019-11-05 12:26:53,674\tINFO rollout_worker.py:501 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m { 'data': { 'action_logp': np.ndarray((200,), dtype=float32, min=-0.696, max=-0.69, mean=-0.693),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'action_prob': np.ndarray((200,), dtype=float32, min=0.499, max=0.501, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.465),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=0.992, max=30.359, mean=10.283),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.01, max=0.01, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.055),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=470622567.0, max=1905473261.0, mean=1176801956.38),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.45, max=2.386, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.103, max=2.039, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.45),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.945),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=35.0, mean=10.095),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=1.0, max=30.359, mean=10.283),\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=-0.009, max=0.008, mean=-0.001)},\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=32085)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m 2019-11-05 12:26:53,698\tINFO rollout_worker.py:501 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m { 'data': { 'action_logp': np.ndarray((200,), dtype=float32, min=-0.699, max=-0.688, mean=-0.693),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'action_prob': np.ndarray((200,), dtype=float32, min=0.497, max=0.503, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.47),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=0.995, max=36.381, mean=13.641),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.009, max=0.009, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.04),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=115857438.0, max=1983284906.0, mean=873933738.5),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-1.83, max=2.213, mean=-0.01),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-1.483, max=2.112, mean=-0.01),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.45),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.955),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=44.0, mean=14.16),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=1.0, max=36.381, mean=13.641),\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=-0.004, max=0.005, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=32098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32090)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32090)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32090)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32090)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32090)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32090)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32090)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32090)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32090)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32090)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32090)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32090)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32094)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32094)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32094)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32094)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32094)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32094)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32094)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32094)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32094)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32094)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32094)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32094)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32095)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32095)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32095)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32095)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32095)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32095)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32095)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32095)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32095)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32095)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32095)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32095)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32091)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32091)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32091)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32091)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32091)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32091)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32091)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32091)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32091)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32091)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32091)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32091)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32088)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32088)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32088)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32088)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32088)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32088)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32088)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32088)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32088)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32088)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32088)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32088)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32092)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32092)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32092)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32092)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32092)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32092)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32092)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32092)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32092)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32092)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32092)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32092)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32096)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32096)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32096)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32096)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32096)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32096)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32096)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32096)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32096)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32096)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32096)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32096)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32093)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32093)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32093)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32093)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32093)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32093)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32093)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32093)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32093)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32093)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32093)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32093)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32087)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32087)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32087)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32087)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32087)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32087)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32087)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32087)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32087)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32087)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32087)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32087)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=32097)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,065\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_1/kernel:0' shape=(4, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,065\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,066\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(4, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,066\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,066\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,066\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,066\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(256, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,066\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,066\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_out/kernel:0' shape=(256, 2) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,066\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_out/bias:0' shape=(2,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,066\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/value_out/kernel:0' shape=(256, 1) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,066\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,071\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m { 'inputs': [ np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.477),\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=1.0, mean=0.953),\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m               np.ndarray((4000, 4), dtype=float32, min=-2.551, max=2.646, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.695, max=-0.691, mean=-0.693),\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.499),\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-1.271, max=4.367, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.005, max=0.005, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m               np.ndarray((4000, 4), dtype=float32, min=-2.551, max=2.646, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.477),\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=1.0, mean=0.953),\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.997, max=49.511, mean=11.929),\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.005, max=0.005, mean=-0.0)],\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m                     <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m                     <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32083)\u001b[0m 2019-11-05 12:26:59,071\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,121\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_1/kernel:0' shape=(4, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,121\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,121\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(4, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,121\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,121\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,121\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,121\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(256, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,121\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,121\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_out/kernel:0' shape=(256, 2) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,121\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_out/bias:0' shape=(2,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,121\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/value_out/kernel:0' shape=(256, 1) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,121\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,125\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m { 'inputs': [ np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.476),\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=1.0, mean=0.956),\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m               np.ndarray((4000, 4), dtype=float32, min=-2.519, max=2.956, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.699, max=-0.687, mean=-0.693),\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.498),\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-1.215, max=4.125, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.01, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m               np.ndarray((4000, 4), dtype=float32, min=-2.519, max=2.956, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.476),\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=1.0, mean=0.956),\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.995, max=53.878, mean=13.029),\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.008, max=0.007, mean=-0.0)],\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m                     <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m                     <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32084)\u001b[0m 2019-11-05 12:26:59,125\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,261\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_1/kernel:0' shape=(4, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,261\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,261\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(4, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,261\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,261\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,261\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,261\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(256, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,261\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,261\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_out/kernel:0' shape=(256, 2) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,261\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_out/bias:0' shape=(2,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,261\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/value_out/kernel:0' shape=(256, 1) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,262\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,268\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m { 'inputs': [ np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.481),\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=1.0, mean=0.955),\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m               np.ndarray((4000, 4), dtype=float32, min=-2.479, max=2.471, mean=-0.007),\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.697, max=-0.689, mean=-0.693),\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.503),\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-1.297, max=4.742, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.011, max=0.011, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m               np.ndarray((4000, 4), dtype=float32, min=-2.479, max=2.471, mean=-0.007),\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.481),\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=1.0, mean=0.955),\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.992, max=52.941, mean=12.146),\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.009, max=0.009, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m                     <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m                     <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32086)\u001b[0m 2019-11-05 12:26:59,268\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-05\n",
      "  done: false\n",
      "  episode_len_mean: 22.93103448275862\n",
      "  episode_reward_max: 110.0\n",
      "  episode_reward_mean: 22.93103448275862\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 174\n",
      "  episodes_total: 174\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5978.905\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.6592265367507935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03551456332206726\n",
      "        policy_loss: -0.0465213917195797\n",
      "        total_loss: 98.96653747558594\n",
      "        vf_explained_var: 0.23356541991233826\n",
      "        vf_loss: 99.0059585571289\n",
      "    load_time_ms: 149.781\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 5771.815\n",
      "    update_time_ms: 1003.61\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.09115938245281822\n",
      "    mean_inference_ms: 1.0876992260685745\n",
      "    mean_processing_ms: 0.22948077248799983\n",
      "  time_since_restore: 12.99305772781372\n",
      "  time_this_iter_s: 12.99305772781372\n",
      "  time_total_s: 12.99305772781372\n",
      "  timestamp: 1572985625\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 12 s, 1 iter, 4000 ts, 22.9 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING\n",
      "\n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-05\n",
      "  done: false\n",
      "  episode_len_mean: 22.133333333333333\n",
      "  episode_reward_max: 75.0\n",
      "  episode_reward_mean: 22.133333333333333\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 180\n",
      "  episodes_total: 180\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 6006.44\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.6594988703727722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.034184813499450684\n",
      "        policy_loss: -0.046662092208862305\n",
      "        total_loss: 71.0130844116211\n",
      "        vf_explained_var: 0.2562512457370758\n",
      "        vf_loss: 71.05290985107422\n",
      "    load_time_ms: 111.457\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 5940.26\n",
      "    update_time_ms: 933.997\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.09372924989654076\n",
      "    mean_inference_ms: 1.119351869700879\n",
      "    mean_processing_ms: 0.2284421827816361\n",
      "  time_since_restore: 13.075527667999268\n",
      "  time_this_iter_s: 13.075527667999268\n",
      "  time_total_s: 13.075527667999268\n",
      "  timestamp: 1572985625\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-05\n",
      "  done: false\n",
      "  episode_len_mean: 21.37433155080214\n",
      "  episode_reward_max: 68.0\n",
      "  episode_reward_mean: 21.37433155080214\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 187\n",
      "  episodes_total: 187\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 6202.563\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.65839684009552\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03581550344824791\n",
      "        policy_loss: -0.04472034424543381\n",
      "        total_loss: 67.32965850830078\n",
      "        vf_explained_var: 0.3208985924720764\n",
      "        vf_loss: 67.36721801757812\n",
      "    load_time_ms: 160.26\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 5818.099\n",
      "    update_time_ms: 1074.308\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.09305034390511259\n",
      "    mean_inference_ms: 1.0922058198905478\n",
      "    mean_processing_ms: 0.2295246901317883\n",
      "  time_since_restore: 13.330303192138672\n",
      "  time_this_iter_s: 13.330303192138672\n",
      "  time_total_s: 13.330303192138672\n",
      "  timestamp: 1572985625\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-15\n",
      "  done: false\n",
      "  episode_len_mean: 45.24\n",
      "  episode_reward_max: 150.0\n",
      "  episode_reward_mean: 45.24\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 252\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5708.382\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.5931130647659302\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021448835730552673\n",
      "        policy_loss: -0.029072362929582596\n",
      "        total_loss: 338.5256042480469\n",
      "        vf_explained_var: 0.18667808175086975\n",
      "        vf_loss: 338.5483093261719\n",
      "    load_time_ms: 76.151\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 7936\n",
      "    sample_time_ms: 5372.841\n",
      "    update_time_ms: 505.207\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08867909359595999\n",
      "    mean_inference_ms: 1.0306418229285568\n",
      "    mean_processing_ms: 0.21569406869524904\n",
      "  time_since_restore: 23.422125339508057\n",
      "  time_this_iter_s: 10.429067611694336\n",
      "  time_total_s: 23.422125339508057\n",
      "  timestamp: 1572985635\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 13 s, 1 iter, 4000 ts, 21.4 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 23 s, 2 iter, 8000 ts, 45.2 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 13 s, 1 iter, 4000 ts, 22.1 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-15\n",
      "  done: false\n",
      "  episode_len_mean: 38.05825242718446\n",
      "  episode_reward_max: 121.0\n",
      "  episode_reward_mean: 38.05825242718446\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 103\n",
      "  episodes_total: 290\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5735.048\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.610302209854126\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02802569605410099\n",
      "        policy_loss: -0.03770089149475098\n",
      "        total_loss: 196.94100952148438\n",
      "        vf_explained_var: 0.2866315245628357\n",
      "        vf_loss: 196.97030639648438\n",
      "    load_time_ms: 81.709\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 7936\n",
      "    sample_time_ms: 5426.131\n",
      "    update_time_ms: 539.246\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08927278407825974\n",
      "    mean_inference_ms: 1.0180880078493448\n",
      "    mean_processing_ms: 0.21519784315900953\n",
      "  time_since_restore: 23.648500442504883\n",
      "  time_this_iter_s: 10.318197250366211\n",
      "  time_total_s: 23.648500442504883\n",
      "  timestamp: 1572985635\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-15\n",
      "  done: false\n",
      "  episode_len_mean: 44.2\n",
      "  episode_reward_max: 155.0\n",
      "  episode_reward_mean: 44.2\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 260\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5710.744\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.5996946692466736\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020824598148465157\n",
      "        policy_loss: -0.03395312651991844\n",
      "        total_loss: 290.6732177734375\n",
      "        vf_explained_var: 0.16873322427272797\n",
      "        vf_loss: 290.7009582519531\n",
      "    load_time_ms: 56.895\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 7936\n",
      "    sample_time_ms: 5444.431\n",
      "    update_time_ms: 469.08\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08944597871840376\n",
      "    mean_inference_ms: 1.0459866474156347\n",
      "    mean_processing_ms: 0.2149903367155358\n",
      "  time_since_restore: 23.454301595687866\n",
      "  time_this_iter_s: 10.378773927688599\n",
      "  time_total_s: 23.454301595687866\n",
      "  timestamp: 1572985635\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-26\n",
      "  done: false\n",
      "  episode_len_mean: 68.56\n",
      "  episode_reward_max: 190.0\n",
      "  episode_reward_mean: 68.56\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 293\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5588.6\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.5763333439826965\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009114841930568218\n",
      "        policy_loss: -0.01657574437558651\n",
      "        total_loss: 362.8189392089844\n",
      "        vf_explained_var: 0.2898622453212738\n",
      "        vf_loss: 362.8314208984375\n",
      "    load_time_ms: 51.483\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 5280.188\n",
      "    update_time_ms: 338.508\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08763343572482181\n",
      "    mean_inference_ms: 1.0088677214780668\n",
      "    mean_processing_ms: 0.20860185641590495\n",
      "  time_since_restore: 33.8819522857666\n",
      "  time_this_iter_s: 10.459826946258545\n",
      "  time_total_s: 33.8819522857666\n",
      "  timestamp: 1572985646\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 23 s, 2 iter, 8000 ts, 38.1 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 33 s, 3 iter, 12000 ts, 68.6 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 23 s, 2 iter, 8000 ts, 44.2 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-26\n",
      "  done: false\n",
      "  episode_len_mean: 60.72\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 60.72\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 332\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5653.817\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5806428790092468\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011755076237022877\n",
      "        policy_loss: -0.014005251228809357\n",
      "        total_loss: 413.66717529296875\n",
      "        vf_explained_var: 0.35836225748062134\n",
      "        vf_loss: 413.6758728027344\n",
      "    load_time_ms: 55.191\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 5344.001\n",
      "    update_time_ms: 360.457\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08891606514067778\n",
      "    mean_inference_ms: 1.0140432221687237\n",
      "    mean_processing_ms: 0.21181326221002142\n",
      "  time_since_restore: 34.337382793426514\n",
      "  time_this_iter_s: 10.68888235092163\n",
      "  time_total_s: 34.337382793426514\n",
      "  timestamp: 1572985646\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-26\n",
      "  done: false\n",
      "  episode_len_mean: 71.43\n",
      "  episode_reward_max: 197.0\n",
      "  episode_reward_mean: 71.43\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 296\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5613.4\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.5633388757705688\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008720635436475277\n",
      "        policy_loss: -0.014542387798428535\n",
      "        total_loss: 505.0604248046875\n",
      "        vf_explained_var: 0.2421419769525528\n",
      "        vf_loss: 505.071044921875\n",
      "    load_time_ms: 38.764\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 11904\n",
      "    sample_time_ms: 5388.564\n",
      "    update_time_ms: 313.931\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08822779358783035\n",
      "    mean_inference_ms: 1.0259526523045082\n",
      "    mean_processing_ms: 0.20953265021070383\n",
      "  time_since_restore: 34.1639769077301\n",
      "  time_this_iter_s: 10.709675312042236\n",
      "  time_total_s: 34.1639769077301\n",
      "  timestamp: 1572985646\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-36\n",
      "  done: false\n",
      "  episode_len_mean: 96.89\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 96.89\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 320\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5535.461\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.5819402933120728\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005530779715627432\n",
      "        policy_loss: -0.008253130130469799\n",
      "        total_loss: 285.24468994140625\n",
      "        vf_explained_var: 0.5373584628105164\n",
      "        vf_loss: 285.25042724609375\n",
      "    load_time_ms: 39.264\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 15872\n",
      "    sample_time_ms: 5251.744\n",
      "    update_time_ms: 255.958\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08735657669032484\n",
      "    mean_inference_ms: 1.0046295556241123\n",
      "    mean_processing_ms: 0.2053977907135765\n",
      "  time_since_restore: 44.44272255897522\n",
      "  time_this_iter_s: 10.560770273208618\n",
      "  time_total_s: 44.44272255897522\n",
      "  timestamp: 1572985656\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 34 s, 3 iter, 12000 ts, 60.7 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 44 s, 4 iter, 16000 ts, 96.9 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 34 s, 3 iter, 12000 ts, 71.4 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-37\n",
      "  done: false\n",
      "  episode_len_mean: 87.98\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 87.98\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 366\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5587.155\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5583837032318115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012239267118275166\n",
      "        policy_loss: -0.015485149808228016\n",
      "        total_loss: 238.77084350585938\n",
      "        vf_explained_var: 0.5802785754203796\n",
      "        vf_loss: 238.7808380126953\n",
      "    load_time_ms: 42.231\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 15872\n",
      "    sample_time_ms: 5286.054\n",
      "    update_time_ms: 274.037\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0884785000513242\n",
      "    mean_inference_ms: 1.0082879302515364\n",
      "    mean_processing_ms: 0.20757874836990656\n",
      "  time_since_restore: 44.863662242889404\n",
      "  time_this_iter_s: 10.52627944946289\n",
      "  time_total_s: 44.863662242889404\n",
      "  timestamp: 1572985657\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-37\n",
      "  done: false\n",
      "  episode_len_mean: 98.38\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 98.38\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 321\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5576.658\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.5533934235572815\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006560051813721657\n",
      "        policy_loss: -0.010633972473442554\n",
      "        total_loss: 378.9465026855469\n",
      "        vf_explained_var: 0.36941829323768616\n",
      "        vf_loss: 378.9542236328125\n",
      "    load_time_ms: 29.74\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 15872\n",
      "    sample_time_ms: 5417.555\n",
      "    update_time_ms: 236.892\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08818661185662778\n",
      "    mean_inference_ms: 1.0275936193089197\n",
      "    mean_processing_ms: 0.20742527951135684\n",
      "  time_since_restore: 45.14921045303345\n",
      "  time_this_iter_s: 10.985233545303345\n",
      "  time_total_s: 45.14921045303345\n",
      "  timestamp: 1572985657\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-47\n",
      "  done: false\n",
      "  episode_len_mean: 123.25\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 123.25\n",
      "  episode_reward_min: 17.0\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 344\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5491.162\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.5520247220993042\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008807122707366943\n",
      "        policy_loss: -0.015657033771276474\n",
      "        total_loss: 259.14373779296875\n",
      "        vf_explained_var: 0.6435591578483582\n",
      "        vf_loss: 259.1554260253906\n",
      "    load_time_ms: 31.848\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 19840\n",
      "    sample_time_ms: 5202.885\n",
      "    update_time_ms: 206.643\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08699417335313449\n",
      "    mean_inference_ms: 0.9987502113635687\n",
      "    mean_processing_ms: 0.20187176642958093\n",
      "  time_since_restore: 54.78527307510376\n",
      "  time_this_iter_s: 10.34255051612854\n",
      "  time_total_s: 54.78527307510376\n",
      "  timestamp: 1572985667\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 44 s, 4 iter, 16000 ts, 88 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 54 s, 5 iter, 20000 ts, 123 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 45 s, 4 iter, 16000 ts, 98.4 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-47\n",
      "  done: false\n",
      "  episode_len_mean: 119.09\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 119.09\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 391\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5528.588\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5528947114944458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01078119594603777\n",
      "        policy_loss: -0.011237530037760735\n",
      "        total_loss: 231.52633666992188\n",
      "        vf_explained_var: 0.6878073215484619\n",
      "        vf_loss: 231.53271484375\n",
      "    load_time_ms: 34.75\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 19840\n",
      "    sample_time_ms: 5255.907\n",
      "    update_time_ms: 220.422\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08806327515905017\n",
      "    mean_inference_ms: 1.0034328941926955\n",
      "    mean_processing_ms: 0.20355204257786993\n",
      "  time_since_restore: 55.31347846984863\n",
      "  time_this_iter_s: 10.449816226959229\n",
      "  time_total_s: 55.31347846984863\n",
      "  timestamp: 1572985667\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-48\n",
      "  done: false\n",
      "  episode_len_mean: 126.71\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 126.71\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 341\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5541.813\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.5490338802337646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0025702076964080334\n",
      "        policy_loss: -0.0027343202382326126\n",
      "        total_loss: 264.3009033203125\n",
      "        vf_explained_var: 0.599951446056366\n",
      "        vf_loss: 264.30255126953125\n",
      "    load_time_ms: 24.288\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 19840\n",
      "    sample_time_ms: 5371.284\n",
      "    update_time_ms: 190.584\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08806135897839025\n",
      "    mean_inference_ms: 1.027310791256365\n",
      "    mean_processing_ms: 0.2050934512659605\n",
      "  time_since_restore: 55.75195837020874\n",
      "  time_this_iter_s: 10.602747917175293\n",
      "  time_total_s: 55.75195837020874\n",
      "  timestamp: 1572985668\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-57\n",
      "  done: false\n",
      "  episode_len_mean: 144.37\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 144.37\n",
      "  episode_reward_min: 20.0\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 368\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5510.134\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.5270785093307495\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008681702427566051\n",
      "        policy_loss: -0.014560242183506489\n",
      "        total_loss: 159.0757598876953\n",
      "        vf_explained_var: 0.7275335192680359\n",
      "        vf_loss: 159.08641052246094\n",
      "    load_time_ms: 27.178\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 23808\n",
      "    sample_time_ms: 5163.585\n",
      "    update_time_ms: 173.211\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08675150273239972\n",
      "    mean_inference_ms: 0.9932269395233391\n",
      "    mean_processing_ms: 0.19914254607877496\n",
      "  time_since_restore: 65.3748767375946\n",
      "  time_this_iter_s: 10.589603662490845\n",
      "  time_total_s: 65.3748767375946\n",
      "  timestamp: 1572985677\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 55 s, 5 iter, 20000 ts, 119 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 65 s, 6 iter, 24000 ts, 144 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 55 s, 5 iter, 20000 ts, 127 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-58\n",
      "  done: false\n",
      "  episode_len_mean: 140.79\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 140.79\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 413\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5542.373\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5514686703681946\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01073738094419241\n",
      "        policy_loss: -0.008312521502375603\n",
      "        total_loss: 110.95795440673828\n",
      "        vf_explained_var: 0.8348385691642761\n",
      "        vf_loss: 110.96142578125\n",
      "    load_time_ms: 29.414\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 23808\n",
      "    sample_time_ms: 5227.987\n",
      "    update_time_ms: 184.618\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08787296478698747\n",
      "    mean_inference_ms: 1.000116127996188\n",
      "    mean_processing_ms: 0.20145276979175744\n",
      "  time_since_restore: 66.02842307090759\n",
      "  time_this_iter_s: 10.71494460105896\n",
      "  time_total_s: 66.02842307090759\n",
      "  timestamp: 1572985678\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-27-58\n",
      "  done: false\n",
      "  episode_len_mean: 158.34\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 158.34\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 362\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5498.213\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.5205859541893005\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006450889632105827\n",
      "        policy_loss: -0.004253878258168697\n",
      "        total_loss: 216.9969024658203\n",
      "        vf_explained_var: 0.5339537262916565\n",
      "        vf_loss: 216.9996795654297\n",
      "    load_time_ms: 20.587\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 23808\n",
      "    sample_time_ms: 5343.5\n",
      "    update_time_ms: 159.737\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08794035193444881\n",
      "    mean_inference_ms: 1.0259893448665087\n",
      "    mean_processing_ms: 0.20253311082726047\n",
      "  time_since_restore: 66.2512435913086\n",
      "  time_this_iter_s: 10.499285221099854\n",
      "  time_total_s: 66.2512435913086\n",
      "  timestamp: 1572985678\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-08\n",
      "  done: false\n",
      "  episode_len_mean: 163.76\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 163.76\n",
      "  episode_reward_min: 20.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 391\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5512.982\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.5354496836662292\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00762329064309597\n",
      "        policy_loss: -0.013339575380086899\n",
      "        total_loss: 211.96542358398438\n",
      "        vf_explained_var: 0.6053429841995239\n",
      "        vf_loss: 211.9752960205078\n",
      "    load_time_ms: 23.548\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 27776\n",
      "    sample_time_ms: 5144.682\n",
      "    update_time_ms: 149.39\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08656392426109819\n",
      "    mean_inference_ms: 0.9885440647774084\n",
      "    mean_processing_ms: 0.1966471195850405\n",
      "  time_since_restore: 75.95254373550415\n",
      "  time_this_iter_s: 10.577666997909546\n",
      "  time_total_s: 75.95254373550415\n",
      "  timestamp: 1572985688\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 66 s, 6 iter, 24000 ts, 141 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 75 s, 7 iter, 28000 ts, 164 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 66 s, 6 iter, 24000 ts, 158 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-08\n",
      "  done: false\n",
      "  episode_len_mean: 155.34\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 155.34\n",
      "  episode_reward_min: 25.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 435\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5540.486\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5313830375671387\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011972228065133095\n",
      "        policy_loss: -0.01097913645207882\n",
      "        total_loss: 98.84397888183594\n",
      "        vf_explained_var: 0.8619473576545715\n",
      "        vf_loss: 98.84955596923828\n",
      "    load_time_ms: 25.683\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 27776\n",
      "    sample_time_ms: 5191.017\n",
      "    update_time_ms: 158.991\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08766009587665628\n",
      "    mean_inference_ms: 0.9955697853588558\n",
      "    mean_processing_ms: 0.19907257031437833\n",
      "  time_since_restore: 76.54159545898438\n",
      "  time_this_iter_s: 10.513172388076782\n",
      "  time_total_s: 76.54159545898438\n",
      "  timestamp: 1572985688\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-09\n",
      "  done: false\n",
      "  episode_len_mean: 175.98\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 175.98\n",
      "  episode_reward_min: 42.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 383\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5483.089\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.5074794292449951\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006976602599024773\n",
      "        policy_loss: -0.008549378253519535\n",
      "        total_loss: 178.5673828125\n",
      "        vf_explained_var: 0.6901128888130188\n",
      "        vf_loss: 178.57432556152344\n",
      "    load_time_ms: 17.986\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 27776\n",
      "    sample_time_ms: 5321.601\n",
      "    update_time_ms: 137.722\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08790978408155353\n",
      "    mean_inference_ms: 1.024661630362299\n",
      "    mean_processing_ms: 0.20086503004796896\n",
      "  time_since_restore: 76.85031485557556\n",
      "  time_this_iter_s: 10.599071264266968\n",
      "  time_total_s: 76.85031485557556\n",
      "  timestamp: 1572985689\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-19\n",
      "  done: false\n",
      "  episode_len_mean: 172.57\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 172.57\n",
      "  episode_reward_min: 75.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 412\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5519.632\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.5469009876251221\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007526540197432041\n",
      "        policy_loss: -0.015306991524994373\n",
      "        total_loss: 197.9813232421875\n",
      "        vf_explained_var: 0.6918059587478638\n",
      "        vf_loss: 197.99327087402344\n",
      "    load_time_ms: 20.881\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 31744\n",
      "    sample_time_ms: 5182.682\n",
      "    update_time_ms: 131.504\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08654977325204742\n",
      "    mean_inference_ms: 0.9864762091529863\n",
      "    mean_processing_ms: 0.19515262758881227\n",
      "  time_since_restore: 86.98368120193481\n",
      "  time_this_iter_s: 11.031137466430664\n",
      "  time_total_s: 86.98368120193481\n",
      "  timestamp: 1572985699\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 76 s, 7 iter, 28000 ts, 155 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 86 s, 8 iter, 32000 ts, 173 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 76 s, 7 iter, 28000 ts, 176 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-19\n",
      "  done: false\n",
      "  episode_len_mean: 168.02\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 168.02\n",
      "  episode_reward_min: 26.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 458\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5527.836\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.554101824760437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008409562520682812\n",
      "        policy_loss: -0.007935376837849617\n",
      "        total_loss: 137.2529754638672\n",
      "        vf_explained_var: 0.8124548196792603\n",
      "        vf_loss: 137.25714111328125\n",
      "    load_time_ms: 22.733\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 31744\n",
      "    sample_time_ms: 5213.046\n",
      "    update_time_ms: 139.663\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08760849148268106\n",
      "    mean_inference_ms: 0.9935874786252181\n",
      "    mean_processing_ms: 0.19740357659376023\n",
      "  time_since_restore: 87.36217617988586\n",
      "  time_this_iter_s: 10.82058072090149\n",
      "  time_total_s: 87.36217617988586\n",
      "  timestamp: 1572985699\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-19\n",
      "  done: false\n",
      "  episode_len_mean: 188.36\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 188.36\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 404\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5466.045\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.5121170878410339\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003742773551493883\n",
      "        policy_loss: -0.006678013596683741\n",
      "        total_loss: 168.90194702148438\n",
      "        vf_explained_var: 0.6767474412918091\n",
      "        vf_loss: 168.90780639648438\n",
      "    load_time_ms: 16.298\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 31744\n",
      "    sample_time_ms: 5305.18\n",
      "    update_time_ms: 121.041\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08780785521927306\n",
      "    mean_inference_ms: 1.0218937114223292\n",
      "    mean_processing_ms: 0.19917925632610667\n",
      "  time_since_restore: 87.4042866230011\n",
      "  time_this_iter_s: 10.553971767425537\n",
      "  time_total_s: 87.4042866230011\n",
      "  timestamp: 1572985699\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 178.78\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 178.78\n",
      "  episode_reward_min: 75.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 433\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5502.143\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.5422672033309937\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006746163126081228\n",
      "        policy_loss: -0.013728813268244267\n",
      "        total_loss: 298.27691650390625\n",
      "        vf_explained_var: 0.5705915093421936\n",
      "        vf_loss: 298.2876281738281\n",
      "    load_time_ms: 18.938\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 35712\n",
      "    sample_time_ms: 5178.67\n",
      "    update_time_ms: 117.684\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08658759138746998\n",
      "    mean_inference_ms: 0.9855478540458907\n",
      "    mean_processing_ms: 0.19400174028502795\n",
      "  time_since_restore: 97.51538753509521\n",
      "  time_this_iter_s: 10.5317063331604\n",
      "  time_total_s: 97.51538753509521\n",
      "  timestamp: 1572985709\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 87 s, 8 iter, 32000 ts, 168 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 97 s, 9 iter, 36000 ts, 179 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 87 s, 8 iter, 32000 ts, 188 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-30\n",
      "  done: false\n",
      "  episode_len_mean: 179.62\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 179.62\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 479\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5509.621\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5493206977844238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0145783182233572\n",
      "        policy_loss: -0.007670021615922451\n",
      "        total_loss: 103.92825317382812\n",
      "        vf_explained_var: 0.826849102973938\n",
      "        vf_loss: 103.92937469482422\n",
      "    load_time_ms: 20.706\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 35712\n",
      "    sample_time_ms: 5190.985\n",
      "    update_time_ms: 124.863\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08758366416274263\n",
      "    mean_inference_ms: 0.991204707979477\n",
      "    mean_processing_ms: 0.1962407533696074\n",
      "  time_since_restore: 97.7582757472992\n",
      "  time_this_iter_s: 10.39609956741333\n",
      "  time_total_s: 97.7582757472992\n",
      "  timestamp: 1572985710\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-30\n",
      "  done: false\n",
      "  episode_len_mean: 193.02\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.02\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 424\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5455.597\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.11249999701976776\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.4856272041797638\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009852597489953041\n",
      "        policy_loss: -0.004374347161501646\n",
      "        total_loss: 142.5706024169922\n",
      "        vf_explained_var: 0.7488800287246704\n",
      "        vf_loss: 142.57386779785156\n",
      "    load_time_ms: 14.827\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 35712\n",
      "    sample_time_ms: 5298.126\n",
      "    update_time_ms: 107.979\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08763408633450244\n",
      "    mean_inference_ms: 1.0180300765809351\n",
      "    mean_processing_ms: 0.19780191427828978\n",
      "  time_since_restore: 98.03264665603638\n",
      "  time_this_iter_s: 10.628360033035278\n",
      "  time_total_s: 98.03264665603638\n",
      "  timestamp: 1572985710\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-40\n",
      "  done: false\n",
      "  episode_len_mean: 181.44\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 181.44\n",
      "  episode_reward_min: 85.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 501\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5510.521\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5485736131668091\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013757622800767422\n",
      "        policy_loss: -0.011817173101007938\n",
      "        total_loss: 107.01314544677734\n",
      "        vf_explained_var: 0.8469539284706116\n",
      "        vf_loss: 107.01875305175781\n",
      "    load_time_ms: 18.88\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 39680\n",
      "    sample_time_ms: 5163.075\n",
      "    update_time_ms: 112.763\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08750443704194694\n",
      "    mean_inference_ms: 0.9883203115438729\n",
      "    mean_processing_ms: 0.1952050005962429\n",
      "  time_since_restore: 108.2034661769867\n",
      "  time_this_iter_s: 10.4451904296875\n",
      "  time_total_s: 108.2034661769867\n",
      "  timestamp: 1572985720\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 108 s, 10 iter, 40000 ts, 181 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 97 s, 9 iter, 36000 ts, 179 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 98 s, 9 iter, 36000 ts, 193 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-40\n",
      "  done: false\n",
      "  episode_len_mean: 185.35\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 185.35\n",
      "  episode_reward_min: 75.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 453\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5495.893\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.5321015119552612\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006816898472607136\n",
      "        policy_loss: -0.013376418501138687\n",
      "        total_loss: 512.0289916992188\n",
      "        vf_explained_var: 0.3274379074573517\n",
      "        vf_loss: 512.039306640625\n",
      "    load_time_ms: 17.839\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 39680\n",
      "    sample_time_ms: 5169.417\n",
      "    update_time_ms: 106.709\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08664293541653918\n",
      "    mean_inference_ms: 0.9857929532252107\n",
      "    mean_processing_ms: 0.19307861796706946\n",
      "  time_since_restore: 108.06313967704773\n",
      "  time_this_iter_s: 10.547752141952515\n",
      "  time_total_s: 108.06313967704773\n",
      "  timestamp: 1572985720\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-41\n",
      "  done: false\n",
      "  episode_len_mean: 193.68\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.68\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 444\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5444.106\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.11249999701976776\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.4763122498989105\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0035396357998251915\n",
      "        policy_loss: -0.00222333543933928\n",
      "        total_loss: 75.79769897460938\n",
      "        vf_explained_var: 0.8691245913505554\n",
      "        vf_loss: 75.79954528808594\n",
      "    load_time_ms: 13.561\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 39680\n",
      "    sample_time_ms: 5297.549\n",
      "    update_time_ms: 98.11\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0875004558803682\n",
      "    mean_inference_ms: 1.0158911547619598\n",
      "    mean_processing_ms: 0.19683549512089749\n",
      "  time_since_restore: 108.68421030044556\n",
      "  time_this_iter_s: 10.65156364440918\n",
      "  time_total_s: 108.68421030044556\n",
      "  timestamp: 1572985721\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-50\n",
      "  done: false\n",
      "  episode_len_mean: 191.34\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.34\n",
      "  episode_reward_min: 75.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 473\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5425.579\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.51752769947052\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007057374808937311\n",
      "        policy_loss: -0.01250881515443325\n",
      "        total_loss: 418.62823486328125\n",
      "        vf_explained_var: 0.44407153129577637\n",
      "        vf_loss: 418.6375732421875\n",
      "    load_time_ms: 3.093\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 43648\n",
      "    sample_time_ms: 5091.619\n",
      "    update_time_ms: 7.032\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08666939445527287\n",
      "    mean_inference_ms: 0.986330481983683\n",
      "    mean_processing_ms: 0.19241344627191775\n",
      "  time_since_restore: 118.35169172286987\n",
      "  time_this_iter_s: 10.288552045822144\n",
      "  time_total_s: 118.35169172286987\n",
      "  timestamp: 1572985730\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 108 s, 10 iter, 40000 ts, 181 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 118 s, 11 iter, 44000 ts, 191 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 108 s, 10 iter, 40000 ts, 194 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-51\n",
      "  done: false\n",
      "  episode_len_mean: 180.28\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 180.28\n",
      "  episode_reward_min: 65.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 524\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5418.419\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5142425894737244\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014278082177042961\n",
      "        policy_loss: -0.010477389208972454\n",
      "        total_loss: 242.22596740722656\n",
      "        vf_explained_var: 0.6943144202232361\n",
      "        vf_loss: 242.2300262451172\n",
      "    load_time_ms: 3.132\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 43648\n",
      "    sample_time_ms: 5106.088\n",
      "    update_time_ms: 5.738\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08743835386145832\n",
      "    mean_inference_ms: 0.9872208623557225\n",
      "    mean_processing_ms: 0.19436816414744765\n",
      "  time_since_restore: 118.74909806251526\n",
      "  time_this_iter_s: 10.545631885528564\n",
      "  time_total_s: 118.74909806251526\n",
      "  timestamp: 1572985731\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-28-51\n",
      "  done: false\n",
      "  episode_len_mean: 195.08\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.08\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 464\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5379.262\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.4854038953781128\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010383198037743568\n",
      "        policy_loss: -0.009422036819159985\n",
      "        total_loss: 38.31562805175781\n",
      "        vf_explained_var: 0.9301999807357788\n",
      "        vf_loss: 38.324466705322266\n",
      "    load_time_ms: 2.745\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 43648\n",
      "    sample_time_ms: 5202.388\n",
      "    update_time_ms: 5.079\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08732624627241214\n",
      "    mean_inference_ms: 1.013582751524939\n",
      "    mean_processing_ms: 0.1959216261565598\n",
      "  time_since_restore: 119.04413795471191\n",
      "  time_this_iter_s: 10.359927654266357\n",
      "  time_total_s: 119.04413795471191\n",
      "  timestamp: 1572985731\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-01\n",
      "  done: false\n",
      "  episode_len_mean: 195.85\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.85\n",
      "  episode_reward_min: 93.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 493\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5413.688\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.520317018032074\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008612724021077156\n",
      "        policy_loss: -0.013589759357273579\n",
      "        total_loss: 515.0565795898438\n",
      "        vf_explained_var: 0.3265341818332672\n",
      "        vf_loss: 515.0662841796875\n",
      "    load_time_ms: 3.204\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 47616\n",
      "    sample_time_ms: 5108.764\n",
      "    update_time_ms: 6.851\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08671381850471492\n",
      "    mean_inference_ms: 0.9869877158198517\n",
      "    mean_processing_ms: 0.1920038634244891\n",
      "  time_since_restore: 128.83281087875366\n",
      "  time_this_iter_s: 10.481119155883789\n",
      "  time_total_s: 128.83281087875366\n",
      "  timestamp: 1572985741\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 118 s, 11 iter, 44000 ts, 180 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 128 s, 12 iter, 48000 ts, 196 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 119 s, 11 iter, 44000 ts, 195 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-01\n",
      "  done: false\n",
      "  episode_len_mean: 182.91\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 182.91\n",
      "  episode_reward_min: 65.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 545\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5423.257\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5500184893608093\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015459663234651089\n",
      "        policy_loss: -0.013127333484590054\n",
      "        total_loss: 143.0847930908203\n",
      "        vf_explained_var: 0.8170089721679688\n",
      "        vf_loss: 143.09095764160156\n",
      "    load_time_ms: 3.142\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 47616\n",
      "    sample_time_ms: 5119.748\n",
      "    update_time_ms: 5.79\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08733041914154549\n",
      "    mean_inference_ms: 0.9865659763071867\n",
      "    mean_processing_ms: 0.19369701154225927\n",
      "  time_since_restore: 129.25079774856567\n",
      "  time_this_iter_s: 10.501699686050415\n",
      "  time_total_s: 129.25079774856567\n",
      "  timestamp: 1572985741\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-02\n",
      "  done: false\n",
      "  episode_len_mean: 196.12\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.12\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 484\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5355.731\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.4681386649608612\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00789796095341444\n",
      "        policy_loss: -0.006529758218675852\n",
      "        total_loss: 34.82743835449219\n",
      "        vf_explained_var: 0.9228783249855042\n",
      "        vf_loss: 34.833526611328125\n",
      "    load_time_ms: 2.765\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 47616\n",
      "    sample_time_ms: 5240.083\n",
      "    update_time_ms: 5.082\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08713794666470175\n",
      "    mean_inference_ms: 1.0124097636065204\n",
      "    mean_processing_ms: 0.19507558963695865\n",
      "  time_since_restore: 129.56313562393188\n",
      "  time_this_iter_s: 10.51899766921997\n",
      "  time_total_s: 129.56313562393188\n",
      "  timestamp: 1572985742\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-11\n",
      "  done: false\n",
      "  episode_len_mean: 197.25\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.25\n",
      "  episode_reward_min: 124.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 514\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5394.267\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.5226157903671265\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009022849611938\n",
      "        policy_loss: -0.01645749993622303\n",
      "        total_loss: 421.7471618652344\n",
      "        vf_explained_var: 0.37089160084724426\n",
      "        vf_loss: 421.7595520019531\n",
      "    load_time_ms: 3.14\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 51584\n",
      "    sample_time_ms: 5111.507\n",
      "    update_time_ms: 7.163\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08662002748144328\n",
      "    mean_inference_ms: 0.9859691869779458\n",
      "    mean_processing_ms: 0.1915219873928764\n",
      "  time_since_restore: 139.12883019447327\n",
      "  time_this_iter_s: 10.296019315719604\n",
      "  time_total_s: 139.12883019447327\n",
      "  timestamp: 1572985751\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 129 s, 12 iter, 48000 ts, 183 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 139 s, 13 iter, 52000 ts, 197 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 129 s, 12 iter, 48000 ts, 196 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-12\n",
      "  done: false\n",
      "  episode_len_mean: 186.54\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 186.54\n",
      "  episode_reward_min: 65.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 565\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5421.409\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5394280552864075\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015872633084654808\n",
      "        policy_loss: -0.009996804408729076\n",
      "        total_loss: 344.1707763671875\n",
      "        vf_explained_var: 0.6107707619667053\n",
      "        vf_loss: 344.1737060546875\n",
      "    load_time_ms: 3.115\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 51584\n",
      "    sample_time_ms: 5116.262\n",
      "    update_time_ms: 5.88\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08716409718733205\n",
      "    mean_inference_ms: 0.9857470435799308\n",
      "    mean_processing_ms: 0.1930197171821837\n",
      "  time_since_restore: 139.88308548927307\n",
      "  time_this_iter_s: 10.632287740707397\n",
      "  time_total_s: 139.88308548927307\n",
      "  timestamp: 1572985752\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-12\n",
      "  done: false\n",
      "  episode_len_mean: 198.67\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.67\n",
      "  episode_reward_min: 152.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 504\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5341.226\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.4585723876953125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012410672381520271\n",
      "        policy_loss: -0.01215112954378128\n",
      "        total_loss: 35.79769515991211\n",
      "        vf_explained_var: 0.9031159281730652\n",
      "        vf_loss: 35.80915069580078\n",
      "    load_time_ms: 2.731\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 51584\n",
      "    sample_time_ms: 5254.412\n",
      "    update_time_ms: 5.388\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08705001047713497\n",
      "    mean_inference_ms: 1.0122461820644784\n",
      "    mean_processing_ms: 0.19441948325557667\n",
      "  time_since_restore: 140.27231454849243\n",
      "  time_this_iter_s: 10.709178924560547\n",
      "  time_total_s: 140.27231454849243\n",
      "  timestamp: 1572985752\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-21\n",
      "  done: false\n",
      "  episode_len_mean: 197.58\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.58\n",
      "  episode_reward_min: 132.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 534\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5348.379\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.5145180821418762\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010457213968038559\n",
      "        policy_loss: -0.019395606592297554\n",
      "        total_loss: 437.3054504394531\n",
      "        vf_explained_var: 0.3248726427555084\n",
      "        vf_loss: 437.32012939453125\n",
      "    load_time_ms: 3.058\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 55552\n",
      "    sample_time_ms: 5077.403\n",
      "    update_time_ms: 7.264\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08652832328705085\n",
      "    mean_inference_ms: 0.9840065367592843\n",
      "    mean_processing_ms: 0.19106609457703505\n",
      "  time_since_restore: 148.88878846168518\n",
      "  time_this_iter_s: 9.759958267211914\n",
      "  time_total_s: 148.88878846168518\n",
      "  timestamp: 1572985761\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 139 s, 13 iter, 52000 ts, 187 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 148 s, 14 iter, 56000 ts, 198 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 140 s, 13 iter, 52000 ts, 199 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-22\n",
      "  done: false\n",
      "  episode_len_mean: 187.42\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 187.42\n",
      "  episode_reward_min: 65.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 586\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5399.68\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5479881167411804\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015763219445943832\n",
      "        policy_loss: -0.008962027728557587\n",
      "        total_loss: 269.7751159667969\n",
      "        vf_explained_var: 0.6326408982276917\n",
      "        vf_loss: 269.7769775390625\n",
      "    load_time_ms: 3.035\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 55552\n",
      "    sample_time_ms: 5110.172\n",
      "    update_time_ms: 4.903\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08704333068600534\n",
      "    mean_inference_ms: 0.9854513606892255\n",
      "    mean_processing_ms: 0.19249778820488103\n",
      "  time_since_restore: 150.1226372718811\n",
      "  time_this_iter_s: 10.239551782608032\n",
      "  time_total_s: 150.1226372718811\n",
      "  timestamp: 1572985762\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-23\n",
      "  done: false\n",
      "  episode_len_mean: 199.78\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.78\n",
      "  episode_reward_min: 192.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 524\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5358.914\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.48627299070358276\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005965272430330515\n",
      "        policy_loss: -0.00298541528172791\n",
      "        total_loss: 43.48231887817383\n",
      "        vf_explained_var: 0.8522170782089233\n",
      "        vf_loss: 43.4849739074707\n",
      "    load_time_ms: 2.669\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 55552\n",
      "    sample_time_ms: 5202.329\n",
      "    update_time_ms: 5.343\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08697555361199877\n",
      "    mean_inference_ms: 1.011329559009283\n",
      "    mean_processing_ms: 0.19380522831933125\n",
      "  time_since_restore: 150.91673755645752\n",
      "  time_this_iter_s: 10.644423007965088\n",
      "  time_total_s: 150.91673755645752\n",
      "  timestamp: 1572985763\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-32\n",
      "  done: false\n",
      "  episode_len_mean: 198.62\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.62\n",
      "  episode_reward_min: 140.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 554\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5369.261\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.5113914012908936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010201829485595226\n",
      "        policy_loss: -0.01382418628782034\n",
      "        total_loss: 398.6817932128906\n",
      "        vf_explained_var: 0.26239505410194397\n",
      "        vf_loss: 398.6910095214844\n",
      "    load_time_ms: 2.987\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 59520\n",
      "    sample_time_ms: 5097.293\n",
      "    update_time_ms: 6.767\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0864878162940805\n",
      "    mean_inference_ms: 0.9825718119247756\n",
      "    mean_processing_ms: 0.19077388802120457\n",
      "  time_since_restore: 159.63623309135437\n",
      "  time_this_iter_s: 10.74744462966919\n",
      "  time_total_s: 159.63623309135437\n",
      "  timestamp: 1572985772\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 150 s, 14 iter, 56000 ts, 187 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 159 s, 15 iter, 60000 ts, 199 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 150 s, 14 iter, 56000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-32\n",
      "  done: false\n",
      "  episode_len_mean: 189.9\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 189.9\n",
      "  episode_reward_min: 65.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 606\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5399.449\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5246264338493347\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013461531139910221\n",
      "        policy_loss: -0.007158341817557812\n",
      "        total_loss: 411.0088195800781\n",
      "        vf_explained_var: 0.5745015144348145\n",
      "        vf_loss: 411.00994873046875\n",
      "    load_time_ms: 2.753\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 59520\n",
      "    sample_time_ms: 5093.174\n",
      "    update_time_ms: 4.95\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08696220395285029\n",
      "    mean_inference_ms: 0.9852165685096352\n",
      "    mean_processing_ms: 0.19209330994895815\n",
      "  time_since_restore: 160.3970127105713\n",
      "  time_this_iter_s: 10.274375438690186\n",
      "  time_total_s: 160.3970127105713\n",
      "  timestamp: 1572985772\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-34\n",
      "  done: false\n",
      "  episode_len_mean: 199.86\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.86\n",
      "  episode_reward_min: 193.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 544\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5399.23\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.4584614932537079\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0078144371509552\n",
      "        policy_loss: -0.007026185747236013\n",
      "        total_loss: 52.17683029174805\n",
      "        vf_explained_var: 0.7414143681526184\n",
      "        vf_loss: 52.18341827392578\n",
      "    load_time_ms: 2.916\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 59520\n",
      "    sample_time_ms: 5173.127\n",
      "    update_time_ms: 5.942\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08690446999937058\n",
      "    mean_inference_ms: 1.0092082603353003\n",
      "    mean_processing_ms: 0.19323311145665711\n",
      "  time_since_restore: 161.64105939865112\n",
      "  time_this_iter_s: 10.724321842193604\n",
      "  time_total_s: 161.64105939865112\n",
      "  timestamp: 1572985774\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-42\n",
      "  done: false\n",
      "  episode_len_mean: 197.33\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.33\n",
      "  episode_reward_min: 71.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 575\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5354.264\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4974926710128784\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010407919995486736\n",
      "        policy_loss: -0.020995862782001495\n",
      "        total_loss: 469.5069580078125\n",
      "        vf_explained_var: 0.2108803540468216\n",
      "        vf_loss: 469.5232238769531\n",
      "    load_time_ms: 2.823\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 63488\n",
      "    sample_time_ms: 5133.684\n",
      "    update_time_ms: 7.151\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08655153825367982\n",
      "    mean_inference_ms: 0.9822720246419163\n",
      "    mean_processing_ms: 0.19052702371402766\n",
      "  time_since_restore: 170.44736695289612\n",
      "  time_this_iter_s: 10.811133861541748\n",
      "  time_total_s: 170.44736695289612\n",
      "  timestamp: 1572985782\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 160 s, 15 iter, 60000 ts, 190 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 170 s, 16 iter, 64000 ts, 197 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 161 s, 15 iter, 60000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-43\n",
      "  done: false\n",
      "  episode_len_mean: 195.88\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.88\n",
      "  episode_reward_min: 102.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 626\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5351.106\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5227473378181458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013051294721662998\n",
      "        policy_loss: -0.008847120217978954\n",
      "        total_loss: 569.6304321289062\n",
      "        vf_explained_var: 0.4116639792919159\n",
      "        vf_loss: 569.6333618164062\n",
      "    load_time_ms: 2.733\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 63488\n",
      "    sample_time_ms: 5120.781\n",
      "    update_time_ms: 5.204\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08693725651651821\n",
      "    mean_inference_ms: 0.9851094202599863\n",
      "    mean_processing_ms: 0.19187177534712355\n",
      "  time_since_restore: 170.9120216369629\n",
      "  time_this_iter_s: 10.515008926391602\n",
      "  time_total_s: 170.9120216369629\n",
      "  timestamp: 1572985783\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-44\n",
      "  done: false\n",
      "  episode_len_mean: 199.99\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.99\n",
      "  episode_reward_min: 199.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 564\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5447.506\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.46333062648773193\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005894116126000881\n",
      "        policy_loss: -0.004842062015086412\n",
      "        total_loss: 75.29933166503906\n",
      "        vf_explained_var: 0.4429558217525482\n",
      "        vf_loss: 75.3038558959961\n",
      "    load_time_ms: 2.905\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 63488\n",
      "    sample_time_ms: 5147.394\n",
      "    update_time_ms: 5.773\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08684299752788509\n",
      "    mean_inference_ms: 1.0075257545124645\n",
      "    mean_processing_ms: 0.1926906603553524\n",
      "  time_since_restore: 172.36542534828186\n",
      "  time_this_iter_s: 10.724365949630737\n",
      "  time_total_s: 172.36542534828186\n",
      "  timestamp: 1572985784\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-53\n",
      "  done: false\n",
      "  episode_len_mean: 198.16\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.16\n",
      "  episode_reward_min: 102.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 646\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5319.379\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5105199813842773\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01569800265133381\n",
      "        policy_loss: -0.006046464201062918\n",
      "        total_loss: 573.5614624023438\n",
      "        vf_explained_var: 0.41891467571258545\n",
      "        vf_loss: 573.5604248046875\n",
      "    load_time_ms: 2.65\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 67456\n",
      "    sample_time_ms: 5131.149\n",
      "    update_time_ms: 5.209\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08694478011113492\n",
      "    mean_inference_ms: 0.9845466725414022\n",
      "    mean_processing_ms: 0.19174980526669355\n",
      "  time_since_restore: 181.21524143218994\n",
      "  time_this_iter_s: 10.30321979522705\n",
      "  time_total_s: 181.21524143218994\n",
      "  timestamp: 1572985793\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.7/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 181 s, 17 iter, 68000 ts, 198 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 170 s, 16 iter, 64000 ts, 197 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32086], 172 s, 16 iter, 64000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-53\n",
      "  done: false\n",
      "  episode_len_mean: 196.66\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.66\n",
      "  episode_reward_min: 71.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 595\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5335.85\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.48888468742370605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009133677929639816\n",
      "        policy_loss: -0.01832456700503826\n",
      "        total_loss: 369.9010009765625\n",
      "        vf_explained_var: 0.23620149493217468\n",
      "        vf_loss: 369.9151916503906\n",
      "    load_time_ms: 2.835\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 67456\n",
      "    sample_time_ms: 5161.941\n",
      "    update_time_ms: 7.161\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08661824311527053\n",
      "    mean_inference_ms: 0.9824066586108771\n",
      "    mean_processing_ms: 0.19037679026667104\n",
      "  time_since_restore: 181.1249589920044\n",
      "  time_this_iter_s: 10.677592039108276\n",
      "  time_total_s: 181.1249589920044\n",
      "  timestamp: 1572985793\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "Result for PPO_CartPole-v0_2_lr=0.0001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-29-55\n",
      "  done: true\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 200.0\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 584\n",
      "  experiment_id: cfabab52241345e0ab2fd173c217824b\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5436.55\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 9.999999747378752e-05\n",
      "        entropy: 0.4358729124069214\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010973620228469372\n",
      "        policy_loss: -0.007661328185349703\n",
      "        total_loss: 76.77180480957031\n",
      "        vf_explained_var: 0.45013532042503357\n",
      "        vf_loss: 76.77883911132812\n",
      "    load_time_ms: 2.984\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 67456\n",
      "    sample_time_ms: 5184.024\n",
      "    update_time_ms: 5.742\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32086\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08688579149786119\n",
      "    mean_inference_ms: 1.0063179805316542\n",
      "    mean_processing_ms: 0.1923321845176036\n",
      "  time_since_restore: 183.21990132331848\n",
      "  time_this_iter_s: 10.854475975036621\n",
      "  time_total_s: 183.21990132331848\n",
      "  timestamp: 1572985795\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 95a3a56e\n",
      "  \n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-30-03\n",
      "  done: false\n",
      "  episode_len_mean: 193.72\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.72\n",
      "  episode_reward_min: 32.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 669\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5228.85\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.49877554178237915\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013295636512339115\n",
      "        policy_loss: -0.010115856304764748\n",
      "        total_loss: 657.8023681640625\n",
      "        vf_explained_var: 0.41771820187568665\n",
      "        vf_loss: 657.8064575195312\n",
      "    load_time_ms: 2.603\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 71424\n",
      "    sample_time_ms: 5110.235\n",
      "    update_time_ms: 5.208\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08703114739238202\n",
      "    mean_inference_ms: 0.9837798899907017\n",
      "    mean_processing_ms: 0.19176328964325992\n",
      "  time_since_restore: 190.9213786125183\n",
      "  time_this_iter_s: 9.70613718032837\n",
      "  time_total_s: 190.9213786125183\n",
      "  timestamp: 1572985803\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 2, 'TERMINATED': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 190 s, 18 iter, 72000 ts, 194 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 181 s, 17 iter, 68000 ts, 197 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-30-03\n",
      "  done: false\n",
      "  episode_len_mean: 195.61\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.61\n",
      "  episode_reward_min: 53.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 616\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5244.162\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.48653119802474976\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00901686493307352\n",
      "        policy_loss: -0.01683908887207508\n",
      "        total_loss: 307.2572326660156\n",
      "        vf_explained_var: 0.32939961552619934\n",
      "        vf_loss: 307.2700500488281\n",
      "    load_time_ms: 2.836\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 71424\n",
      "    sample_time_ms: 5146.746\n",
      "    update_time_ms: 7.091\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0867533239198925\n",
      "    mean_inference_ms: 0.9829982623606597\n",
      "    mean_processing_ms: 0.19028878232910684\n",
      "  time_since_restore: 191.0900387763977\n",
      "  time_this_iter_s: 9.96507978439331\n",
      "  time_total_s: 191.0900387763977\n",
      "  timestamp: 1572985803\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-30-12\n",
      "  done: false\n",
      "  episode_len_mean: 191.55\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.55\n",
      "  episode_reward_min: 32.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 690\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5158.318\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.49958908557891846\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019817838445305824\n",
      "        policy_loss: -0.004657898098230362\n",
      "        total_loss: 632.9559326171875\n",
      "        vf_explained_var: 0.3619565963745117\n",
      "        vf_loss: 632.9517211914062\n",
      "    load_time_ms: 2.338\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 75392\n",
      "    sample_time_ms: 5087.894\n",
      "    update_time_ms: 5.157\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08706209543246328\n",
      "    mean_inference_ms: 0.982764135499056\n",
      "    mean_processing_ms: 0.19164026521946892\n",
      "  time_since_restore: 200.38491249084473\n",
      "  time_this_iter_s: 9.463533878326416\n",
      "  time_total_s: 200.38491249084473\n",
      "  timestamp: 1572985812\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 2, 'TERMINATED': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 200 s, 19 iter, 76000 ts, 192 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 191 s, 18 iter, 72000 ts, 196 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-30-13\n",
      "  done: false\n",
      "  episode_len_mean: 192.01\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.01\n",
      "  episode_reward_min: 48.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 638\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5214.139\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4914950430393219\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012403782457113266\n",
      "        policy_loss: -0.016611982136964798\n",
      "        total_loss: 320.48809814453125\n",
      "        vf_explained_var: 0.3658207356929779\n",
      "        vf_loss: 320.4990539550781\n",
      "    load_time_ms: 2.693\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 75392\n",
      "    sample_time_ms: 5103.684\n",
      "    update_time_ms: 7.57\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08683364767211142\n",
      "    mean_inference_ms: 0.9835185251960931\n",
      "    mean_processing_ms: 0.1901799689969896\n",
      "  time_since_restore: 200.9003884792328\n",
      "  time_this_iter_s: 9.810349702835083\n",
      "  time_total_s: 200.9003884792328\n",
      "  timestamp: 1572985813\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-30-22\n",
      "  done: false\n",
      "  episode_len_mean: 191.23\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.23\n",
      "  episode_reward_min: 48.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 659\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5165.035\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4972602427005768\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010506963357329369\n",
      "        policy_loss: -0.019341513514518738\n",
      "        total_loss: 322.0653381347656\n",
      "        vf_explained_var: 0.41769930720329285\n",
      "        vf_loss: 322.0799255371094\n",
      "    load_time_ms: 2.101\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 79360\n",
      "    sample_time_ms: 5039.044\n",
      "    update_time_ms: 7.712\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08678221917421411\n",
      "    mean_inference_ms: 0.9820996785142049\n",
      "    mean_processing_ms: 0.18993058120463907\n",
      "  time_since_restore: 210.30812644958496\n",
      "  time_this_iter_s: 9.407737970352173\n",
      "  time_total_s: 210.30812644958496\n",
      "  timestamp: 1572985822\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 2, 'TERMINATED': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 200 s, 19 iter, 76000 ts, 192 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 210 s, 20 iter, 80000 ts, 191 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-30-23\n",
      "  done: false\n",
      "  episode_len_mean: 190.27\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 190.27\n",
      "  episode_reward_min: 32.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 711\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5121.764\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.49662649631500244\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013530793599784374\n",
      "        policy_loss: -0.010660598054528236\n",
      "        total_loss: 432.3680725097656\n",
      "        vf_explained_var: 0.4558311700820923\n",
      "        vf_loss: 432.3726501464844\n",
      "    load_time_ms: 2.301\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 79360\n",
      "    sample_time_ms: 5100.756\n",
      "    update_time_ms: 5.494\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08707711134376783\n",
      "    mean_inference_ms: 0.9819003729696021\n",
      "    mean_processing_ms: 0.19148441681976006\n",
      "  time_since_restore: 210.59561610221863\n",
      "  time_this_iter_s: 10.210703611373901\n",
      "  time_total_s: 210.59561610221863\n",
      "  timestamp: 1572985823\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-30-32\n",
      "  done: false\n",
      "  episode_len_mean: 190.75\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 190.75\n",
      "  episode_reward_min: 48.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 679\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5140.314\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4848381280899048\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010156766511499882\n",
      "        policy_loss: -0.020338529720902443\n",
      "        total_loss: 383.03741455078125\n",
      "        vf_explained_var: 0.3041630983352661\n",
      "        vf_loss: 383.0532531738281\n",
      "    load_time_ms: 2.236\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 83328\n",
      "    sample_time_ms: 4992.634\n",
      "    update_time_ms: 7.957\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08662121039214207\n",
      "    mean_inference_ms: 0.9790571725478349\n",
      "    mean_processing_ms: 0.18957766397917675\n",
      "  time_since_restore: 219.88552331924438\n",
      "  time_this_iter_s: 9.577396869659424\n",
      "  time_total_s: 219.88552331924438\n",
      "  timestamp: 1572985832\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 2, 'TERMINATED': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 210 s, 20 iter, 80000 ts, 190 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 219 s, 21 iter, 84000 ts, 191 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-30-32\n",
      "  done: false\n",
      "  episode_len_mean: 184.98\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 184.98\n",
      "  episode_reward_min: 32.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 734\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5078.225\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5180526375770569\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013770956546068192\n",
      "        policy_loss: -0.015314348042011261\n",
      "        total_loss: 306.3204345703125\n",
      "        vf_explained_var: 0.6031966209411621\n",
      "        vf_loss: 306.32952880859375\n",
      "    load_time_ms: 2.727\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 83328\n",
      "    sample_time_ms: 5042.774\n",
      "    update_time_ms: 5.629\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0869963450025548\n",
      "    mean_inference_ms: 0.9795208029526908\n",
      "    mean_processing_ms: 0.19112212027190878\n",
      "  time_since_restore: 220.12816882133484\n",
      "  time_this_iter_s: 9.532552719116211\n",
      "  time_total_s: 220.12816882133484\n",
      "  timestamp: 1572985832\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-30-41\n",
      "  done: false\n",
      "  episode_len_mean: 189.73\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 189.73\n",
      "  episode_reward_min: 48.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 700\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5099.054\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.476145476102829\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009531361050903797\n",
      "        policy_loss: -0.02040848508477211\n",
      "        total_loss: 334.9764709472656\n",
      "        vf_explained_var: 0.4222848117351532\n",
      "        vf_loss: 334.9925537109375\n",
      "    load_time_ms: 2.12\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 87296\n",
      "    sample_time_ms: 4920.943\n",
      "    update_time_ms: 7.99\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08635503291868173\n",
      "    mean_inference_ms: 0.9741260797871266\n",
      "    mean_processing_ms: 0.18902621310510284\n",
      "  time_since_restore: 229.23380827903748\n",
      "  time_this_iter_s: 9.34828495979309\n",
      "  time_total_s: 229.23380827903748\n",
      "  timestamp: 1572985841\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 2, 'TERMINATED': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 220 s, 21 iter, 84000 ts, 185 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 229 s, 22 iter, 88000 ts, 190 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-30-42\n",
      "  done: false\n",
      "  episode_len_mean: 183.67\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 183.67\n",
      "  episode_reward_min: 32.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 754\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5021.577\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.5035939812660217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014912571758031845\n",
      "        policy_loss: -0.009714228101074696\n",
      "        total_loss: 389.1358337402344\n",
      "        vf_explained_var: 0.4426691234111786\n",
      "        vf_loss: 389.13885498046875\n",
      "    load_time_ms: 3.12\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 87296\n",
      "    sample_time_ms: 5009.59\n",
      "    update_time_ms: 5.462\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08688681682593292\n",
      "    mean_inference_ms: 0.9771871772300343\n",
      "    mean_processing_ms: 0.19072236223685368\n",
      "  time_since_restore: 229.73236751556396\n",
      "  time_this_iter_s: 9.604198694229126\n",
      "  time_total_s: 229.73236751556396\n",
      "  timestamp: 1572985842\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-30-51\n",
      "  done: false\n",
      "  episode_len_mean: 193.07\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.07\n",
      "  episode_reward_min: 108.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 721\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5058.178\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.474447101354599\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011538886465132236\n",
      "        policy_loss: -0.022029152140021324\n",
      "        total_loss: 350.52813720703125\n",
      "        vf_explained_var: 0.45682618021965027\n",
      "        vf_loss: 350.544921875\n",
      "    load_time_ms: 2.17\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 91264\n",
      "    sample_time_ms: 4864.85\n",
      "    update_time_ms: 7.668\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08600980215227277\n",
      "    mean_inference_ms: 0.968340511180626\n",
      "    mean_processing_ms: 0.18835847406992337\n",
      "  time_since_restore: 238.55687832832336\n",
      "  time_this_iter_s: 9.323070049285889\n",
      "  time_total_s: 238.55687832832336\n",
      "  timestamp: 1572985851\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 2, 'TERMINATED': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 229 s, 22 iter, 88000 ts, 184 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 238 s, 23 iter, 92000 ts, 193 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-30-51\n",
      "  done: false\n",
      "  episode_len_mean: 189.46\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 189.46\n",
      "  episode_reward_min: 46.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 774\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4944.572\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.48948025703430176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020792843773961067\n",
      "        policy_loss: -0.0028033750131726265\n",
      "        total_loss: 190.37338256835938\n",
      "        vf_explained_var: 0.6993885636329651\n",
      "        vf_loss: 190.3668212890625\n",
      "    load_time_ms: 3.246\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 91264\n",
      "    sample_time_ms: 4949.358\n",
      "    update_time_ms: 5.425\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0867000180404648\n",
      "    mean_inference_ms: 0.9740969748719488\n",
      "    mean_processing_ms: 0.19020456192243276\n",
      "  time_since_restore: 238.99156522750854\n",
      "  time_this_iter_s: 9.25919771194458\n",
      "  time_total_s: 238.99156522750854\n",
      "  timestamp: 1572985851\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-00\n",
      "  done: false\n",
      "  episode_len_mean: 195.51\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.51\n",
      "  episode_reward_min: 125.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 741\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 5055.284\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4872661530971527\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011726696975529194\n",
      "        policy_loss: -0.021165957674384117\n",
      "        total_loss: 341.89654541015625\n",
      "        vf_explained_var: 0.3461951017379761\n",
      "        vf_loss: 341.9123229980469\n",
      "    load_time_ms: 2.335\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 95232\n",
      "    sample_time_ms: 4865.589\n",
      "    update_time_ms: 7.727\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08572086793076954\n",
      "    mean_inference_ms: 0.9635013619769432\n",
      "    mean_processing_ms: 0.18778212757302334\n",
      "  time_since_restore: 248.29733800888062\n",
      "  time_this_iter_s: 9.740459680557251\n",
      "  time_total_s: 248.29733800888062\n",
      "  timestamp: 1572985860\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 2, 'TERMINATED': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 238 s, 23 iter, 92000 ts, 189 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 248 s, 24 iter, 96000 ts, 196 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-01\n",
      "  done: false\n",
      "  episode_len_mean: 190.54\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 190.54\n",
      "  episode_reward_min: 61.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 795\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4912.807\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.48954230546951294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011639682576060295\n",
      "        policy_loss: -0.014021767303347588\n",
      "        total_loss: 163.9007568359375\n",
      "        vf_explained_var: 0.7466498017311096\n",
      "        vf_loss: 163.90692138671875\n",
      "    load_time_ms: 3.275\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 95232\n",
      "    sample_time_ms: 4913.847\n",
      "    update_time_ms: 5.589\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08648963844095871\n",
      "    mean_inference_ms: 0.9707810137052594\n",
      "    mean_processing_ms: 0.18971297638093126\n",
      "  time_since_restore: 248.5553789138794\n",
      "  time_this_iter_s: 9.56381368637085\n",
      "  time_total_s: 248.5553789138794\n",
      "  timestamp: 1572985861\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-10\n",
      "  done: false\n",
      "  episode_len_mean: 195.82\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.82\n",
      "  episode_reward_min: 140.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 761\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4957.571\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.48996731638908386\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010235955938696861\n",
      "        policy_loss: -0.018659887835383415\n",
      "        total_loss: 307.4423828125\n",
      "        vf_explained_var: 0.37362098693847656\n",
      "        vf_loss: 307.4564514160156\n",
      "    load_time_ms: 2.397\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 99200\n",
      "    sample_time_ms: 4817.75\n",
      "    update_time_ms: 7.728\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0854842192231766\n",
      "    mean_inference_ms: 0.9595618904659703\n",
      "    mean_processing_ms: 0.1873019401877015\n",
      "  time_since_restore: 257.58625078201294\n",
      "  time_this_iter_s: 9.288912773132324\n",
      "  time_total_s: 257.58625078201294\n",
      "  timestamp: 1572985870\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 2, 'TERMINATED': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 248 s, 24 iter, 96000 ts, 191 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 257 s, 25 iter, 100000 ts, 196 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-10\n",
      "  done: false\n",
      "  episode_len_mean: 193.62\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.62\n",
      "  episode_reward_min: 61.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 815\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4866.518\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.48734626173973083\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011144046671688557\n",
      "        policy_loss: -0.010759801603853703\n",
      "        total_loss: 515.3237915039062\n",
      "        vf_explained_var: 0.43696969747543335\n",
      "        vf_loss: 515.3270263671875\n",
      "    load_time_ms: 3.284\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 99200\n",
      "    sample_time_ms: 4891.066\n",
      "    update_time_ms: 5.638\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08628633503340748\n",
      "    mean_inference_ms: 0.9674040893611584\n",
      "    mean_processing_ms: 0.1892446251363321\n",
      "  time_since_restore: 258.1375479698181\n",
      "  time_this_iter_s: 9.58216905593872\n",
      "  time_total_s: 258.1375479698181\n",
      "  timestamp: 1572985870\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-19\n",
      "  done: false\n",
      "  episode_len_mean: 196.3\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.3\n",
      "  episode_reward_min: 140.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 781\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4898.074\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.47849327325820923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012288895435631275\n",
      "        policy_loss: -0.021448543295264244\n",
      "        total_loss: 373.6078796386719\n",
      "        vf_explained_var: 0.2883893549442291\n",
      "        vf_loss: 373.6238708496094\n",
      "    load_time_ms: 2.544\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 103168\n",
      "    sample_time_ms: 4727.309\n",
      "    update_time_ms: 7.413\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08526539945212022\n",
      "    mean_inference_ms: 0.9558738857027909\n",
      "    mean_processing_ms: 0.18685283670204364\n",
      "  time_since_restore: 266.8913719654083\n",
      "  time_this_iter_s: 9.305121183395386\n",
      "  time_total_s: 266.8913719654083\n",
      "  timestamp: 1572985879\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 2, 'TERMINATED': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 258 s, 25 iter, 100000 ts, 194 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 266 s, 26 iter, 104000 ts, 196 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-19\n",
      "  done: false\n",
      "  episode_len_mean: 198.15\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.15\n",
      "  episode_reward_min: 61.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 835\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4817.986\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.4652600586414337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010253754444420338\n",
      "        policy_loss: -0.0072382595390081406\n",
      "        total_loss: 458.44354248046875\n",
      "        vf_explained_var: 0.3983128070831299\n",
      "        vf_loss: 458.4438781738281\n",
      "    load_time_ms: 3.236\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 103168\n",
      "    sample_time_ms: 4815.36\n",
      "    update_time_ms: 5.184\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08608502282407306\n",
      "    mean_inference_ms: 0.9640964475665644\n",
      "    mean_processing_ms: 0.18878347639236145\n",
      "  time_since_restore: 267.3985345363617\n",
      "  time_this_iter_s: 9.260986566543579\n",
      "  time_total_s: 267.3985345363617\n",
      "  timestamp: 1572985879\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-29\n",
      "  done: false\n",
      "  episode_len_mean: 196.36\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.36\n",
      "  episode_reward_min: 118.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 802\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4807.065\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.46650663018226624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013769052922725677\n",
      "        policy_loss: -0.024792924523353577\n",
      "        total_loss: 297.5478210449219\n",
      "        vf_explained_var: 0.41436830163002014\n",
      "        vf_loss: 297.5663757324219\n",
      "    load_time_ms: 2.571\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 107136\n",
      "    sample_time_ms: 4690.506\n",
      "    update_time_ms: 7.489\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08511455186463429\n",
      "    mean_inference_ms: 0.9532218765204437\n",
      "    mean_processing_ms: 0.18648546840908928\n",
      "  time_since_restore: 276.28928804397583\n",
      "  time_this_iter_s: 9.397916078567505\n",
      "  time_total_s: 276.28928804397583\n",
      "  timestamp: 1572985889\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 2, 'TERMINATED': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 267 s, 26 iter, 104000 ts, 198 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 276 s, 27 iter, 108000 ts, 196 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-29\n",
      "  done: false\n",
      "  episode_len_mean: 199.54\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.54\n",
      "  episode_reward_min: 154.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 855\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4728.697\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.472368985414505\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012332304380834103\n",
      "        policy_loss: -0.008733646012842655\n",
      "        total_loss: 274.234619140625\n",
      "        vf_explained_var: 0.4953482151031494\n",
      "        vf_loss: 274.2350158691406\n",
      "    load_time_ms: 3.238\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 107136\n",
      "    sample_time_ms: 4804.182\n",
      "    update_time_ms: 5.064\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08590397285109024\n",
      "    mean_inference_ms: 0.9612312886405542\n",
      "    mean_processing_ms: 0.1883778601223888\n",
      "  time_since_restore: 276.6909728050232\n",
      "  time_this_iter_s: 9.292438268661499\n",
      "  time_total_s: 276.6909728050232\n",
      "  timestamp: 1572985889\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 199.54\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.54\n",
      "  episode_reward_min: 154.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 875\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4728.417\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.4677897095680237\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013137759640812874\n",
      "        policy_loss: -0.007757481653243303\n",
      "        total_loss: 224.4859619140625\n",
      "        vf_explained_var: 0.5691013932228088\n",
      "        vf_loss: 224.48484802246094\n",
      "    load_time_ms: 3.364\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 111104\n",
      "    sample_time_ms: 4765.719\n",
      "    update_time_ms: 5.136\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08576425256398686\n",
      "    mean_inference_ms: 0.9589693544096863\n",
      "    mean_processing_ms: 0.18806182575933084\n",
      "  time_since_restore: 286.01277804374695\n",
      "  time_this_iter_s: 9.321805238723755\n",
      "  time_total_s: 286.01277804374695\n",
      "  timestamp: 1572985898\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 2, 'TERMINATED': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 286 s, 28 iter, 112000 ts, 200 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 276 s, 27 iter, 108000 ts, 196 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 195.92\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.92\n",
      "  episode_reward_min: 118.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 823\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4839.395\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4735201597213745\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011124598793685436\n",
      "        policy_loss: -0.02283952571451664\n",
      "        total_loss: 290.26763916015625\n",
      "        vf_explained_var: 0.48938408493995667\n",
      "        vf_loss: 290.2854919433594\n",
      "    load_time_ms: 2.577\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 111104\n",
      "    sample_time_ms: 4653.1\n",
      "    update_time_ms: 7.351\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08501985671705642\n",
      "    mean_inference_ms: 0.9513348147006786\n",
      "    mean_processing_ms: 0.18620825523052636\n",
      "  time_since_restore: 286.19812059402466\n",
      "  time_this_iter_s: 9.908832550048828\n",
      "  time_total_s: 286.19812059402466\n",
      "  timestamp: 1572985898\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-48\n",
      "  done: false\n",
      "  episode_len_mean: 195.79\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.79\n",
      "  episode_reward_min: 118.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 843\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4796.403\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4606379270553589\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011227717623114586\n",
      "        policy_loss: -0.02460978925228119\n",
      "        total_loss: 303.966796875\n",
      "        vf_explained_var: 0.41139307618141174\n",
      "        vf_loss: 303.9862976074219\n",
      "    load_time_ms: 2.613\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 115072\n",
      "    sample_time_ms: 4643.344\n",
      "    update_time_ms: 6.575\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08493324341937492\n",
      "    mean_inference_ms: 0.9493067050325286\n",
      "    mean_processing_ms: 0.1859413162900564\n",
      "  time_since_restore: 295.4632933139801\n",
      "  time_this_iter_s: 9.265172719955444\n",
      "  time_total_s: 295.4632933139801\n",
      "  timestamp: 1572985908\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.5/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'RUNNING': 2, 'TERMINATED': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32083], 286 s, 28 iter, 112000 ts, 200 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 295 s, 29 iter, 116000 ts, 196 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_0_lr=0.01:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-48\n",
      "  done: true\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 200.0\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 895\n",
      "  experiment_id: 75340ea5ac59413e83879f28bfa1b112\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4753.372\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.009999999776482582\n",
      "        entropy: 0.44634512066841125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013047897256910801\n",
      "        policy_loss: -0.002890830859541893\n",
      "        total_loss: 162.4501190185547\n",
      "        vf_explained_var: 0.706311047077179\n",
      "        vf_loss: 162.4442138671875\n",
      "    load_time_ms: 3.4\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 115072\n",
      "    sample_time_ms: 4765.328\n",
      "    update_time_ms: 5.004\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32083\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08566007592752949\n",
      "    mean_inference_ms: 0.9569804572074224\n",
      "    mean_processing_ms: 0.1877665336065879\n",
      "  time_since_restore: 295.7213041782379\n",
      "  time_this_iter_s: 9.708526134490967\n",
      "  time_total_s: 295.7213041782379\n",
      "  timestamp: 1572985908\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 95a3a56c\n",
      "  \n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-31-56\n",
      "  done: false\n",
      "  episode_len_mean: 194.87\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 194.87\n",
      "  episode_reward_min: 114.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 864\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4713.724\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4667441248893738\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014246500097215176\n",
      "        policy_loss: -0.020314980298280716\n",
      "        total_loss: 325.9002685546875\n",
      "        vf_explained_var: 0.45404842495918274\n",
      "        vf_loss: 325.9141540527344\n",
      "    load_time_ms: 2.617\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 119040\n",
      "    sample_time_ms: 4632.81\n",
      "    update_time_ms: 6.33\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08483626083449952\n",
      "    mean_inference_ms: 0.946819458846762\n",
      "    mean_processing_ms: 0.18564407763433302\n",
      "  time_since_restore: 303.9366993904114\n",
      "  time_this_iter_s: 8.473406076431274\n",
      "  time_total_s: 303.9366993904114\n",
      "  timestamp: 1572985916\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 303 s, 30 iter, 120000 ts, 195 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-32-04\n",
      "  done: false\n",
      "  episode_len_mean: 193.99\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.99\n",
      "  episode_reward_min: 114.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 884\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4608.738\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4497004449367523\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009976673871278763\n",
      "        policy_loss: -0.020545359700918198\n",
      "        total_loss: 302.6678466796875\n",
      "        vf_explained_var: 0.45815303921699524\n",
      "        vf_loss: 302.6839294433594\n",
      "    load_time_ms: 2.452\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 123008\n",
      "    sample_time_ms: 4568.827\n",
      "    update_time_ms: 5.796\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0847112463437753\n",
      "    mean_inference_ms: 0.9438139411619443\n",
      "    mean_processing_ms: 0.1852985033171769\n",
      "  time_since_restore: 311.8167405128479\n",
      "  time_this_iter_s: 7.880041122436523\n",
      "  time_total_s: 311.8167405128479\n",
      "  timestamp: 1572985924\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 311 s, 31 iter, 124000 ts, 194 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-32-12\n",
      "  done: false\n",
      "  episode_len_mean: 195.62\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.62\n",
      "  episode_reward_min: 114.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 904\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4520.423\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4443799555301666\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00959513895213604\n",
      "        policy_loss: -0.018669866025447845\n",
      "        total_loss: 257.7544250488281\n",
      "        vf_explained_var: 0.5774444937705994\n",
      "        vf_loss: 257.7687683105469\n",
      "    load_time_ms: 2.425\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 126976\n",
      "    sample_time_ms: 4539.836\n",
      "    update_time_ms: 5.715\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.084539397564612\n",
      "    mean_inference_ms: 0.9398023415957328\n",
      "    mean_processing_ms: 0.184901054012971\n",
      "  time_since_restore: 319.99391889572144\n",
      "  time_this_iter_s: 8.177178382873535\n",
      "  time_total_s: 319.99391889572144\n",
      "  timestamp: 1572985932\n",
      "  timesteps_since_restore: 128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 319 s, 32 iter, 128000 ts, 196 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-32-21\n",
      "  done: false\n",
      "  episode_len_mean: 197.27\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.27\n",
      "  episode_reward_min: 114.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 924\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4459.992\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4486759305000305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014844480901956558\n",
      "        policy_loss: -0.028189383447170258\n",
      "        total_loss: 183.1259765625\n",
      "        vf_explained_var: 0.6495241522789001\n",
      "        vf_loss: 183.14747619628906\n",
      "    load_time_ms: 2.392\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 130944\n",
      "    sample_time_ms: 4497.994\n",
      "    update_time_ms: 5.516\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08433337229454053\n",
      "    mean_inference_ms: 0.9348825545200979\n",
      "    mean_processing_ms: 0.18445472170096072\n",
      "  time_since_restore: 328.29758167266846\n",
      "  time_this_iter_s: 8.303662776947021\n",
      "  time_total_s: 328.29758167266846\n",
      "  timestamp: 1572985941\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 328 s, 33 iter, 132000 ts, 197 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-32-29\n",
      "  done: false\n",
      "  episode_len_mean: 196.93\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.93\n",
      "  episode_reward_min: 114.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 945\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4397.381\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.43337249755859375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01382717676460743\n",
      "        policy_loss: -0.02713603340089321\n",
      "        total_loss: 319.5330810546875\n",
      "        vf_explained_var: 0.5287578701972961\n",
      "        vf_loss: 319.5539245605469\n",
      "    load_time_ms: 2.192\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 134912\n",
      "    sample_time_ms: 4418.247\n",
      "    update_time_ms: 6.089\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0840900737965093\n",
      "    mean_inference_ms: 0.9292553271733723\n",
      "    mean_processing_ms: 0.18394557742980142\n",
      "  time_since_restore: 336.61987113952637\n",
      "  time_this_iter_s: 8.32228946685791\n",
      "  time_total_s: 336.61987113952637\n",
      "  timestamp: 1572985949\n",
      "  timesteps_since_restore: 136000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 336 s, 34 iter, 136000 ts, 197 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-32-37\n",
      "  done: false\n",
      "  episode_len_mean: 196.67\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.67\n",
      "  episode_reward_min: 134.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 965\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4390.446\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4318889081478119\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012440945021808147\n",
      "        policy_loss: -0.02200685814023018\n",
      "        total_loss: 353.4205017089844\n",
      "        vf_explained_var: 0.4431115984916687\n",
      "        vf_loss: 353.4368896484375\n",
      "    load_time_ms: 2.161\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 138880\n",
      "    sample_time_ms: 4341.759\n",
      "    update_time_ms: 6.099\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0838503251452028\n",
      "    mean_inference_ms: 0.9237039919146082\n",
      "    mean_processing_ms: 0.18345385460067523\n",
      "  time_since_restore: 345.0715916156769\n",
      "  time_this_iter_s: 8.451720476150513\n",
      "  time_total_s: 345.0715916156769\n",
      "  timestamp: 1572985957\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 345 s, 35 iter, 140000 ts, 197 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-32-46\n",
      "  done: false\n",
      "  episode_len_mean: 196.79\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.79\n",
      "  episode_reward_min: 124.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 986\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4320.237\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4329744577407837\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013851892203092575\n",
      "        policy_loss: -0.021392004564404488\n",
      "        total_loss: 298.51788330078125\n",
      "        vf_explained_var: 0.594563901424408\n",
      "        vf_loss: 298.53302001953125\n",
      "    load_time_ms: 2.149\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 142848\n",
      "    sample_time_ms: 4303.279\n",
      "    update_time_ms: 5.782\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08363276415003842\n",
      "    mean_inference_ms: 0.9184483759396288\n",
      "    mean_processing_ms: 0.18301962157197635\n",
      "  time_since_restore: 353.2854416370392\n",
      "  time_this_iter_s: 8.213850021362305\n",
      "  time_total_s: 353.2854416370392\n",
      "  timestamp: 1572985966\n",
      "  timesteps_since_restore: 144000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 353 s, 36 iter, 144000 ts, 197 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-32-54\n",
      "  done: false\n",
      "  episode_len_mean: 196.31\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.31\n",
      "  episode_reward_min: 124.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1006\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4300.095\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.42040473222732544\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014701520092785358\n",
      "        policy_loss: -0.02402576245367527\n",
      "        total_loss: 305.1895751953125\n",
      "        vf_explained_var: 0.5252305865287781\n",
      "        vf_loss: 305.2070007324219\n",
      "    load_time_ms: 2.132\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 146816\n",
      "    sample_time_ms: 4221.245\n",
      "    update_time_ms: 5.548\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08344003288641395\n",
      "    mean_inference_ms: 0.9137032463084546\n",
      "    mean_processing_ms: 0.18262640842173178\n",
      "  time_since_restore: 361.6587519645691\n",
      "  time_this_iter_s: 8.373310327529907\n",
      "  time_total_s: 361.6587519645691\n",
      "  timestamp: 1572985974\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 361 s, 37 iter, 148000 ts, 196 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-33-02\n",
      "  done: false\n",
      "  episode_len_mean: 196.53\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.53\n",
      "  episode_reward_min: 124.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1026\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4195.304\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.40932828187942505\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01484175305813551\n",
      "        policy_loss: -0.025102484971284866\n",
      "        total_loss: 321.51055908203125\n",
      "        vf_explained_var: 0.4352220892906189\n",
      "        vf_loss: 321.5290222167969\n",
      "    load_time_ms: 2.291\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 150784\n",
      "    sample_time_ms: 4122.332\n",
      "    update_time_ms: 5.588\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0832510670304824\n",
      "    mean_inference_ms: 0.9089818528024878\n",
      "    mean_processing_ms: 0.18223039553558062\n",
      "  time_since_restore: 369.531587600708\n",
      "  time_this_iter_s: 7.872835636138916\n",
      "  time_total_s: 369.531587600708\n",
      "  timestamp: 1572985982\n",
      "  timesteps_since_restore: 152000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 369 s, 38 iter, 152000 ts, 197 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-33-10\n",
      "  done: false\n",
      "  episode_len_mean: 191.94\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.94\n",
      "  episode_reward_min: 41.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1049\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4149.088\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.40209102630615234\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012034050188958645\n",
      "        policy_loss: -0.02392585203051567\n",
      "        total_loss: 394.1364440917969\n",
      "        vf_explained_var: 0.3957706093788147\n",
      "        vf_loss: 394.1549987792969\n",
      "    load_time_ms: 2.307\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 154752\n",
      "    sample_time_ms: 4070.667\n",
      "    update_time_ms: 5.677\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08306356498102532\n",
      "    mean_inference_ms: 0.9040004161460712\n",
      "    mean_processing_ms: 0.18182620666448876\n",
      "  time_since_restore: 377.8160002231598\n",
      "  time_this_iter_s: 8.284412622451782\n",
      "  time_total_s: 377.8160002231598\n",
      "  timestamp: 1572985990\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 377 s, 39 iter, 156000 ts, 192 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-33-18\n",
      "  done: false\n",
      "  episode_len_mean: 192.36\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.36\n",
      "  episode_reward_min: 41.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1069\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4115.886\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.40130454301834106\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013099879957735538\n",
      "        policy_loss: -0.01992391049861908\n",
      "        total_loss: 249.99050903320312\n",
      "        vf_explained_var: 0.5048967003822327\n",
      "        vf_loss: 250.00453186035156\n",
      "    load_time_ms: 2.257\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 158720\n",
      "    sample_time_ms: 4052.882\n",
      "    update_time_ms: 5.297\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08291717112305878\n",
      "    mean_inference_ms: 0.9001285380774263\n",
      "    mean_processing_ms: 0.181509434572355\n",
      "  time_since_restore: 385.7739906311035\n",
      "  time_this_iter_s: 7.957990407943726\n",
      "  time_total_s: 385.7739906311035\n",
      "  timestamp: 1572985998\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 385 s, 40 iter, 160000 ts, 192 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-33-26\n",
      "  done: false\n",
      "  episode_len_mean: 192.84\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.84\n",
      "  episode_reward_min: 41.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1089\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4113.683\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4180697202682495\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013253063894808292\n",
      "        policy_loss: -0.017836539074778557\n",
      "        total_loss: 318.9594421386719\n",
      "        vf_explained_var: 0.49078232049942017\n",
      "        vf_loss: 318.9713439941406\n",
      "    load_time_ms: 2.211\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 162688\n",
      "    sample_time_ms: 4070.878\n",
      "    update_time_ms: 5.239\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0827719469662637\n",
      "    mean_inference_ms: 0.8964617307940085\n",
      "    mean_processing_ms: 0.18118136827299447\n",
      "  time_since_restore: 393.8117301464081\n",
      "  time_this_iter_s: 8.037739515304565\n",
      "  time_total_s: 393.8117301464081\n",
      "  timestamp: 1572986006\n",
      "  timesteps_since_restore: 164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 393 s, 41 iter, 164000 ts, 193 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-33-34\n",
      "  done: false\n",
      "  episode_len_mean: 191.76\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.76\n",
      "  episode_reward_min: 41.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1110\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4111.542\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.42283713817596436\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012332350015640259\n",
      "        policy_loss: -0.020792318508028984\n",
      "        total_loss: 274.0472717285156\n",
      "        vf_explained_var: 0.542161762714386\n",
      "        vf_loss: 274.0625\n",
      "    load_time_ms: 2.216\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 166656\n",
      "    sample_time_ms: 4061.223\n",
      "    update_time_ms: 5.259\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08261299803391368\n",
      "    mean_inference_ms: 0.8927940388990766\n",
      "    mean_processing_ms: 0.18084924285536308\n",
      "  time_since_restore: 401.86852502822876\n",
      "  time_this_iter_s: 8.056794881820679\n",
      "  time_total_s: 401.86852502822876\n",
      "  timestamp: 1572986014\n",
      "  timesteps_since_restore: 168000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 401 s, 42 iter, 168000 ts, 192 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-33-42\n",
      "  done: false\n",
      "  episode_len_mean: 186.5\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 186.5\n",
      "  episode_reward_min: 41.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1133\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4100.841\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4155823290348053\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014477997086942196\n",
      "        policy_loss: -0.027155838906764984\n",
      "        total_loss: 190.0409698486328\n",
      "        vf_explained_var: 0.6691969633102417\n",
      "        vf_loss: 190.0615997314453\n",
      "    load_time_ms: 2.213\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 170624\n",
      "    sample_time_ms: 4036.555\n",
      "    update_time_ms: 5.365\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08244049524829773\n",
      "    mean_inference_ms: 0.8890395506927036\n",
      "    mean_processing_ms: 0.1804832486123091\n",
      "  time_since_restore: 409.81059646606445\n",
      "  time_this_iter_s: 7.942071437835693\n",
      "  time_total_s: 409.81059646606445\n",
      "  timestamp: 1572986022\n",
      "  timesteps_since_restore: 172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 409 s, 43 iter, 172000 ts, 186 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-33-50\n",
      "  done: false\n",
      "  episode_len_mean: 191.14\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.14\n",
      "  episode_reward_min: 44.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1153\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4079.004\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4187871813774109\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011887425556778908\n",
      "        policy_loss: -0.019022878259420395\n",
      "        total_loss: 218.2142791748047\n",
      "        vf_explained_var: 0.7181507349014282\n",
      "        vf_loss: 218.22793579101562\n",
      "    load_time_ms: 2.21\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 174592\n",
      "    sample_time_ms: 4043.754\n",
      "    update_time_ms: 4.172\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08229369861509689\n",
      "    mean_inference_ms: 0.88584819843989\n",
      "    mean_processing_ms: 0.18018240654568046\n",
      "  time_since_restore: 417.97279715538025\n",
      "  time_this_iter_s: 8.162200689315796\n",
      "  time_total_s: 417.97279715538025\n",
      "  timestamp: 1572986030\n",
      "  timesteps_since_restore: 176000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 417 s, 44 iter, 176000 ts, 191 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-33-59\n",
      "  done: false\n",
      "  episode_len_mean: 188.04\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 188.04\n",
      "  episode_reward_min: 44.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1175\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4053.595\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.41501566767692566\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014402967877686024\n",
      "        policy_loss: -0.026167288422584534\n",
      "        total_loss: 381.5533142089844\n",
      "        vf_explained_var: 0.45325765013694763\n",
      "        vf_loss: 381.5730895996094\n",
      "    load_time_ms: 2.233\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 178560\n",
      "    sample_time_ms: 4057.147\n",
      "    update_time_ms: 4.296\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08214314453312906\n",
      "    mean_inference_ms: 0.8824956144883522\n",
      "    mean_processing_ms: 0.1798665287066136\n",
      "  time_since_restore: 426.3062033653259\n",
      "  time_this_iter_s: 8.333406209945679\n",
      "  time_total_s: 426.3062033653259\n",
      "  timestamp: 1572986039\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 426 s, 45 iter, 180000 ts, 188 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-34-07\n",
      "  done: false\n",
      "  episode_len_mean: 186.88\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 186.88\n",
      "  episode_reward_min: 50.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1197\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4067.308\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.40772342681884766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015233608894050121\n",
      "        policy_loss: -0.026318004354834557\n",
      "        total_loss: 219.25015258789062\n",
      "        vf_explained_var: 0.6713211536407471\n",
      "        vf_loss: 219.2696075439453\n",
      "    load_time_ms: 2.11\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 182528\n",
      "    sample_time_ms: 4041.962\n",
      "    update_time_ms: 4.421\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08199938804939962\n",
      "    mean_inference_ms: 0.8791701661668972\n",
      "    mean_processing_ms: 0.1795624770186803\n",
      "  time_since_restore: 434.5040225982666\n",
      "  time_this_iter_s: 8.197819232940674\n",
      "  time_total_s: 434.5040225982666\n",
      "  timestamp: 1572986047\n",
      "  timesteps_since_restore: 184000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 434 s, 46 iter, 184000 ts, 187 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-34-15\n",
      "  done: false\n",
      "  episode_len_mean: 184.67\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 184.67\n",
      "  episode_reward_min: 36.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1220\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4023.691\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4272986650466919\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015398195013403893\n",
      "        policy_loss: -0.029197953641414642\n",
      "        total_loss: 269.4647216796875\n",
      "        vf_explained_var: 0.562201976776123\n",
      "        vf_loss: 269.48699951171875\n",
      "    load_time_ms: 2.106\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 186496\n",
      "    sample_time_ms: 4051.908\n",
      "    update_time_ms: 4.252\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08187766856085325\n",
      "    mean_inference_ms: 0.8761474265422885\n",
      "    mean_processing_ms: 0.1793010023959338\n",
      "  time_since_restore: 442.5395200252533\n",
      "  time_this_iter_s: 8.035497426986694\n",
      "  time_total_s: 442.5395200252533\n",
      "  timestamp: 1572986055\n",
      "  timesteps_since_restore: 188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 442 s, 47 iter, 188000 ts, 185 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-34-23\n",
      "  done: false\n",
      "  episode_len_mean: 182.99\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 182.99\n",
      "  episode_reward_min: 36.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1242\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4061.584\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4015154242515564\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014597385190427303\n",
      "        policy_loss: -0.029538961127400398\n",
      "        total_loss: 320.3030700683594\n",
      "        vf_explained_var: 0.49974820017814636\n",
      "        vf_loss: 320.3260803222656\n",
      "    load_time_ms: 1.949\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 190464\n",
      "    sample_time_ms: 4059.903\n",
      "    update_time_ms: 4.13\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08176236632602957\n",
      "    mean_inference_ms: 0.8734007186600323\n",
      "    mean_processing_ms: 0.17906158728326715\n",
      "  time_since_restore: 450.8686661720276\n",
      "  time_this_iter_s: 8.329146146774292\n",
      "  time_total_s: 450.8686661720276\n",
      "  timestamp: 1572986063\n",
      "  timesteps_since_restore: 192000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 450 s, 48 iter, 192000 ts, 183 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-34-32\n",
      "  done: false\n",
      "  episode_len_mean: 184.43\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 184.43\n",
      "  episode_reward_min: 36.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1262\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4052.689\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.410075843334198\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015736162662506104\n",
      "        policy_loss: -0.026385625824332237\n",
      "        total_loss: 237.15345764160156\n",
      "        vf_explained_var: 0.5809234976768494\n",
      "        vf_loss: 237.1727752685547\n",
      "    load_time_ms: 1.915\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 194432\n",
      "    sample_time_ms: 4070.268\n",
      "    update_time_ms: 4.094\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08166735938981903\n",
      "    mean_inference_ms: 0.8711150730281108\n",
      "    mean_processing_ms: 0.17886377670808495\n",
      "  time_since_restore: 459.1679439544678\n",
      "  time_this_iter_s: 8.299277782440186\n",
      "  time_total_s: 459.1679439544678\n",
      "  timestamp: 1572986072\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 459 s, 49 iter, 196000 ts, 184 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-34-40\n",
      "  done: false\n",
      "  episode_len_mean: 185.64\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 185.64\n",
      "  episode_reward_min: 36.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1282\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4092.335\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4032042622566223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013033447787165642\n",
      "        policy_loss: -0.024005498737096786\n",
      "        total_loss: 179.32669067382812\n",
      "        vf_explained_var: 0.6801775693893433\n",
      "        vf_loss: 179.3448028564453\n",
      "    load_time_ms: 1.95\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 198400\n",
      "    sample_time_ms: 4080.258\n",
      "    update_time_ms: 4.114\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0815917177421845\n",
      "    mean_inference_ms: 0.8690520315380362\n",
      "    mean_processing_ms: 0.1787045213852463\n",
      "  time_since_restore: 467.6229054927826\n",
      "  time_this_iter_s: 8.45496153831482\n",
      "  time_total_s: 467.6229054927826\n",
      "  timestamp: 1572986080\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 467 s, 50 iter, 200000 ts, 186 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-34-49\n",
      "  done: false\n",
      "  episode_len_mean: 189.61\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 189.61\n",
      "  episode_reward_min: 36.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1303\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4115.16\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.41257384419441223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014652037061750889\n",
      "        policy_loss: -0.027567334473133087\n",
      "        total_loss: 191.25230407714844\n",
      "        vf_explained_var: 0.7311077117919922\n",
      "        vf_loss: 191.2732391357422\n",
      "    load_time_ms: 1.999\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 202368\n",
      "    sample_time_ms: 4127.359\n",
      "    update_time_ms: 4.202\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08155237339270938\n",
      "    mean_inference_ms: 0.8673821798287557\n",
      "    mean_processing_ms: 0.17862344089366192\n",
      "  time_since_restore: 476.3626844882965\n",
      "  time_this_iter_s: 8.739778995513916\n",
      "  time_total_s: 476.3626844882965\n",
      "  timestamp: 1572986089\n",
      "  timesteps_since_restore: 204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 476 s, 51 iter, 204000 ts, 190 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-34-58\n",
      "  done: false\n",
      "  episode_len_mean: 191.68\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.68\n",
      "  episode_reward_min: 44.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1324\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4128.058\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.408285915851593\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017283260822296143\n",
      "        policy_loss: -0.02629941515624523\n",
      "        total_loss: 180.01470947265625\n",
      "        vf_explained_var: 0.7610281705856323\n",
      "        vf_loss: 180.0332489013672\n",
      "    load_time_ms: 2.003\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 206336\n",
      "    sample_time_ms: 4180.089\n",
      "    update_time_ms: 4.21\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08154158429828907\n",
      "    mean_inference_ms: 0.866059187479712\n",
      "    mean_processing_ms: 0.17858689417555496\n",
      "  time_since_restore: 485.0765082836151\n",
      "  time_this_iter_s: 8.713823795318604\n",
      "  time_total_s: 485.0765082836151\n",
      "  timestamp: 1572986098\n",
      "  timesteps_since_restore: 208000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 485 s, 52 iter, 208000 ts, 192 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-35-06\n",
      "  done: false\n",
      "  episode_len_mean: 195.21\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.21\n",
      "  episode_reward_min: 44.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1344\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4106.58\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4068296253681183\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013049785979092121\n",
      "        policy_loss: -0.023754872381687164\n",
      "        total_loss: 213.27772521972656\n",
      "        vf_explained_var: 0.6746631264686584\n",
      "        vf_loss: 213.2956085205078\n",
      "    load_time_ms: 1.985\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 210304\n",
      "    sample_time_ms: 4219.058\n",
      "    update_time_ms: 4.249\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08154946374377667\n",
      "    mean_inference_ms: 0.8650191881136045\n",
      "    mean_processing_ms: 0.17857805022017442\n",
      "  time_since_restore: 493.20127868652344\n",
      "  time_this_iter_s: 8.124770402908325\n",
      "  time_total_s: 493.20127868652344\n",
      "  timestamp: 1572986106\n",
      "  timesteps_since_restore: 212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 493 s, 53 iter, 212000 ts, 195 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-35-15\n",
      "  done: false\n",
      "  episode_len_mean: 191.92\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.92\n",
      "  episode_reward_min: 44.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1366\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4170.045\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.40191489458084106\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015797492116689682\n",
      "        policy_loss: -0.022460905835032463\n",
      "        total_loss: 276.4967346191406\n",
      "        vf_explained_var: 0.5477262735366821\n",
      "        vf_loss: 276.5120849609375\n",
      "    load_time_ms: 2.077\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 214272\n",
      "    sample_time_ms: 4224.387\n",
      "    update_time_ms: 4.868\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08155570581824781\n",
      "    mean_inference_ms: 0.8639261246364265\n",
      "    mean_processing_ms: 0.17855953796731774\n",
      "  time_since_restore: 502.0589916706085\n",
      "  time_this_iter_s: 8.857712984085083\n",
      "  time_total_s: 502.0589916706085\n",
      "  timestamp: 1572986115\n",
      "  timesteps_since_restore: 216000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 502 s, 54 iter, 216000 ts, 192 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-35-22\n",
      "  done: false\n",
      "  episode_len_mean: 188.07\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 188.07\n",
      "  episode_reward_min: 44.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1388\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4161.983\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4050382375717163\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012220004573464394\n",
      "        policy_loss: -0.02560381218791008\n",
      "        total_loss: 181.48703002929688\n",
      "        vf_explained_var: 0.7316744327545166\n",
      "        vf_loss: 181.5071258544922\n",
      "    load_time_ms: 2.072\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 218240\n",
      "    sample_time_ms: 4182.059\n",
      "    update_time_ms: 4.685\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08151151656263336\n",
      "    mean_inference_ms: 0.8624012419771414\n",
      "    mean_processing_ms: 0.17845289895678043\n",
      "  time_since_restore: 509.88983058929443\n",
      "  time_this_iter_s: 7.830838918685913\n",
      "  time_total_s: 509.88983058929443\n",
      "  timestamp: 1572986122\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 509 s, 55 iter, 220000 ts, 188 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-35-31\n",
      "  done: false\n",
      "  episode_len_mean: 188.49\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 188.49\n",
      "  episode_reward_min: 29.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1410\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4173.033\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4210829436779022\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016772110015153885\n",
      "        policy_loss: -0.03319539874792099\n",
      "        total_loss: 275.05963134765625\n",
      "        vf_explained_var: 0.6089757084846497\n",
      "        vf_loss: 275.0852966308594\n",
      "    load_time_ms: 2.024\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 222208\n",
      "    sample_time_ms: 4163.098\n",
      "    update_time_ms: 4.62\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08140956989940051\n",
      "    mean_inference_ms: 0.860244055106356\n",
      "    mean_processing_ms: 0.17824772146124473\n",
      "  time_since_restore: 518.0094227790833\n",
      "  time_this_iter_s: 8.119592189788818\n",
      "  time_total_s: 518.0094227790833\n",
      "  timestamp: 1572986131\n",
      "  timesteps_since_restore: 224000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 518 s, 56 iter, 224000 ts, 188 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-35-39\n",
      "  done: false\n",
      "  episode_len_mean: 186.46\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 186.46\n",
      "  episode_reward_min: 29.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1431\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4230.019\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4051172137260437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014897383749485016\n",
      "        policy_loss: -0.028724925592541695\n",
      "        total_loss: 180.54061889648438\n",
      "        vf_explained_var: 0.7131003141403198\n",
      "        vf_loss: 180.5626220703125\n",
      "    load_time_ms: 2.023\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 226176\n",
      "    sample_time_ms: 4111.2\n",
      "    update_time_ms: 5.358\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0812739039663925\n",
      "    mean_inference_ms: 0.8576432676018918\n",
      "    mean_processing_ms: 0.17798150458016937\n",
      "  time_since_restore: 526.1031520366669\n",
      "  time_this_iter_s: 8.093729257583618\n",
      "  time_total_s: 526.1031520366669\n",
      "  timestamp: 1572986139\n",
      "  timesteps_since_restore: 228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 526 s, 57 iter, 228000 ts, 186 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-35-47\n",
      "  done: false\n",
      "  episode_len_mean: 187.16\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 187.16\n",
      "  episode_reward_min: 29.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1451\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4247.405\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.39875030517578125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015245331451296806\n",
      "        policy_loss: -0.025353359058499336\n",
      "        total_loss: 207.84007263183594\n",
      "        vf_explained_var: 0.682187020778656\n",
      "        vf_loss: 207.85855102539062\n",
      "    load_time_ms: 1.974\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 230144\n",
      "    sample_time_ms: 4088.932\n",
      "    update_time_ms: 5.39\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08112456810556015\n",
      "    mean_inference_ms: 0.8549101072528952\n",
      "    mean_processing_ms: 0.17769856369349943\n",
      "  time_since_restore: 534.3833832740784\n",
      "  time_this_iter_s: 8.280231237411499\n",
      "  time_total_s: 534.3833832740784\n",
      "  timestamp: 1572986147\n",
      "  timesteps_since_restore: 232000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 534 s, 58 iter, 232000 ts, 187 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-35-55\n",
      "  done: false\n",
      "  episode_len_mean: 189.29\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 189.29\n",
      "  episode_reward_min: 29.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1472\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4272.666\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.39225631952285767\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015733974054455757\n",
      "        policy_loss: -0.02738039568066597\n",
      "        total_loss: 223.9193115234375\n",
      "        vf_explained_var: 0.6407338976860046\n",
      "        vf_loss: 223.93959045410156\n",
      "    load_time_ms: 1.953\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 234112\n",
      "    sample_time_ms: 4054.434\n",
      "    update_time_ms: 5.329\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0809635501613298\n",
      "    mean_inference_ms: 0.851978580786986\n",
      "    mean_processing_ms: 0.17740060049921014\n",
      "  time_since_restore: 542.5895738601685\n",
      "  time_this_iter_s: 8.206190586090088\n",
      "  time_total_s: 542.5895738601685\n",
      "  timestamp: 1572986155\n",
      "  timesteps_since_restore: 236000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 542 s, 59 iter, 236000 ts, 189 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-36-04\n",
      "  done: false\n",
      "  episode_len_mean: 191.0\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.0\n",
      "  episode_reward_min: 29.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1493\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4302.091\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.3907185196876526\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013253171928226948\n",
      "        policy_loss: -0.026231344789266586\n",
      "        total_loss: 187.36705017089844\n",
      "        vf_explained_var: 0.6712178587913513\n",
      "        vf_loss: 187.38731384277344\n",
      "    load_time_ms: 1.938\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 238080\n",
      "    sample_time_ms: 4040.748\n",
      "    update_time_ms: 5.421\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08083715916236973\n",
      "    mean_inference_ms: 0.849428600854206\n",
      "    mean_processing_ms: 0.1771493476424361\n",
      "  time_since_restore: 551.2024757862091\n",
      "  time_this_iter_s: 8.61290192604065\n",
      "  time_total_s: 551.2024757862091\n",
      "  timestamp: 1572986164\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 551 s, 60 iter, 240000 ts, 191 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-36-12\n",
      "  done: false\n",
      "  episode_len_mean: 192.09\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 192.09\n",
      "  episode_reward_min: 63.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1514\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4294.819\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.39060455560684204\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015492845326662064\n",
      "        policy_loss: -0.021882586181163788\n",
      "        total_loss: 243.65550231933594\n",
      "        vf_explained_var: 0.5968085527420044\n",
      "        vf_loss: 243.67039489746094\n",
      "    load_time_ms: 1.969\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 242048\n",
      "    sample_time_ms: 4021.417\n",
      "    update_time_ms: 5.73\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08074784932993434\n",
      "    mean_inference_ms: 0.8474048273734951\n",
      "    mean_processing_ms: 0.17695694240468074\n",
      "  time_since_restore: 559.6789054870605\n",
      "  time_this_iter_s: 8.47642970085144\n",
      "  time_total_s: 559.6789054870605\n",
      "  timestamp: 1572986172\n",
      "  timesteps_since_restore: 244000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.5/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 559 s, 61 iter, 244000 ts, 192 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 190.65\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 190.65\n",
      "  episode_reward_min: 70.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1536\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4303.826\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.39428094029426575\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01575721986591816\n",
      "        policy_loss: -0.02373390644788742\n",
      "        total_loss: 295.11224365234375\n",
      "        vf_explained_var: 0.5348296165466309\n",
      "        vf_loss: 295.1289367675781\n",
      "    load_time_ms: 1.984\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 246016\n",
      "    sample_time_ms: 4011.84\n",
      "    update_time_ms: 5.609\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08069834870671443\n",
      "    mean_inference_ms: 0.845940622588464\n",
      "    mean_processing_ms: 0.17684028518537695\n",
      "  time_since_restore: 568.4028527736664\n",
      "  time_this_iter_s: 8.723947286605835\n",
      "  time_total_s: 568.4028527736664\n",
      "  timestamp: 1572986181\n",
      "  timesteps_since_restore: 248000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 11.8/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 568 s, 62 iter, 248000 ts, 191 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-36-29\n",
      "  done: false\n",
      "  episode_len_mean: 187.83\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 187.83\n",
      "  episode_reward_min: 70.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1557\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4309.083\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.3882676661014557\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014169353991746902\n",
      "        policy_loss: -0.026168856769800186\n",
      "        total_loss: 280.1837463378906\n",
      "        vf_explained_var: 0.582373321056366\n",
      "        vf_loss: 280.20355224609375\n",
      "    load_time_ms: 2.036\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 249984\n",
      "    sample_time_ms: 3999.27\n",
      "    update_time_ms: 6.218\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08067328394509135\n",
      "    mean_inference_ms: 0.8448812803901167\n",
      "    mean_processing_ms: 0.1767662196913134\n",
      "  time_since_restore: 576.4546184539795\n",
      "  time_this_iter_s: 8.05176568031311\n",
      "  time_total_s: 576.4546184539795\n",
      "  timestamp: 1572986189\n",
      "  timesteps_since_restore: 252000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 576 s, 63 iter, 252000 ts, 188 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-36-38\n",
      "  done: false\n",
      "  episode_len_mean: 187.63\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 187.63\n",
      "  episode_reward_min: 70.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1578\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4247.303\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.38702312111854553\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01620194874703884\n",
      "        policy_loss: -0.02357584610581398\n",
      "        total_loss: 266.516845703125\n",
      "        vf_explained_var: 0.5705113410949707\n",
      "        vf_loss: 266.53314208984375\n",
      "    load_time_ms: 1.997\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 253952\n",
      "    sample_time_ms: 4014.775\n",
      "    update_time_ms: 5.639\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08066835200076185\n",
      "    mean_inference_ms: 0.8441268585079797\n",
      "    mean_processing_ms: 0.17672471798046308\n",
      "  time_since_restore: 584.8438384532928\n",
      "  time_this_iter_s: 8.389219999313354\n",
      "  time_total_s: 584.8438384532928\n",
      "  timestamp: 1572986198\n",
      "  timesteps_since_restore: 256000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 584 s, 64 iter, 256000 ts, 188 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-36-47\n",
      "  done: false\n",
      "  episode_len_mean: 189.12\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 189.12\n",
      "  episode_reward_min: 70.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1598\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4287.464\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.3801359534263611\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013088351115584373\n",
      "        policy_loss: -0.02559269592165947\n",
      "        total_loss: 260.3595275878906\n",
      "        vf_explained_var: 0.5999523401260376\n",
      "        vf_loss: 260.37921142578125\n",
      "    load_time_ms: 2.055\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 257920\n",
      "    sample_time_ms: 4082.169\n",
      "    update_time_ms: 5.844\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08066802062185091\n",
      "    mean_inference_ms: 0.8435380848795583\n",
      "    mean_processing_ms: 0.17670177793235994\n",
      "  time_since_restore: 593.7605810165405\n",
      "  time_this_iter_s: 8.91674256324768\n",
      "  time_total_s: 593.7605810165405\n",
      "  timestamp: 1572986207\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 593 s, 65 iter, 260000 ts, 189 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-36-55\n",
      "  done: false\n",
      "  episode_len_mean: 191.13\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 191.13\n",
      "  episode_reward_min: 70.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1619\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4307.283\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.3937402367591858\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013536515645682812\n",
      "        policy_loss: -0.023029932752251625\n",
      "        total_loss: 326.4122619628906\n",
      "        vf_explained_var: 0.5697389245033264\n",
      "        vf_loss: 326.4291687011719\n",
      "    load_time_ms: 2.054\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 261888\n",
      "    sample_time_ms: 4132.396\n",
      "    update_time_ms: 6.047\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08065856860306818\n",
      "    mean_inference_ms: 0.8428188002903599\n",
      "    mean_processing_ms: 0.17666823124169895\n",
      "  time_since_restore: 602.5845806598663\n",
      "  time_this_iter_s: 8.823999643325806\n",
      "  time_total_s: 602.5845806598663\n",
      "  timestamp: 1572986215\n",
      "  timesteps_since_restore: 264000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 602 s, 66 iter, 264000 ts, 191 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-37-03\n",
      "  done: false\n",
      "  episode_len_mean: 193.7\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 193.7\n",
      "  episode_reward_min: 100.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1639\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4250.651\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.39889994263648987\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015299915336072445\n",
      "        policy_loss: -0.022341890260577202\n",
      "        total_loss: 332.53729248046875\n",
      "        vf_explained_var: 0.4735816419124603\n",
      "        vf_loss: 332.55279541015625\n",
      "    load_time_ms: 2.081\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 265856\n",
      "    sample_time_ms: 4160.305\n",
      "    update_time_ms: 5.42\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08062982860257664\n",
      "    mean_inference_ms: 0.8418851230940695\n",
      "    mean_processing_ms: 0.1765927309167521\n",
      "  time_since_restore: 610.4003694057465\n",
      "  time_this_iter_s: 7.815788745880127\n",
      "  time_total_s: 610.4003694057465\n",
      "  timestamp: 1572986223\n",
      "  timesteps_since_restore: 268000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 610 s, 67 iter, 268000 ts, 194 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-37-12\n",
      "  done: false\n",
      "  episode_len_mean: 196.07\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.07\n",
      "  episode_reward_min: 117.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1659\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4241.184\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.4105580151081085\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013995690271258354\n",
      "        policy_loss: -0.025933681055903435\n",
      "        total_loss: 280.0565490722656\n",
      "        vf_explained_var: 0.6054999828338623\n",
      "        vf_loss: 280.076171875\n",
      "    load_time_ms: 2.086\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 269824\n",
      "    sample_time_ms: 4179.486\n",
      "    update_time_ms: 6.153\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08058979255173324\n",
      "    mean_inference_ms: 0.8408662255339078\n",
      "    mean_processing_ms: 0.17649193990954706\n",
      "  time_since_restore: 618.785263299942\n",
      "  time_this_iter_s: 8.384893894195557\n",
      "  time_total_s: 618.785263299942\n",
      "  timestamp: 1572986232\n",
      "  timesteps_since_restore: 272000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 618 s, 68 iter, 272000 ts, 196 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-37-20\n",
      "  done: false\n",
      "  episode_len_mean: 195.88\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.88\n",
      "  episode_reward_min: 79.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1680\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4271.183\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.3917136490345001\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013871840201318264\n",
      "        policy_loss: -0.02698969841003418\n",
      "        total_loss: 298.0992736816406\n",
      "        vf_explained_var: 0.532955527305603\n",
      "        vf_loss: 298.1199645996094\n",
      "    load_time_ms: 2.098\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 273792\n",
      "    sample_time_ms: 4159.291\n",
      "    update_time_ms: 6.183\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08051043885940158\n",
      "    mean_inference_ms: 0.83942329618212\n",
      "    mean_processing_ms: 0.1763307214914805\n",
      "  time_since_restore: 627.0897951126099\n",
      "  time_this_iter_s: 8.304531812667847\n",
      "  time_total_s: 627.0897951126099\n",
      "  timestamp: 1572986240\n",
      "  timesteps_since_restore: 276000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 627 s, 69 iter, 276000 ts, 196 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-37-28\n",
      "  done: false\n",
      "  episode_len_mean: 196.44\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 196.44\n",
      "  episode_reward_min: 79.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1700\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4265.335\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.37860092520713806\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01090900506824255\n",
      "        policy_loss: -0.019930077716708183\n",
      "        total_loss: 276.89349365234375\n",
      "        vf_explained_var: 0.48478567600250244\n",
      "        vf_loss: 276.9085388183594\n",
      "    load_time_ms: 2.17\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 277760\n",
      "    sample_time_ms: 4159.342\n",
      "    update_time_ms: 6.117\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08043108100227801\n",
      "    mean_inference_ms: 0.8379429434925268\n",
      "    mean_processing_ms: 0.17616792870436157\n",
      "  time_since_restore: 635.6445379257202\n",
      "  time_this_iter_s: 8.554742813110352\n",
      "  time_total_s: 635.6445379257202\n",
      "  timestamp: 1572986248\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 635 s, 70 iter, 280000 ts, 196 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-37-36\n",
      "  done: false\n",
      "  episode_len_mean: 197.5\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.5\n",
      "  episode_reward_min: 79.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1720\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4219.163\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.389227032661438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013017114251852036\n",
      "        policy_loss: -0.02298463135957718\n",
      "        total_loss: 245.60401916503906\n",
      "        vf_explained_var: 0.522537112236023\n",
      "        vf_loss: 245.6211395263672\n",
      "    load_time_ms: 2.156\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 281728\n",
      "    sample_time_ms: 4140.187\n",
      "    update_time_ms: 5.707\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08035595165491943\n",
      "    mean_inference_ms: 0.8364713233111736\n",
      "    mean_processing_ms: 0.17599790759081038\n",
      "  time_since_restore: 643.4624881744385\n",
      "  time_this_iter_s: 7.817950248718262\n",
      "  time_total_s: 643.4624881744385\n",
      "  timestamp: 1572986256\n",
      "  timesteps_since_restore: 284000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 643 s, 71 iter, 284000 ts, 198 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-37-44\n",
      "  done: false\n",
      "  episode_len_mean: 197.88\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 197.88\n",
      "  episode_reward_min: 79.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1740\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4197.864\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.3757779896259308\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014827958308160305\n",
      "        policy_loss: -0.023216132074594498\n",
      "        total_loss: 332.60101318359375\n",
      "        vf_explained_var: 0.40943536162376404\n",
      "        vf_loss: 332.6175231933594\n",
      "    load_time_ms: 2.225\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 285696\n",
      "    sample_time_ms: 4108.627\n",
      "    update_time_ms: 5.787\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08028846389163193\n",
      "    mean_inference_ms: 0.8351358338910113\n",
      "    mean_processing_ms: 0.1758494680482969\n",
      "  time_since_restore: 651.6406719684601\n",
      "  time_this_iter_s: 8.178183794021606\n",
      "  time_total_s: 651.6406719684601\n",
      "  timestamp: 1572986264\n",
      "  timesteps_since_restore: 288000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 651 s, 72 iter, 288000 ts, 198 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-37-53\n",
      "  done: false\n",
      "  episode_len_mean: 198.39\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.39\n",
      "  episode_reward_min: 79.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1760\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4214.422\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.3927290141582489\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016598330810666084\n",
      "        policy_loss: -0.027934718877077103\n",
      "        total_loss: 290.7284240722656\n",
      "        vf_explained_var: 0.44431766867637634\n",
      "        vf_loss: 290.7489013671875\n",
      "    load_time_ms: 2.247\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 289664\n",
      "    sample_time_ms: 4092.617\n",
      "    update_time_ms: 5.097\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0802263832753457\n",
      "    mean_inference_ms: 0.8338520257479182\n",
      "    mean_processing_ms: 0.17570963738935352\n",
      "  time_since_restore: 659.6911017894745\n",
      "  time_this_iter_s: 8.050429821014404\n",
      "  time_total_s: 659.6911017894745\n",
      "  timestamp: 1572986273\n",
      "  timesteps_since_restore: 292000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 659 s, 73 iter, 292000 ts, 198 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-38-00\n",
      "  done: false\n",
      "  episode_len_mean: 199.65\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.65\n",
      "  episode_reward_min: 171.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1780\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4214.077\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.38025373220443726\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01439874991774559\n",
      "        policy_loss: -0.025017809122800827\n",
      "        total_loss: 255.76905822753906\n",
      "        vf_explained_var: 0.5420412421226501\n",
      "        vf_loss: 255.78761291503906\n",
      "    load_time_ms: 2.269\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 293632\n",
      "    sample_time_ms: 4030.728\n",
      "    update_time_ms: 5.088\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08016764565437386\n",
      "    mean_inference_ms: 0.8326140063964772\n",
      "    mean_processing_ms: 0.17557484383093677\n",
      "  time_since_restore: 667.458419084549\n",
      "  time_this_iter_s: 7.767317295074463\n",
      "  time_total_s: 667.458419084549\n",
      "  timestamp: 1572986280\n",
      "  timesteps_since_restore: 296000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 667 s, 74 iter, 296000 ts, 200 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-38-08\n",
      "  done: false\n",
      "  episode_len_mean: 199.94\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.94\n",
      "  episode_reward_min: 194.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1800\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4128.449\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.3831712305545807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014742747880518436\n",
      "        policy_loss: -0.023326758295297623\n",
      "        total_loss: 262.13336181640625\n",
      "        vf_explained_var: 0.513116717338562\n",
      "        vf_loss: 262.1500549316406\n",
      "    load_time_ms: 2.216\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 297600\n",
      "    sample_time_ms: 4012.072\n",
      "    update_time_ms: 4.85\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0801078165462364\n",
      "    mean_inference_ms: 0.8314353204405203\n",
      "    mean_processing_ms: 0.1754384977117477\n",
      "  time_since_restore: 675.3200931549072\n",
      "  time_this_iter_s: 7.861674070358276\n",
      "  time_total_s: 675.3200931549072\n",
      "  timestamp: 1572986288\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 2, 'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tRUNNING, [2 CPUs, 0 GPUs], [pid=32084], 675 s, 75 iter, 300000 ts, 200 rew\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-05 12:38:17,099\tINFO tune.py:274 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n",
      "2019-11-05 12:38:17,101\tWARNING experiment_analysis.py:34 -- pandas not installed. Run `pip install pandas` for Analysis utilities.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_1_lr=0.001:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-05_12-38-17\n",
      "  done: true\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 200.0\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1820\n",
      "  experiment_id: e747fd86955240deb52c88c92b47a82f\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 4090.253\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        entropy: 0.37202951312065125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012737389653921127\n",
      "        policy_loss: -0.02445712499320507\n",
      "        total_loss: 335.3049621582031\n",
      "        vf_explained_var: 0.4215317666530609\n",
      "        vf_loss: 335.32366943359375\n",
      "    load_time_ms: 2.269\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 301568\n",
      "    sample_time_ms: 4002.596\n",
      "    update_time_ms: 4.896\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 32084\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08004309638731572\n",
      "    mean_inference_ms: 0.8302493249183042\n",
      "    mean_processing_ms: 0.17531005810048828\n",
      "  time_since_restore: 683.6650750637054\n",
      "  time_this_iter_s: 8.344981908798218\n",
      "  time_total_s: 683.6650750637054\n",
      "  timestamp: 1572986297\n",
      "  timesteps_since_restore: 304000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: 95a3a56d\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 3})\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32084], 683 s, 76 iter, 304000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/3.96 GiB heap, 0.0/1.37 GiB objects\n",
      "Memory usage on this node: 12.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 3 ({'TERMINATED': 3})\n",
      "TERMINATED trials:\n",
      " - PPO_CartPole-v0_0_lr=0.01:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32083], 295 s, 29 iter, 116000 ts, 200 rew\n",
      " - PPO_CartPole-v0_1_lr=0.001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32084], 683 s, 76 iter, 304000 ts, 200 rew\n",
      " - PPO_CartPole-v0_2_lr=0.0001:\tTERMINATED, [2 CPUs, 0 GPUs], [pid=32086], 183 s, 17 iter, 68000 ts, 200 rew\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fedd51c7d68>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init()\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"episode_reward_mean\": 5000},\n",
    "    config={\n",
    "        \"env\": \"Walker2d-v3\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 8,\n",
    "        \"eager\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ray(3.6)",
   "language": "python",
   "name": "ray"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
