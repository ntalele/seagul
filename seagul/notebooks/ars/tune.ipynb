{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.rllib.agents.ppo.appo import DEFAULT_CONFIG\n",
    "from seagul.envs.mujoco.five_link import FiveLinkWalkerEnv\n",
    "from pybullet_envs.gym_locomotion_envs import Walker2DBulletEnv\n",
    "from ray.tune.registry import register_env\n",
    "import pybullet_envs\n",
    "\n",
    "\n",
    "\n",
    "def five_link_creator(env_config):\n",
    "    return FiveLinkWalkerEnv()  # return an env instance\n",
    "\n",
    "\n",
    "def bullet_walker_creator(env_config):\n",
    "    return Walker2DBulletEnv()  # return an env instance\n",
    "\n",
    "register_env(\"five_link-v3\", five_link_creator)\n",
    "register_env(\"Walker2DBulletEnv-v0\", bullet_walker_creator)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 15:21:56,171\tINFO resource_spec.py:205 -- Starting Ray with 4.2 GiB memory available for workers and up to 2.12 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 7.5/15.7 GiB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 7.6/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:21:58,131\tINFO trainer.py:344 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:21:59,581\tINFO rollout_worker.py:768 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f1194477cf8>}\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:21:59,581\tINFO rollout_worker.py:769 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f11944779b0>}\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:21:59,581\tINFO rollout_worker.py:370 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f1194456240>}\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:21:59,808\tINFO multi_gpu_optimizer.py:93 -- LocalMultiGPUOptimizer devices ['/gpu:0']\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7681)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7685)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7690)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7694)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7692)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7693)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7687)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7684)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7682)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7686)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7691)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7683)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m 2019-11-06 15:22:05,216\tINFO rollout_worker.py:467 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m 2019-11-06 15:22:05,218\tINFO sampler.py:310 -- Raw obs from env: { 0: { 'agent0': np.ndarray((17,), dtype=float64, min=-0.004, max=1.247, mean=0.073)}}\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m 2019-11-06 15:22:05,218\tINFO sampler.py:311 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m 2019-11-06 15:22:05,218\tINFO sampler.py:409 -- Preprocessed obs: np.ndarray((17,), dtype=float64, min=-0.004, max=1.247, mean=0.073)\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m 2019-11-06 15:22:05,218\tINFO sampler.py:413 -- Filtered obs: np.ndarray((17,), dtype=float64, min=-0.004, max=1.247, mean=0.073)\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7689)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m 2019-11-06 15:22:05,219\tINFO sampler.py:528 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                                   'obs': np.ndarray((17,), dtype=float64, min=-0.004, max=1.247, mean=0.073),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                                   'prev_action': np.ndarray((6,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m 2019-11-06 15:22:05,220\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m 2019-11-06 15:22:05,304\tINFO sampler.py:555 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m { 'default_policy': ( np.ndarray((1, 6), dtype=float32, min=-1.701, max=-0.153, mean=-0.645),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                       { 'action_logp': np.ndarray((1,), dtype=float32, min=-7.775, max=-7.775, mean=-7.775),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'behaviour_logits': np.ndarray((1, 12), dtype=float32, min=-0.007, max=0.004, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.002, max=0.002, mean=0.002)})}\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m 2019-11-06 15:22:05,401\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m { 'agent0': { 'data': { 'action_logp': np.ndarray((43,), dtype=float32, min=-12.13, max=-5.859, mean=-8.596),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'action_prob': np.ndarray((43,), dtype=float32, min=0.0, max=0.003, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'actions': np.ndarray((43, 6), dtype=float32, min=-2.683, max=2.94, mean=0.078),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'advantages': np.ndarray((43,), dtype=float32, min=-4.04, max=9.263, mean=3.203),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'agent_index': np.ndarray((43,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'behaviour_logits': np.ndarray((43, 12), dtype=float32, min=-0.018, max=0.016, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'dones': np.ndarray((43,), dtype=bool, min=0.0, max=1.0, mean=0.023),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'eps_id': np.ndarray((43,), dtype=int64, min=1833573921.0, max=1833573921.0, mean=1833573921.0),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'infos': np.ndarray((43,), dtype=object, head={'x_position': -0.004438164961579034, 'x_velocity': -0.20322276237204182}),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'new_obs': np.ndarray((43, 17), dtype=float32, min=-10.0, max=10.0, mean=-0.577),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'obs': np.ndarray((43, 17), dtype=float32, min=-10.0, max=10.0, mean=-0.518),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'prev_actions': np.ndarray((43, 6), dtype=float32, min=-2.683, max=2.94, mean=0.084),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'prev_rewards': np.ndarray((43,), dtype=float32, min=-1.326, max=1.284, mean=0.25),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'rewards': np.ndarray((43,), dtype=float32, min=-1.326, max=1.284, mean=0.245),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         't': np.ndarray((43,), dtype=int64, min=0.0, max=42.0, mean=21.0),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'unroll_id': np.ndarray((43,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'value_targets': np.ndarray((43,), dtype=float32, min=-4.03, max=9.265, mean=3.204),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m                         'vf_preds': np.ndarray((43,), dtype=float32, min=-0.011, max=0.011, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7680)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m 2019-11-06 15:22:05,900\tINFO rollout_worker.py:501 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m { 'data': { 'action_logp': np.ndarray((200,), dtype=float32, min=-16.854, max=-5.776, mean=-8.457),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'action_prob': np.ndarray((200,), dtype=float32, min=0.0, max=0.003, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'actions': np.ndarray((200, 6), dtype=float32, min=-2.99, max=3.575, mean=0.045),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-5.302, max=25.848, mean=3.665),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'behaviour_logits': np.ndarray((200, 12), dtype=float32, min=-0.018, max=0.016, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.04),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=130280831.0, max=1835646660.0, mean=1108003580.575),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'x_position': -0.004438164961579034, 'x_velocity': -0.20322276237204182}),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'new_obs': np.ndarray((200, 17), dtype=float32, min=-10.0, max=10.0, mean=-0.687),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'obs': np.ndarray((200, 17), dtype=float32, min=-10.0, max=10.0, mean=-0.602),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'prev_actions': np.ndarray((200, 6), dtype=float32, min=-2.99, max=3.575, mean=0.049),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-1.418, max=1.459, mean=0.221),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-1.418, max=1.608, mean=0.222),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=43.0, mean=15.03),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=-5.292, max=25.85, mean=3.667),\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=-0.014, max=0.019, mean=0.003)},\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=7695)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,891\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_1/kernel:0' shape=(17, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,891\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_1/bias:0' shape=(64,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,891\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(17, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,891\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(64,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,892\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_2/kernel:0' shape=(64, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,892\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_2/bias:0' shape=(64,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,892\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(64, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,892\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(64,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,892\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_out/kernel:0' shape=(64, 12) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,892\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/fc_out/bias:0' shape=(12,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,892\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/value_out/kernel:0' shape=(64, 1) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,892\tINFO tf_policy.py:358 -- Optimizing variable <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,894\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m { 'inputs': [ np.ndarray((4000, 6), dtype=float32, min=-3.873, max=3.95, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-1.705, max=1.948, mean=-0.008),\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m               np.ndarray((4000, 17), dtype=float32, min=-10.0, max=10.0, mean=-0.991),\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-17.739, max=-5.663, mean=-8.47),\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m               np.ndarray((4000, 6), dtype=float32, min=-3.873, max=3.95, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-3.356, max=5.73, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m               np.ndarray((4000, 12), dtype=float32, min=-0.022, max=0.018, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m               np.ndarray((4000, 17), dtype=float32, min=-10.0, max=10.0, mean=-0.991),\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m               np.ndarray((4000, 6), dtype=float32, min=-3.873, max=3.95, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-1.705, max=1.948, mean=-0.008),\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-17.29, max=25.85, mean=-1.357),\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.015, max=0.021, mean=0.003)],\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 6) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 17) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m                     <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 6) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 12) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 17) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m                     <tf.Tensor 'default_policy/action:0' shape=(?, 6) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7688)\u001b[0m 2019-11-06 15:22:06,894\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-22-08\n",
      "  done: false\n",
      "  episode_len_mean: 18.142857142857142\n",
      "  episode_reward_max: 33.26364664769675\n",
      "  episode_reward_mean: -0.6467543114890273\n",
      "  episode_reward_min: -11.374863090643274\n",
      "  episodes_this_iter: 210\n",
      "  episodes_total: 210\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1736.921\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.481297492980957\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028308959677815437\n",
      "        policy_loss: -0.05328552797436714\n",
      "        total_loss: 21.455503463745117\n",
      "        vf_explained_var: 0.049983762204647064\n",
      "        vf_loss: 21.503128051757812\n",
      "    load_time_ms: 55.424\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 1762.761\n",
      "    update_time_ms: 1654.711\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.4625\n",
      "    gpu_util_percent0: 0.25875000000000004\n",
      "    ram_util_percent: 63.2375\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45649094662205564\n",
      "    mean_inference_ms: 1.3473081336722652\n",
      "    mean_processing_ms: 0.3403220570120583\n",
      "  time_since_restore: 5.281537055969238\n",
      "  time_this_iter_s: 5.281537055969238\n",
      "  time_total_s: 5.281537055969238\n",
      "  timestamp: 1573082528\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 5 s, 1 iter, 4000 ts, -0.647 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-22-15\n",
      "  done: false\n",
      "  episode_len_mean: 34.130434782608695\n",
      "  episode_reward_max: 52.95304915554775\n",
      "  episode_reward_mean: 12.023571420083004\n",
      "  episode_reward_min: -4.5414771101897795\n",
      "  episodes_this_iter: 115\n",
      "  episodes_total: 667\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1579.988\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.378434181213379\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015405713580548763\n",
      "        policy_loss: -0.04604826495051384\n",
      "        total_loss: 110.42723846435547\n",
      "        vf_explained_var: 0.06829755753278732\n",
      "        vf_loss: 110.46634674072266\n",
      "    load_time_ms: 15.44\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 15872\n",
      "    sample_time_ms: 929.811\n",
      "    update_time_ms: 417.651\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.23333333333334\n",
      "    gpu_util_percent0: 0.20000000000000004\n",
      "    ram_util_percent: 64.06666666666666\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4554904932443138\n",
      "    mean_inference_ms: 1.1427782258312775\n",
      "    mean_processing_ms: 0.32637465655297165\n",
      "  time_since_restore: 11.909828424453735\n",
      "  time_this_iter_s: 2.239546060562134\n",
      "  time_total_s: 11.909828424453735\n",
      "  timestamp: 1573082535\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 11 s, 4 iter, 16000 ts, 12 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-22-21\n",
      "  done: false\n",
      "  episode_len_mean: 71.69\n",
      "  episode_reward_max: 313.59221163567764\n",
      "  episode_reward_mean: 45.12282993135263\n",
      "  episode_reward_min: -24.131534054914585\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 858\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1547.865\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.326831817626953\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009364386089146137\n",
      "        policy_loss: -0.03711775317788124\n",
      "        total_loss: 636.9142456054688\n",
      "        vf_explained_var: 0.177864208817482\n",
      "        vf_loss: 636.947265625\n",
      "    load_time_ms: 9.623\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 27776\n",
      "    sample_time_ms: 786.69\n",
      "    update_time_ms: 240.935\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.43333333333333\n",
      "    gpu_util_percent0: 0.19333333333333336\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44208542031361153\n",
      "    mean_inference_ms: 1.0497917230883584\n",
      "    mean_processing_ms: 0.3005481154833088\n",
      "  time_since_restore: 18.28825306892395\n",
      "  time_this_iter_s: 2.0863986015319824\n",
      "  time_total_s: 18.28825306892395\n",
      "  timestamp: 1573082541\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 18 s, 7 iter, 28000 ts, 45.1 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-22-28\n",
      "  done: false\n",
      "  episode_len_mean: 104.61\n",
      "  episode_reward_max: 400.614106553282\n",
      "  episode_reward_mean: 93.3042574182981\n",
      "  episode_reward_min: -14.925719216814791\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 969\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1528.646\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.265231132507324\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010058480314910412\n",
      "        policy_loss: -0.032738860696554184\n",
      "        total_loss: 1397.70361328125\n",
      "        vf_explained_var: 0.2197909653186798\n",
      "        vf_loss: 1397.7318115234375\n",
      "    load_time_ms: 7.356\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 39680\n",
      "    sample_time_ms: 722.767\n",
      "    update_time_ms: 170.589\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.7\n",
      "    gpu_util_percent0: 0.19\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4358303237031439\n",
      "    mean_inference_ms: 1.0104210302032937\n",
      "    mean_processing_ms: 0.28434093198716254\n",
      "  time_since_restore: 24.549360275268555\n",
      "  time_this_iter_s: 2.086094379425049\n",
      "  time_total_s: 24.549360275268555\n",
      "  timestamp: 1573082548\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 24 s, 10 iter, 40000 ts, 93.3 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-22-34\n",
      "  done: false\n",
      "  episode_len_mean: 139.55\n",
      "  episode_reward_max: 507.6520415805208\n",
      "  episode_reward_mean: 119.12105080861011\n",
      "  episode_reward_min: -1.5584896764442893\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1054\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1491.131\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.202828407287598\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011231969110667706\n",
      "        policy_loss: -0.04168837517499924\n",
      "        total_loss: 989.1471557617188\n",
      "        vf_explained_var: 0.33147838711738586\n",
      "        vf_loss: 989.1837158203125\n",
      "    load_time_ms: 2.227\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 51584\n",
      "    sample_time_ms: 594.16\n",
      "    update_time_ms: 5.824\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.1\n",
      "    gpu_util_percent0: 0.1366666666666667\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.43635862293616556\n",
      "    mean_inference_ms: 1.0039602383862816\n",
      "    mean_processing_ms: 0.2740928808131169\n",
      "  time_since_restore: 30.800825595855713\n",
      "  time_this_iter_s: 2.09224534034729\n",
      "  time_total_s: 30.800825595855713\n",
      "  timestamp: 1573082554\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 30 s, 13 iter, 52000 ts, 119 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-22-40\n",
      "  done: false\n",
      "  episode_len_mean: 169.88\n",
      "  episode_reward_max: 507.6520415805208\n",
      "  episode_reward_mean: 162.16820864346008\n",
      "  episode_reward_min: -1.5584896764442893\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1121\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1479.895\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.109565734863281\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01202609483152628\n",
      "        policy_loss: -0.043036676943302155\n",
      "        total_loss: 1503.9234619140625\n",
      "        vf_explained_var: 0.4151718318462372\n",
      "        vf_loss: 1503.961181640625\n",
      "    load_time_ms: 2.544\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 63488\n",
      "    sample_time_ms: 585.455\n",
      "    update_time_ms: 6.328\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.03333333333333\n",
      "    gpu_util_percent0: 0.12333333333333334\n",
      "    ram_util_percent: 64.33333333333333\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.43942406755933017\n",
      "    mean_inference_ms: 1.0198408167259025\n",
      "    mean_processing_ms: 0.2713691044701257\n",
      "  time_since_restore: 37.14012861251831\n",
      "  time_this_iter_s: 2.1197221279144287\n",
      "  time_total_s: 37.14012861251831\n",
      "  timestamp: 1573082560\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 37 s, 16 iter, 64000 ts, 162 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-22-47\n",
      "  done: false\n",
      "  episode_len_mean: 192.4\n",
      "  episode_reward_max: 550.0951185620288\n",
      "  episode_reward_mean: 209.00477081461725\n",
      "  episode_reward_min: -0.42788658106445143\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1178\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1496.471\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.014345169067383\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009526965208351612\n",
      "        policy_loss: -0.0410311259329319\n",
      "        total_loss: 1228.0635986328125\n",
      "        vf_explained_var: 0.5077580213546753\n",
      "        vf_loss: 1228.100341796875\n",
      "    load_time_ms: 2.48\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 75392\n",
      "    sample_time_ms: 595.353\n",
      "    update_time_ms: 5.927\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.5\n",
      "    gpu_util_percent0: 0.13\n",
      "    ram_util_percent: 64.43333333333334\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44084328986565713\n",
      "    mean_inference_ms: 1.0312238991439904\n",
      "    mean_processing_ms: 0.2678496823753745\n",
      "  time_since_restore: 43.659669399261475\n",
      "  time_this_iter_s: 2.162393093109131\n",
      "  time_total_s: 43.659669399261475\n",
      "  timestamp: 1573082567\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 43 s, 19 iter, 76000 ts, 209 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-22-53\n",
      "  done: false\n",
      "  episode_len_mean: 209.43\n",
      "  episode_reward_max: 669.0170131953502\n",
      "  episode_reward_mean: 253.48631992519768\n",
      "  episode_reward_min: 0.33349307571433445\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1237\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1511.662\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.917048931121826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010983905754983425\n",
      "        policy_loss: -0.04314393177628517\n",
      "        total_loss: 1230.853759765625\n",
      "        vf_explained_var: 0.48103171586990356\n",
      "        vf_loss: 1230.8919677734375\n",
      "    load_time_ms: 2.422\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 87296\n",
      "    sample_time_ms: 600.868\n",
      "    update_time_ms: 5.825\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.9\n",
      "    gpu_util_percent0: 0.15333333333333332\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4419886767065539\n",
      "    mean_inference_ms: 1.0341761543915888\n",
      "    mean_processing_ms: 0.2647060898197531\n",
      "  time_since_restore: 50.1126184463501\n",
      "  time_this_iter_s: 2.155357837677002\n",
      "  time_total_s: 50.1126184463501\n",
      "  timestamp: 1573082573\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 50 s, 22 iter, 88000 ts, 253 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-23-00\n",
      "  done: false\n",
      "  episode_len_mean: 210.44\n",
      "  episode_reward_max: 783.6942741260772\n",
      "  episode_reward_mean: 259.632757592476\n",
      "  episode_reward_min: 0.33349307571433445\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 1291\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1524.777\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.804072856903076\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011177022010087967\n",
      "        policy_loss: -0.04404948279261589\n",
      "        total_loss: 1067.843994140625\n",
      "        vf_explained_var: 0.5448169112205505\n",
      "        vf_loss: 1067.8829345703125\n",
      "    load_time_ms: 2.28\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 99200\n",
      "    sample_time_ms: 601.865\n",
      "    update_time_ms: 5.8\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.93333333333334\n",
      "    gpu_util_percent0: 0.18000000000000002\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44500708026934854\n",
      "    mean_inference_ms: 1.0357265474681583\n",
      "    mean_processing_ms: 0.26394781292747704\n",
      "  time_since_restore: 56.56670570373535\n",
      "  time_this_iter_s: 2.1072275638580322\n",
      "  time_total_s: 56.56670570373535\n",
      "  timestamp: 1573082580\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 56 s, 25 iter, 100000 ts, 260 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-23-06\n",
      "  done: false\n",
      "  episode_len_mean: 228.56\n",
      "  episode_reward_max: 783.6942741260772\n",
      "  episode_reward_mean: 296.84413733135165\n",
      "  episode_reward_min: 35.42267125309725\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 1342\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1531.692\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.748587608337402\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011317008174955845\n",
      "        policy_loss: -0.044675715267658234\n",
      "        total_loss: 920.5248413085938\n",
      "        vf_explained_var: 0.4479382336139679\n",
      "        vf_loss: 920.564453125\n",
      "    load_time_ms: 2.327\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 111104\n",
      "    sample_time_ms: 612.087\n",
      "    update_time_ms: 5.77\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.800000000000004\n",
      "    gpu_util_percent0: 0.06666666666666667\n",
      "    ram_util_percent: 64.16666666666666\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44490369115155837\n",
      "    mean_inference_ms: 1.0236785958464862\n",
      "    mean_processing_ms: 0.2609996847156493\n",
      "  time_since_restore: 63.21457576751709\n",
      "  time_this_iter_s: 2.2790684700012207\n",
      "  time_total_s: 63.21457576751709\n",
      "  timestamp: 1573082586\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 63 s, 28 iter, 112000 ts, 297 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-23-13\n",
      "  done: false\n",
      "  episode_len_mean: 221.9\n",
      "  episode_reward_max: 713.2760399874086\n",
      "  episode_reward_mean: 320.8477630725887\n",
      "  episode_reward_min: 83.04373146117179\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 1399\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1524.735\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.759164333343506\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01134612038731575\n",
      "        policy_loss: -0.04194311052560806\n",
      "        total_loss: 1090.9671630859375\n",
      "        vf_explained_var: 0.3664567172527313\n",
      "        vf_loss: 1091.00390625\n",
      "    load_time_ms: 2.413\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 123008\n",
      "    sample_time_ms: 615.974\n",
      "    update_time_ms: 5.828\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.43333333333334\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44604971613353606\n",
      "    mean_inference_ms: 1.024842969574622\n",
      "    mean_processing_ms: 0.2592119895230056\n",
      "  time_since_restore: 69.64349508285522\n",
      "  time_this_iter_s: 2.0952212810516357\n",
      "  time_total_s: 69.64349508285522\n",
      "  timestamp: 1573082593\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 69 s, 31 iter, 124000 ts, 321 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-23-19\n",
      "  done: false\n",
      "  episode_len_mean: 206.39\n",
      "  episode_reward_max: 684.3352973592341\n",
      "  episode_reward_mean: 303.26404403119795\n",
      "  episode_reward_min: 55.81551423042654\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 1457\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1504.757\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.724743843078613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011937614530324936\n",
      "        policy_loss: -0.045607712119817734\n",
      "        total_loss: 892.38671875\n",
      "        vf_explained_var: 0.5097898840904236\n",
      "        vf_loss: 892.427001953125\n",
      "    load_time_ms: 2.073\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 134912\n",
      "    sample_time_ms: 606.413\n",
      "    update_time_ms: 5.608\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.7\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44803095151449435\n",
      "    mean_inference_ms: 1.0298405869396905\n",
      "    mean_processing_ms: 0.2589637736807634\n",
      "  time_since_restore: 75.8488097190857\n",
      "  time_this_iter_s: 2.080984592437744\n",
      "  time_total_s: 75.8488097190857\n",
      "  timestamp: 1573082599\n",
      "  timesteps_since_restore: 136000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 75 s, 34 iter, 136000 ts, 303 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-23-25\n",
      "  done: false\n",
      "  episode_len_mean: 205.56\n",
      "  episode_reward_max: 518.7152440870123\n",
      "  episode_reward_mean: 300.9912230993177\n",
      "  episode_reward_min: 55.81551423042654\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1517\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1488.658\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.705136299133301\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012173417024314404\n",
      "        policy_loss: -0.04111804813146591\n",
      "        total_loss: 400.5039978027344\n",
      "        vf_explained_var: 0.7401943206787109\n",
      "        vf_loss: 400.5396728515625\n",
      "    load_time_ms: 2.205\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 146816\n",
      "    sample_time_ms: 599.439\n",
      "    update_time_ms: 5.361\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.766666666666666\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.0\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4471285521167345\n",
      "    mean_inference_ms: 1.0231907867993935\n",
      "    mean_processing_ms: 0.2564944787703709\n",
      "  time_since_restore: 82.08543920516968\n",
      "  time_this_iter_s: 2.1034352779388428\n",
      "  time_total_s: 82.08543920516968\n",
      "  timestamp: 1573082605\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 82 s, 37 iter, 148000 ts, 301 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 194.35\n",
      "  episode_reward_max: 518.7152440870123\n",
      "  episode_reward_mean: 305.2678956124531\n",
      "  episode_reward_min: 83.2888657599205\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1582\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1475.66\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.702399730682373\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011068899184465408\n",
      "        policy_loss: -0.04571019858121872\n",
      "        total_loss: 195.43185424804688\n",
      "        vf_explained_var: 0.7988597750663757\n",
      "        vf_loss: 195.4725799560547\n",
      "    load_time_ms: 2.099\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 158720\n",
      "    sample_time_ms: 582.197\n",
      "    update_time_ms: 5.32\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.9\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.0\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44684819055292474\n",
      "    mean_inference_ms: 1.0172686115980587\n",
      "    mean_processing_ms: 0.25428134511726397\n",
      "  time_since_restore: 88.39190173149109\n",
      "  time_this_iter_s: 2.0633625984191895\n",
      "  time_total_s: 88.39190173149109\n",
      "  timestamp: 1573082612\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 88 s, 40 iter, 160000 ts, 305 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-23-38\n",
      "  done: false\n",
      "  episode_len_mean: 186.0\n",
      "  episode_reward_max: 492.31273942348247\n",
      "  episode_reward_mean: 299.2102771417209\n",
      "  episode_reward_min: 64.20122032821293\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1645\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1483.082\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.722749710083008\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010843497700989246\n",
      "        policy_loss: -0.04501071944832802\n",
      "        total_loss: 184.17178344726562\n",
      "        vf_explained_var: 0.8420625329017639\n",
      "        vf_loss: 184.2119140625\n",
      "    load_time_ms: 2.267\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 170624\n",
      "    sample_time_ms: 591.261\n",
      "    update_time_ms: 5.405\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.333333333333336\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44558676818211623\n",
      "    mean_inference_ms: 1.0094159742009041\n",
      "    mean_processing_ms: 0.2519424381115809\n",
      "  time_since_restore: 94.77614092826843\n",
      "  time_this_iter_s: 2.0971944332122803\n",
      "  time_total_s: 94.77614092826843\n",
      "  timestamp: 1573082618\n",
      "  timesteps_since_restore: 172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 94 s, 43 iter, 172000 ts, 299 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-23-44\n",
      "  done: false\n",
      "  episode_len_mean: 190.38\n",
      "  episode_reward_max: 548.0864488009992\n",
      "  episode_reward_mean: 303.83336353561083\n",
      "  episode_reward_min: 58.674722362621736\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1708\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1486.852\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.6752119064331055\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011834435164928436\n",
      "        policy_loss: -0.05040686950087547\n",
      "        total_loss: 136.33985900878906\n",
      "        vf_explained_var: 0.8863520622253418\n",
      "        vf_loss: 136.3849334716797\n",
      "    load_time_ms: 2.32\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 182528\n",
      "    sample_time_ms: 592.402\n",
      "    update_time_ms: 5.135\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.03333333333334\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4472527140171014\n",
      "    mean_inference_ms: 1.0114158224101557\n",
      "    mean_processing_ms: 0.25243239439978704\n",
      "  time_since_restore: 101.0353057384491\n",
      "  time_this_iter_s: 2.097902536392212\n",
      "  time_total_s: 101.0353057384491\n",
      "  timestamp: 1573082624\n",
      "  timesteps_since_restore: 184000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 101 s, 46 iter, 184000 ts, 304 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-23-51\n",
      "  done: false\n",
      "  episode_len_mean: 200.88\n",
      "  episode_reward_max: 554.9947000545106\n",
      "  episode_reward_mean: 319.45600551176017\n",
      "  episode_reward_min: 58.674722362621736\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1766\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1493.358\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.546411514282227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012520438991487026\n",
      "        policy_loss: -0.04850064218044281\n",
      "        total_loss: 177.69667053222656\n",
      "        vf_explained_var: 0.9044833779335022\n",
      "        vf_loss: 177.73948669433594\n",
      "    load_time_ms: 2.133\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 194432\n",
      "    sample_time_ms: 586.862\n",
      "    update_time_ms: 5.34\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.0\n",
      "    gpu_util_percent0: 0.12333333333333334\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44852944300229824\n",
      "    mean_inference_ms: 1.0144232142020835\n",
      "    mean_processing_ms: 0.25224218475637816\n",
      "  time_since_restore: 107.39019227027893\n",
      "  time_this_iter_s: 2.1596832275390625\n",
      "  time_total_s: 107.39019227027893\n",
      "  timestamp: 1573082631\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 107 s, 49 iter, 196000 ts, 319 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-23-57\n",
      "  done: false\n",
      "  episode_len_mean: 204.81\n",
      "  episode_reward_max: 554.9947000545106\n",
      "  episode_reward_mean: 336.48573613208225\n",
      "  episode_reward_min: 90.72224223446892\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1827\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1485.644\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.528573989868164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01182298269122839\n",
      "        policy_loss: -0.04784916713833809\n",
      "        total_loss: 173.83702087402344\n",
      "        vf_explained_var: 0.8594957590103149\n",
      "        vf_loss: 173.87953186035156\n",
      "    load_time_ms: 2.064\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 206336\n",
      "    sample_time_ms: 586.448\n",
      "    update_time_ms: 5.562\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.50000000000001\n",
      "    gpu_util_percent0: 0.27999999999999997\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44940966766665674\n",
      "    mean_inference_ms: 1.015147643390157\n",
      "    mean_processing_ms: 0.25194134261810186\n",
      "  time_since_restore: 113.66158080101013\n",
      "  time_this_iter_s: 2.0784122943878174\n",
      "  time_total_s: 113.66158080101013\n",
      "  timestamp: 1573082637\n",
      "  timesteps_since_restore: 208000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 113 s, 52 iter, 208000 ts, 336 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-24-03\n",
      "  done: false\n",
      "  episode_len_mean: 195.87\n",
      "  episode_reward_max: 584.4552964169378\n",
      "  episode_reward_mean: 334.6743974397388\n",
      "  episode_reward_min: 78.69849364343881\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1887\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.352\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.465047836303711\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012343439273536205\n",
      "        policy_loss: -0.05065234750509262\n",
      "        total_loss: 88.72608184814453\n",
      "        vf_explained_var: 0.922171950340271\n",
      "        vf_loss: 88.77119445800781\n",
      "    load_time_ms: 2.21\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 218240\n",
      "    sample_time_ms: 587.95\n",
      "    update_time_ms: 5.997\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.400000000000006\n",
      "    gpu_util_percent0: 0.22333333333333336\n",
      "    ram_util_percent: 64.03333333333333\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44915334993110734\n",
      "    mean_inference_ms: 1.0137566201826358\n",
      "    mean_processing_ms: 0.2507367679136101\n",
      "  time_since_restore: 119.91635131835938\n",
      "  time_this_iter_s: 2.0972437858581543\n",
      "  time_total_s: 119.91635131835938\n",
      "  timestamp: 1573082643\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 119 s, 55 iter, 220000 ts, 335 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-24-10\n",
      "  done: false\n",
      "  episode_len_mean: 196.88\n",
      "  episode_reward_max: 584.4552964169378\n",
      "  episode_reward_mean: 344.721055597147\n",
      "  episode_reward_min: 78.69849364343881\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1949\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1484.932\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.377521991729736\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013343595899641514\n",
      "        policy_loss: -0.047362010926008224\n",
      "        total_loss: 70.6102523803711\n",
      "        vf_explained_var: 0.94270259141922\n",
      "        vf_loss: 70.651611328125\n",
      "    load_time_ms: 2.131\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 230144\n",
      "    sample_time_ms: 596.192\n",
      "    update_time_ms: 6.119\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.800000000000004\n",
      "    gpu_util_percent0: 0.23666666666666666\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44807392768905074\n",
      "    mean_inference_ms: 1.0073300269418168\n",
      "    mean_processing_ms: 0.2487540757423928\n",
      "  time_since_restore: 126.31422924995422\n",
      "  time_this_iter_s: 2.1421515941619873\n",
      "  time_total_s: 126.31422924995422\n",
      "  timestamp: 1573082650\n",
      "  timesteps_since_restore: 232000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 126 s, 58 iter, 232000 ts, 345 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-24-16\n",
      "  done: false\n",
      "  episode_len_mean: 195.89\n",
      "  episode_reward_max: 470.62435365974204\n",
      "  episode_reward_mean: 355.64189255560336\n",
      "  episode_reward_min: 277.6043938403494\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 2009\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1486.763\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.326906681060791\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013387937098741531\n",
      "        policy_loss: -0.05565190315246582\n",
      "        total_loss: 83.81804656982422\n",
      "        vf_explained_var: 0.9329283833503723\n",
      "        vf_loss: 83.86768341064453\n",
      "    load_time_ms: 2.093\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 242048\n",
      "    sample_time_ms: 593.968\n",
      "    update_time_ms: 5.831\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.4\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4489239872642338\n",
      "    mean_inference_ms: 1.0089038086782351\n",
      "    mean_processing_ms: 0.2487755106365159\n",
      "  time_since_restore: 132.65944480895996\n",
      "  time_this_iter_s: 2.040921211242676\n",
      "  time_total_s: 132.65944480895996\n",
      "  timestamp: 1573082656\n",
      "  timesteps_since_restore: 244000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 132 s, 61 iter, 244000 ts, 356 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-24-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.7\n",
      "  episode_reward_max: 486.3098170781207\n",
      "  episode_reward_mean: 369.4776510493408\n",
      "  episode_reward_min: 285.44428013829673\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2069\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1487.439\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.285132884979248\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010808924213051796\n",
      "        policy_loss: -0.04872439056634903\n",
      "        total_loss: 50.99109649658203\n",
      "        vf_explained_var: 0.962151050567627\n",
      "        vf_loss: 51.03496170043945\n",
      "    load_time_ms: 2.211\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 253952\n",
      "    sample_time_ms: 596.239\n",
      "    update_time_ms: 5.415\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.949999999999996\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4493341870251598\n",
      "    mean_inference_ms: 1.0061723969408551\n",
      "    mean_processing_ms: 0.24884882936242186\n",
      "  time_since_restore: 138.9175078868866\n",
      "  time_this_iter_s: 2.095214366912842\n",
      "  time_total_s: 138.9175078868866\n",
      "  timestamp: 1573082663\n",
      "  timesteps_since_restore: 256000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 138 s, 64 iter, 256000 ts, 369 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-24-29\n",
      "  done: false\n",
      "  episode_len_mean: 201.94\n",
      "  episode_reward_max: 482.09526863729394\n",
      "  episode_reward_mean: 377.10933210731446\n",
      "  episode_reward_min: 302.1241667135207\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 2128\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.07\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.18850564956665\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012805826030671597\n",
      "        policy_loss: -0.05687478929758072\n",
      "        total_loss: 58.681949615478516\n",
      "        vf_explained_var: 0.9576817154884338\n",
      "        vf_loss: 58.73305130004883\n",
      "    load_time_ms: 2.129\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 265856\n",
      "    sample_time_ms: 597.288\n",
      "    update_time_ms: 5.548\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.5\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44927449062501923\n",
      "    mean_inference_ms: 1.002249629488128\n",
      "    mean_processing_ms: 0.24836729545528005\n",
      "  time_since_restore: 145.2290391921997\n",
      "  time_this_iter_s: 2.088405132293701\n",
      "  time_total_s: 145.2290391921997\n",
      "  timestamp: 1573082669\n",
      "  timesteps_since_restore: 268000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 145 s, 67 iter, 268000 ts, 377 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-24-35\n",
      "  done: false\n",
      "  episode_len_mean: 204.01\n",
      "  episode_reward_max: 523.6862602011123\n",
      "  episode_reward_mean: 386.34359580640523\n",
      "  episode_reward_min: 292.0675412327082\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2190\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1472.962\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.095419406890869\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014273873530328274\n",
      "        policy_loss: -0.05110621452331543\n",
      "        total_loss: 61.854312896728516\n",
      "        vf_explained_var: 0.956541121006012\n",
      "        vf_loss: 61.89900207519531\n",
      "    load_time_ms: 2.03\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 277760\n",
      "    sample_time_ms: 593.891\n",
      "    update_time_ms: 5.495\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.26666666666667\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.16666666666667\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.449250410212654\n",
      "    mean_inference_ms: 1.0044643407725549\n",
      "    mean_processing_ms: 0.24784729501392433\n",
      "  time_since_restore: 151.55059242248535\n",
      "  time_this_iter_s: 2.132733106613159\n",
      "  time_total_s: 151.55059242248535\n",
      "  timestamp: 1573082675\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 151 s, 70 iter, 280000 ts, 386 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-24-42\n",
      "  done: false\n",
      "  episode_len_mean: 202.5\n",
      "  episode_reward_max: 523.6862602011123\n",
      "  episode_reward_mean: 385.6171844859349\n",
      "  episode_reward_min: 299.43004982700563\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2248\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1484.973\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.049894332885742\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012324266135692596\n",
      "        policy_loss: -0.05462921783328056\n",
      "        total_loss: 50.62439727783203\n",
      "        vf_explained_var: 0.9658584594726562\n",
      "        vf_loss: 50.67347717285156\n",
      "    load_time_ms: 2.097\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 289664\n",
      "    sample_time_ms: 594.473\n",
      "    update_time_ms: 5.899\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.06666666666667\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44963248954123\n",
      "    mean_inference_ms: 1.0026489217028594\n",
      "    mean_processing_ms: 0.24768490106473387\n",
      "  time_since_restore: 157.88192319869995\n",
      "  time_this_iter_s: 2.1321969032287598\n",
      "  time_total_s: 157.88192319869995\n",
      "  timestamp: 1573082682\n",
      "  timesteps_since_restore: 292000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 157 s, 73 iter, 292000 ts, 386 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-24-48\n",
      "  done: false\n",
      "  episode_len_mean: 205.59\n",
      "  episode_reward_max: 499.17605164787494\n",
      "  episode_reward_mean: 391.9114618908474\n",
      "  episode_reward_min: 311.99136276677416\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 2306\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1488.936\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.960947036743164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011951016262173653\n",
      "        policy_loss: -0.05454884469509125\n",
      "        total_loss: 51.26272201538086\n",
      "        vf_explained_var: 0.963636040687561\n",
      "        vf_loss: 51.3119010925293\n",
      "    load_time_ms: 2.165\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 301568\n",
      "    sample_time_ms: 585.408\n",
      "    update_time_ms: 6.136\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.900000000000006\n",
      "    gpu_util_percent0: 0.12\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4496782147429483\n",
      "    mean_inference_ms: 1.0003979420293212\n",
      "    mean_processing_ms: 0.24707981542441604\n",
      "  time_since_restore: 164.15506052970886\n",
      "  time_this_iter_s: 2.068361282348633\n",
      "  time_total_s: 164.15506052970886\n",
      "  timestamp: 1573082688\n",
      "  timesteps_since_restore: 304000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 164 s, 76 iter, 304000 ts, 392 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-24-54\n",
      "  done: false\n",
      "  episode_len_mean: 204.67\n",
      "  episode_reward_max: 496.3161438232627\n",
      "  episode_reward_mean: 394.8454467864874\n",
      "  episode_reward_min: 318.6367485539651\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2363\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1491.52\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.829981803894043\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012768561020493507\n",
      "        policy_loss: -0.050428811460733414\n",
      "        total_loss: 55.155357360839844\n",
      "        vf_explained_var: 0.9609326124191284\n",
      "        vf_loss: 55.200035095214844\n",
      "    load_time_ms: 2.319\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 313472\n",
      "    sample_time_ms: 583.674\n",
      "    update_time_ms: 5.933\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.56666666666667\n",
      "    gpu_util_percent0: 0.3133333333333333\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4501414937200259\n",
      "    mean_inference_ms: 1.0001998211495722\n",
      "    mean_processing_ms: 0.2466818611300381\n",
      "  time_since_restore: 170.4472255706787\n",
      "  time_this_iter_s: 2.1087265014648438\n",
      "  time_total_s: 170.4472255706787\n",
      "  timestamp: 1573082694\n",
      "  timesteps_since_restore: 316000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 170 s, 79 iter, 316000 ts, 395 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-25-01\n",
      "  done: false\n",
      "  episode_len_mean: 209.5\n",
      "  episode_reward_max: 497.0956000292193\n",
      "  episode_reward_mean: 407.4180944153803\n",
      "  episode_reward_min: 323.57741547648004\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2417\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1489.904\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.761571884155273\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01103158388286829\n",
      "        policy_loss: -0.05174404755234718\n",
      "        total_loss: 69.09514617919922\n",
      "        vf_explained_var: 0.9526938796043396\n",
      "        vf_loss: 69.14192199707031\n",
      "    load_time_ms: 2.46\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 325376\n",
      "    sample_time_ms: 585.365\n",
      "    update_time_ms: 5.757\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.7\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45021066597804404\n",
      "    mean_inference_ms: 0.9988874523434936\n",
      "    mean_processing_ms: 0.2462024629116317\n",
      "  time_since_restore: 176.78224277496338\n",
      "  time_this_iter_s: 2.080962657928467\n",
      "  time_total_s: 176.78224277496338\n",
      "  timestamp: 1573082701\n",
      "  timesteps_since_restore: 328000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 176 s, 82 iter, 328000 ts, 407 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-25-07\n",
      "  done: false\n",
      "  episode_len_mean: 209.03\n",
      "  episode_reward_max: 497.0956000292193\n",
      "  episode_reward_mean: 411.17450556807694\n",
      "  episode_reward_min: 323.57741547648004\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2477\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1490.227\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.720586776733398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014484770596027374\n",
      "        policy_loss: -0.04885626584291458\n",
      "        total_loss: 89.8979721069336\n",
      "        vf_explained_var: 0.9460880160331726\n",
      "        vf_loss: 89.94032287597656\n",
      "    load_time_ms: 2.12\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 337280\n",
      "    sample_time_ms: 584.435\n",
      "    update_time_ms: 5.793\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.199999999999996\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4502604549201363\n",
      "    mean_inference_ms: 0.9964706282858207\n",
      "    mean_processing_ms: 0.24571207526879366\n",
      "  time_since_restore: 183.10657620429993\n",
      "  time_this_iter_s: 2.0780584812164307\n",
      "  time_total_s: 183.10657620429993\n",
      "  timestamp: 1573082707\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 183 s, 85 iter, 340000 ts, 411 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-25-13\n",
      "  done: false\n",
      "  episode_len_mean: 204.63\n",
      "  episode_reward_max: 494.1980789046207\n",
      "  episode_reward_mean: 407.2644425080315\n",
      "  episode_reward_min: 130.05964090923038\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2537\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1492.76\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.713599681854248\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012221002019941807\n",
      "        policy_loss: -0.05103413388133049\n",
      "        total_loss: 88.3375473022461\n",
      "        vf_explained_var: 0.9468038082122803\n",
      "        vf_loss: 88.38307189941406\n",
      "    load_time_ms: 2.254\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 349184\n",
      "    sample_time_ms: 586.791\n",
      "    update_time_ms: 5.806\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.166666666666664\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45031852162180874\n",
      "    mean_inference_ms: 0.9953945975552514\n",
      "    mean_processing_ms: 0.2452824770793919\n",
      "  time_since_restore: 189.40697860717773\n",
      "  time_this_iter_s: 2.1140213012695312\n",
      "  time_total_s: 189.40697860717773\n",
      "  timestamp: 1573082713\n",
      "  timesteps_since_restore: 352000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 189 s, 88 iter, 352000 ts, 407 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-25-20\n",
      "  done: false\n",
      "  episode_len_mean: 209.92\n",
      "  episode_reward_max: 569.0369595605122\n",
      "  episode_reward_mean: 426.6592854015476\n",
      "  episode_reward_min: 336.37616864723344\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 2592\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1481.612\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.672940254211426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013367006555199623\n",
      "        policy_loss: -0.05444737523794174\n",
      "        total_loss: 90.92051696777344\n",
      "        vf_explained_var: 0.94383704662323\n",
      "        vf_loss: 90.96894836425781\n",
      "    load_time_ms: 1.962\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 361088\n",
      "    sample_time_ms: 585.855\n",
      "    update_time_ms: 6.081\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.666666666666664\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4503450983246628\n",
      "    mean_inference_ms: 0.9944606626643422\n",
      "    mean_processing_ms: 0.24488607410171415\n",
      "  time_since_restore: 195.64071559906006\n",
      "  time_this_iter_s: 2.064330577850342\n",
      "  time_total_s: 195.64071559906006\n",
      "  timestamp: 1573082720\n",
      "  timesteps_since_restore: 364000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 195 s, 91 iter, 364000 ts, 427 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-25-26\n",
      "  done: false\n",
      "  episode_len_mean: 218.21\n",
      "  episode_reward_max: 701.3284590314724\n",
      "  episode_reward_mean: 447.8419840716906\n",
      "  episode_reward_min: 345.7647822005033\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 2645\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1473.348\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.633498191833496\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01213531568646431\n",
      "        policy_loss: -0.04790354520082474\n",
      "        total_loss: 68.05712890625\n",
      "        vf_explained_var: 0.961235761642456\n",
      "        vf_loss: 68.09957122802734\n",
      "    load_time_ms: 2.103\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 372992\n",
      "    sample_time_ms: 588.551\n",
      "    update_time_ms: 6.083\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.300000000000004\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4499278296800329\n",
      "    mean_inference_ms: 0.9928946520797288\n",
      "    mean_processing_ms: 0.24421803123780528\n",
      "  time_since_restore: 201.92924761772156\n",
      "  time_this_iter_s: 2.103599786758423\n",
      "  time_total_s: 201.92924761772156\n",
      "  timestamp: 1573082726\n",
      "  timesteps_since_restore: 376000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 201 s, 94 iter, 376000 ts, 448 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-25-32\n",
      "  done: false\n",
      "  episode_len_mean: 221.41\n",
      "  episode_reward_max: 701.3284590314724\n",
      "  episode_reward_mean: 459.96601791496767\n",
      "  episode_reward_min: 372.99881898721736\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 2698\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1465.019\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.5513739585876465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013894435949623585\n",
      "        policy_loss: -0.0616048127412796\n",
      "        total_loss: 48.81924057006836\n",
      "        vf_explained_var: 0.9712188839912415\n",
      "        vf_loss: 48.874603271484375\n",
      "    load_time_ms: 2.1\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 384896\n",
      "    sample_time_ms: 584.908\n",
      "    update_time_ms: 5.897\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.6\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.450124903209269\n",
      "    mean_inference_ms: 0.9903759088262001\n",
      "    mean_processing_ms: 0.24396274372737684\n",
      "  time_since_restore: 208.07716941833496\n",
      "  time_this_iter_s: 2.062647581100464\n",
      "  time_total_s: 208.07716941833496\n",
      "  timestamp: 1573082732\n",
      "  timesteps_since_restore: 388000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 208 s, 97 iter, 388000 ts, 460 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-25-38\n",
      "  done: false\n",
      "  episode_len_mean: 224.57\n",
      "  episode_reward_max: 602.0528384383651\n",
      "  episode_reward_mean: 468.65387486556074\n",
      "  episode_reward_min: 313.29110894049387\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2753\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1473.257\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.487878799438477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013911595568060875\n",
      "        policy_loss: -0.056841891258955\n",
      "        total_loss: 65.32008361816406\n",
      "        vf_explained_var: 0.9684128761291504\n",
      "        vf_loss: 65.37066650390625\n",
      "    load_time_ms: 2.028\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 396800\n",
      "    sample_time_ms: 588.762\n",
      "    update_time_ms: 5.818\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.099999999999994\n",
      "    gpu_util_percent0: 0.049999999999999996\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4505257054787096\n",
      "    mean_inference_ms: 0.9907042587896374\n",
      "    mean_processing_ms: 0.24420921991129987\n",
      "  time_since_restore: 214.4823615550995\n",
      "  time_this_iter_s: 2.070300340652466\n",
      "  time_total_s: 214.4823615550995\n",
      "  timestamp: 1573082738\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 214 s, 100 iter, 400000 ts, 469 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-25-45\n",
      "  done: false\n",
      "  episode_len_mean: 229.31\n",
      "  episode_reward_max: 602.0528384383651\n",
      "  episode_reward_mean: 484.85806084080684\n",
      "  episode_reward_min: 313.29110894049387\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2807\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.617\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.4103498458862305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013341018930077553\n",
      "        policy_loss: -0.048472266644239426\n",
      "        total_loss: 41.32423400878906\n",
      "        vf_explained_var: 0.979945719242096\n",
      "        vf_loss: 41.366703033447266\n",
      "    load_time_ms: 2.124\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 408704\n",
      "    sample_time_ms: 586.564\n",
      "    update_time_ms: 5.719\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.33333333333333\n",
      "    gpu_util_percent0: 0.24\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45081684856248894\n",
      "    mean_inference_ms: 0.9932930450617072\n",
      "    mean_processing_ms: 0.24417777503637464\n",
      "  time_since_restore: 220.78446626663208\n",
      "  time_this_iter_s: 2.0897412300109863\n",
      "  time_total_s: 220.78446626663208\n",
      "  timestamp: 1573082745\n",
      "  timesteps_since_restore: 412000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 220 s, 103 iter, 412000 ts, 485 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-25-51\n",
      "  done: false\n",
      "  episode_len_mean: 231.7\n",
      "  episode_reward_max: 606.9961101311308\n",
      "  episode_reward_mean: 492.916098166628\n",
      "  episode_reward_min: 378.5279669860375\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2858\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1498.397\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.349752426147461\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015406440943479538\n",
      "        policy_loss: -0.05693138763308525\n",
      "        total_loss: 28.71021842956543\n",
      "        vf_explained_var: 0.9863666892051697\n",
      "        vf_loss: 28.76021957397461\n",
      "    load_time_ms: 2.335\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 420608\n",
      "    sample_time_ms: 587.803\n",
      "    update_time_ms: 5.758\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.199999999999996\n",
      "    gpu_util_percent0: 0.35000000000000003\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45083256242502445\n",
      "    mean_inference_ms: 0.9902319614188287\n",
      "    mean_processing_ms: 0.24378420388120972\n",
      "  time_since_restore: 227.15048217773438\n",
      "  time_this_iter_s: 2.1267120838165283\n",
      "  time_total_s: 227.15048217773438\n",
      "  timestamp: 1573082751\n",
      "  timesteps_since_restore: 424000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 227 s, 106 iter, 424000 ts, 493 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-25-58\n",
      "  done: false\n",
      "  episode_len_mean: 234.07\n",
      "  episode_reward_max: 606.9961101311308\n",
      "  episode_reward_mean: 498.24950381871827\n",
      "  episode_reward_min: 428.0544487377067\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 2907\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1495.205\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.244373798370361\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012781606055796146\n",
      "        policy_loss: -0.05397903919219971\n",
      "        total_loss: 21.53254508972168\n",
      "        vf_explained_var: 0.9900147914886475\n",
      "        vf_loss: 21.580768585205078\n",
      "    load_time_ms: 2.211\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 432512\n",
      "    sample_time_ms: 582.148\n",
      "    update_time_ms: 6.054\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.06666666666667\n",
      "    gpu_util_percent0: 0.22999999999999998\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4507306251990345\n",
      "    mean_inference_ms: 0.9894413139663806\n",
      "    mean_processing_ms: 0.24367174115967172\n",
      "  time_since_restore: 233.46241974830627\n",
      "  time_this_iter_s: 2.103631019592285\n",
      "  time_total_s: 233.46241974830627\n",
      "  timestamp: 1573082758\n",
      "  timesteps_since_restore: 436000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 233 s, 109 iter, 436000 ts, 498 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-26-04\n",
      "  done: false\n",
      "  episode_len_mean: 235.91\n",
      "  episode_reward_max: 581.1458461632516\n",
      "  episode_reward_mean: 504.0118776293884\n",
      "  episode_reward_min: 428.0544487377067\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2960\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1497.427\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.178569793701172\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012975075282156467\n",
      "        policy_loss: -0.05273810029029846\n",
      "        total_loss: 40.61470413208008\n",
      "        vf_explained_var: 0.9813568592071533\n",
      "        vf_loss: 40.66160583496094\n",
      "    load_time_ms: 2.239\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 444416\n",
      "    sample_time_ms: 585.431\n",
      "    update_time_ms: 5.741\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.300000000000004\n",
      "    gpu_util_percent0: 0.24333333333333332\n",
      "    ram_util_percent: 64.33333333333333\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45122957201947567\n",
      "    mean_inference_ms: 0.9909078082694006\n",
      "    mean_processing_ms: 0.24374531379501593\n",
      "  time_since_restore: 239.79900789260864\n",
      "  time_this_iter_s: 2.1262381076812744\n",
      "  time_total_s: 239.79900789260864\n",
      "  timestamp: 1573082764\n",
      "  timesteps_since_restore: 448000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 239 s, 112 iter, 448000 ts, 504 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-26-10\n",
      "  done: false\n",
      "  episode_len_mean: 235.07\n",
      "  episode_reward_max: 563.1223243053754\n",
      "  episode_reward_mean: 503.03498012392134\n",
      "  episode_reward_min: 401.74222379769685\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 3009\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1497.131\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.1412177085876465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013668518513441086\n",
      "        policy_loss: -0.05472284182906151\n",
      "        total_loss: 13.011709213256836\n",
      "        vf_explained_var: 0.9937825798988342\n",
      "        vf_loss: 13.060279846191406\n",
      "    load_time_ms: 2.264\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 456320\n",
      "    sample_time_ms: 595.864\n",
      "    update_time_ms: 5.964\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.800000000000004\n",
      "    gpu_util_percent0: 0.24666666666666667\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4516697472322828\n",
      "    mean_inference_ms: 0.9897640595216618\n",
      "    mean_processing_ms: 0.24385325407862202\n",
      "  time_since_restore: 246.22416996955872\n",
      "  time_this_iter_s: 2.1101233959198\n",
      "  time_total_s: 246.22416996955872\n",
      "  timestamp: 1573082770\n",
      "  timesteps_since_restore: 460000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 246 s, 115 iter, 460000 ts, 503 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-26-17\n",
      "  done: false\n",
      "  episode_len_mean: 234.97\n",
      "  episode_reward_max: 600.3827476072465\n",
      "  episode_reward_mean: 503.3699809724326\n",
      "  episode_reward_min: 401.74222379769685\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 3059\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1496.01\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.084891319274902\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01383091788738966\n",
      "        policy_loss: -0.05736878886818886\n",
      "        total_loss: 19.566570281982422\n",
      "        vf_explained_var: 0.9901395440101624\n",
      "        vf_loss: 19.61771583557129\n",
      "    load_time_ms: 2.227\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 468224\n",
      "    sample_time_ms: 594.099\n",
      "    update_time_ms: 5.956\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.7\n",
      "    gpu_util_percent0: 0.045\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45176815906723833\n",
      "    mean_inference_ms: 0.9878342943792731\n",
      "    mean_processing_ms: 0.24364715369264606\n",
      "  time_since_restore: 252.52746319770813\n",
      "  time_this_iter_s: 2.0944712162017822\n",
      "  time_total_s: 252.52746319770813\n",
      "  timestamp: 1573082777\n",
      "  timesteps_since_restore: 472000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 252 s, 118 iter, 472000 ts, 503 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-26-23\n",
      "  done: false\n",
      "  episode_len_mean: 236.91\n",
      "  episode_reward_max: 600.3827476072465\n",
      "  episode_reward_mean: 510.2932622041914\n",
      "  episode_reward_min: 454.0465935453712\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 3111\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1500.933\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.030267238616943\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014224683865904808\n",
      "        policy_loss: -0.05549038201570511\n",
      "        total_loss: 15.575533866882324\n",
      "        vf_explained_var: 0.9931121468544006\n",
      "        vf_loss: 15.624621391296387\n",
      "    load_time_ms: 2.506\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 480128\n",
      "    sample_time_ms: 590.188\n",
      "    update_time_ms: 6.113\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.1\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4513220956250648\n",
      "    mean_inference_ms: 0.986252728830928\n",
      "    mean_processing_ms: 0.24286695174192502\n",
      "  time_since_restore: 258.86040449142456\n",
      "  time_this_iter_s: 2.109703540802002\n",
      "  time_total_s: 258.86040449142456\n",
      "  timestamp: 1573082783\n",
      "  timesteps_since_restore: 484000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 258 s, 121 iter, 484000 ts, 510 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-26-29\n",
      "  done: false\n",
      "  episode_len_mean: 237.55\n",
      "  episode_reward_max: 574.4275692523622\n",
      "  episode_reward_mean: 514.678000593771\n",
      "  episode_reward_min: 452.35243355560283\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 3163\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1493.271\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.970947265625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012567401863634586\n",
      "        policy_loss: -0.051842015236616135\n",
      "        total_loss: 18.140108108520508\n",
      "        vf_explained_var: 0.9922173619270325\n",
      "        vf_loss: 18.18629264831543\n",
      "    load_time_ms: 2.367\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 492032\n",
      "    sample_time_ms: 586.541\n",
      "    update_time_ms: 5.839\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.06666666666667\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4509307554621798\n",
      "    mean_inference_ms: 0.9840482323678046\n",
      "    mean_processing_ms: 0.24254617101037412\n",
      "  time_since_restore: 265.1939277648926\n",
      "  time_this_iter_s: 2.0846076011657715\n",
      "  time_total_s: 265.1939277648926\n",
      "  timestamp: 1573082789\n",
      "  timesteps_since_restore: 496000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 265 s, 124 iter, 496000 ts, 515 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-26-36\n",
      "  done: false\n",
      "  episode_len_mean: 236.32\n",
      "  episode_reward_max: 574.4275692523622\n",
      "  episode_reward_mean: 513.897949359838\n",
      "  episode_reward_min: 450.2322108261854\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 3212\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1495.384\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.922515869140625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014281725510954857\n",
      "        policy_loss: -0.05744052305817604\n",
      "        total_loss: 11.183423042297363\n",
      "        vf_explained_var: 0.9954522252082825\n",
      "        vf_loss: 11.23443603515625\n",
      "    load_time_ms: 2.45\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 503936\n",
      "    sample_time_ms: 587.88\n",
      "    update_time_ms: 6.057\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.26666666666667\n",
      "    gpu_util_percent0: 0.05333333333333334\n",
      "    ram_util_percent: 64.46666666666667\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4516175405655757\n",
      "    mean_inference_ms: 0.9883552811763374\n",
      "    mean_processing_ms: 0.2431288824760763\n",
      "  time_since_restore: 271.5521638393402\n",
      "  time_this_iter_s: 2.178812265396118\n",
      "  time_total_s: 271.5521638393402\n",
      "  timestamp: 1573082796\n",
      "  timesteps_since_restore: 508000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 271 s, 127 iter, 508000 ts, 514 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-26-42\n",
      "  done: false\n",
      "  episode_len_mean: 232.48\n",
      "  episode_reward_max: 570.6455637642026\n",
      "  episode_reward_mean: 509.86444525769474\n",
      "  episode_reward_min: 442.16433644903003\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 3266\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1492.249\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.828675270080566\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014298026449978352\n",
      "        policy_loss: -0.05735493823885918\n",
      "        total_loss: 21.984445571899414\n",
      "        vf_explained_var: 0.9903848767280579\n",
      "        vf_loss: 22.035367965698242\n",
      "    load_time_ms: 2.297\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 515840\n",
      "    sample_time_ms: 592.565\n",
      "    update_time_ms: 5.691\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.25\n",
      "    gpu_util_percent0: 0.245\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4518671664040037\n",
      "    mean_inference_ms: 0.9896997492025491\n",
      "    mean_processing_ms: 0.2431192725561468\n",
      "  time_since_restore: 277.8781521320343\n",
      "  time_this_iter_s: 2.1106998920440674\n",
      "  time_total_s: 277.8781521320343\n",
      "  timestamp: 1573082802\n",
      "  timesteps_since_restore: 520000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 277 s, 130 iter, 520000 ts, 510 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-26-48\n",
      "  done: false\n",
      "  episode_len_mean: 231.61\n",
      "  episode_reward_max: 578.1683629104948\n",
      "  episode_reward_mean: 511.9136541196905\n",
      "  episode_reward_min: 459.64561977579774\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 3316\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.859\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.813772201538086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01516726054251194\n",
      "        policy_loss: -0.055825233459472656\n",
      "        total_loss: 13.155674934387207\n",
      "        vf_explained_var: 0.9944719076156616\n",
      "        vf_loss: 13.204676628112793\n",
      "    load_time_ms: 2.363\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 527744\n",
      "    sample_time_ms: 591.341\n",
      "    update_time_ms: 5.816\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.93333333333333\n",
      "    gpu_util_percent0: 0.37333333333333335\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45241112574947956\n",
      "    mean_inference_ms: 0.9888510189655122\n",
      "    mean_processing_ms: 0.24332090021083452\n",
      "  time_since_restore: 284.12585282325745\n",
      "  time_this_iter_s: 2.063314914703369\n",
      "  time_total_s: 284.12585282325745\n",
      "  timestamp: 1573082808\n",
      "  timesteps_since_restore: 532000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 284 s, 133 iter, 532000 ts, 512 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-26-55\n",
      "  done: false\n",
      "  episode_len_mean: 231.05\n",
      "  episode_reward_max: 578.6464427373925\n",
      "  episode_reward_mean: 514.5179035142027\n",
      "  episode_reward_min: 468.96701428006173\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3368\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1483.355\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.745166778564453\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013715014792978764\n",
      "        policy_loss: -0.05434233695268631\n",
      "        total_loss: 16.55901336669922\n",
      "        vf_explained_var: 0.9927403330802917\n",
      "        vf_loss: 16.6071834564209\n",
      "    load_time_ms: 2.539\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 539648\n",
      "    sample_time_ms: 590.425\n",
      "    update_time_ms: 5.784\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.833333333333336\n",
      "    gpu_util_percent0: 0.37333333333333335\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4527659362985091\n",
      "    mean_inference_ms: 0.9898705112794456\n",
      "    mean_processing_ms: 0.24330132974197394\n",
      "  time_since_restore: 290.37823247909546\n",
      "  time_this_iter_s: 2.0883758068084717\n",
      "  time_total_s: 290.37823247909546\n",
      "  timestamp: 1573082815\n",
      "  timesteps_since_restore: 544000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 290 s, 136 iter, 544000 ts, 515 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-27-01\n",
      "  done: false\n",
      "  episode_len_mean: 229.16\n",
      "  episode_reward_max: 578.6464427373925\n",
      "  episode_reward_mean: 514.3261177116706\n",
      "  episode_reward_min: 443.11625059527375\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 3421\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1471.059\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.704373359680176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012779120355844498\n",
      "        policy_loss: -0.04721074178814888\n",
      "        total_loss: 38.26726531982422\n",
      "        vf_explained_var: 0.9833238124847412\n",
      "        vf_loss: 38.3087272644043\n",
      "    load_time_ms: 2.418\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 551552\n",
      "    sample_time_ms: 586.354\n",
      "    update_time_ms: 5.761\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.35\n",
      "    gpu_util_percent0: 0.39\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4523090350537877\n",
      "    mean_inference_ms: 0.9866197946472581\n",
      "    mean_processing_ms: 0.2426356772834654\n",
      "  time_since_restore: 296.61528396606445\n",
      "  time_this_iter_s: 2.0855610370635986\n",
      "  time_total_s: 296.61528396606445\n",
      "  timestamp: 1573082821\n",
      "  timesteps_since_restore: 556000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 139\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 296 s, 139 iter, 556000 ts, 514 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-27-07\n",
      "  done: false\n",
      "  episode_len_mean: 229.66\n",
      "  episode_reward_max: 608.0321906364362\n",
      "  episode_reward_mean: 517.1229599567403\n",
      "  episode_reward_min: 443.11625059527375\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 3474\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1475.612\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.673609733581543\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012720407918095589\n",
      "        policy_loss: -0.05267666280269623\n",
      "        total_loss: 18.691848754882812\n",
      "        vf_explained_var: 0.9925020933151245\n",
      "        vf_loss: 18.73880386352539\n",
      "    load_time_ms: 2.283\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 563456\n",
      "    sample_time_ms: 585.227\n",
      "    update_time_ms: 5.624\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.166666666666664\n",
      "    gpu_util_percent0: 0.25\n",
      "    ram_util_percent: 64.6\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4521222818954787\n",
      "    mean_inference_ms: 0.9845393817844053\n",
      "    mean_processing_ms: 0.24238062946008707\n",
      "  time_since_restore: 302.9347188472748\n",
      "  time_this_iter_s: 2.111614465713501\n",
      "  time_total_s: 302.9347188472748\n",
      "  timestamp: 1573082827\n",
      "  timesteps_since_restore: 568000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 302 s, 142 iter, 568000 ts, 517 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-27-14\n",
      "  done: false\n",
      "  episode_len_mean: 230.53\n",
      "  episode_reward_max: 608.0321906364362\n",
      "  episode_reward_mean: 519.0675920339975\n",
      "  episode_reward_min: 424.73353657783457\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 3524\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1480.977\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.575970649719238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014680447056889534\n",
      "        policy_loss: -0.05754273012280464\n",
      "        total_loss: 15.719621658325195\n",
      "        vf_explained_var: 0.9925764799118042\n",
      "        vf_loss: 15.770557403564453\n",
      "    load_time_ms: 1.992\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 575360\n",
      "    sample_time_ms: 587.875\n",
      "    update_time_ms: 5.597\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.43333333333333\n",
      "    gpu_util_percent0: 0.24333333333333332\n",
      "    ram_util_percent: 64.6\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4527778127642313\n",
      "    mean_inference_ms: 0.9862662052231843\n",
      "    mean_processing_ms: 0.24275897468428398\n",
      "  time_since_restore: 309.2443697452545\n",
      "  time_this_iter_s: 2.0716967582702637\n",
      "  time_total_s: 309.2443697452545\n",
      "  timestamp: 1573082834\n",
      "  timesteps_since_restore: 580000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 145\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 309 s, 145 iter, 580000 ts, 519 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-27-20\n",
      "  done: false\n",
      "  episode_len_mean: 227.04\n",
      "  episode_reward_max: 597.8933645233459\n",
      "  episode_reward_mean: 515.4748205669191\n",
      "  episode_reward_min: 424.73353657783457\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 3578\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1490.219\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.578133583068848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013629400171339512\n",
      "        policy_loss: -0.0557132251560688\n",
      "        total_loss: 15.706027030944824\n",
      "        vf_explained_var: 0.9933732151985168\n",
      "        vf_loss: 15.755605697631836\n",
      "    load_time_ms: 2.038\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 587264\n",
      "    sample_time_ms: 581.269\n",
      "    update_time_ms: 5.579\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.1\n",
      "    gpu_util_percent0: 0.045\n",
      "    ram_util_percent: 64.6\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45241887146156956\n",
      "    mean_inference_ms: 0.9867069210889798\n",
      "    mean_processing_ms: 0.2424157671743891\n",
      "  time_since_restore: 315.5003659725189\n",
      "  time_this_iter_s: 2.0723507404327393\n",
      "  time_total_s: 315.5003659725189\n",
      "  timestamp: 1573082840\n",
      "  timesteps_since_restore: 592000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 315 s, 148 iter, 592000 ts, 515 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-27-26\n",
      "  done: false\n",
      "  episode_len_mean: 224.3\n",
      "  episode_reward_max: 564.3087109113599\n",
      "  episode_reward_mean: 514.4992562456328\n",
      "  episode_reward_min: 430.6702588381078\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 3632\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1487.738\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.521429538726807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01596558839082718\n",
      "        policy_loss: -0.053710538893938065\n",
      "        total_loss: 14.465149879455566\n",
      "        vf_explained_var: 0.993904709815979\n",
      "        vf_loss: 14.511675834655762\n",
      "    load_time_ms: 2.04\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 599168\n",
      "    sample_time_ms: 585.181\n",
      "    update_time_ms: 5.712\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.13333333333333\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45237883703901133\n",
      "    mean_inference_ms: 0.9874258544445376\n",
      "    mean_processing_ms: 0.24213407761977032\n",
      "  time_since_restore: 321.8080151081085\n",
      "  time_this_iter_s: 2.1060073375701904\n",
      "  time_total_s: 321.8080151081085\n",
      "  timestamp: 1573082846\n",
      "  timesteps_since_restore: 604000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 151\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 321 s, 151 iter, 604000 ts, 514 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-27-33\n",
      "  done: false\n",
      "  episode_len_mean: 226.95\n",
      "  episode_reward_max: 588.8140987640127\n",
      "  episode_reward_mean: 521.6427834004623\n",
      "  episode_reward_min: 430.6702588381078\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 3685\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.253\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.533043384552002\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011498894542455673\n",
      "        policy_loss: -0.051425643265247345\n",
      "        total_loss: 21.470617294311523\n",
      "        vf_explained_var: 0.9914959669113159\n",
      "        vf_loss: 21.516870498657227\n",
      "    load_time_ms: 2.022\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 611072\n",
      "    sample_time_ms: 581.969\n",
      "    update_time_ms: 5.542\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.93333333333334\n",
      "    gpu_util_percent0: 0.06333333333333334\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4530934010688698\n",
      "    mean_inference_ms: 0.9891399001193534\n",
      "    mean_processing_ms: 0.2424772523186161\n",
      "  time_since_restore: 328.0691523551941\n",
      "  time_this_iter_s: 2.0933279991149902\n",
      "  time_total_s: 328.0691523551941\n",
      "  timestamp: 1573082853\n",
      "  timesteps_since_restore: 616000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 328 s, 154 iter, 616000 ts, 522 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-27-39\n",
      "  done: false\n",
      "  episode_len_mean: 227.46\n",
      "  episode_reward_max: 588.8140987640127\n",
      "  episode_reward_mean: 524.7891183211627\n",
      "  episode_reward_min: 420.84124926584263\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 3738\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.063\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.434584140777588\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014264627359807491\n",
      "        policy_loss: -0.04991405829787254\n",
      "        total_loss: 15.512541770935059\n",
      "        vf_explained_var: 0.9932584166526794\n",
      "        vf_loss: 15.556036949157715\n",
      "    load_time_ms: 2.062\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 622976\n",
      "    sample_time_ms: 584.702\n",
      "    update_time_ms: 5.45\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.45\n",
      "    gpu_util_percent0: 0.2\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45316151904809554\n",
      "    mean_inference_ms: 0.9879330099976804\n",
      "    mean_processing_ms: 0.2422935405205326\n",
      "  time_since_restore: 334.3575098514557\n",
      "  time_this_iter_s: 2.0252840518951416\n",
      "  time_total_s: 334.3575098514557\n",
      "  timestamp: 1573082859\n",
      "  timesteps_since_restore: 628000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 157\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 334 s, 157 iter, 628000 ts, 525 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-27-45\n",
      "  done: false\n",
      "  episode_len_mean: 225.57\n",
      "  episode_reward_max: 566.331247960274\n",
      "  episode_reward_mean: 522.589278516425\n",
      "  episode_reward_min: 420.84124926584263\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 3790\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1478.854\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.442592620849609\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013840709812939167\n",
      "        policy_loss: -0.05403241142630577\n",
      "        total_loss: 9.675264358520508\n",
      "        vf_explained_var: 0.9961149096488953\n",
      "        vf_loss: 9.723069190979004\n",
      "    load_time_ms: 2.29\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 634880\n",
      "    sample_time_ms: 582.208\n",
      "    update_time_ms: 5.919\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.73333333333334\n",
      "    gpu_util_percent0: 0.2833333333333333\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45321896851288124\n",
      "    mean_inference_ms: 0.9889537929094044\n",
      "    mean_processing_ms: 0.24242019334937098\n",
      "  time_since_restore: 340.5818510055542\n",
      "  time_this_iter_s: 2.0947206020355225\n",
      "  time_total_s: 340.5818510055542\n",
      "  timestamp: 1573082865\n",
      "  timesteps_since_restore: 640000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 340 s, 160 iter, 640000 ts, 523 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-27-52\n",
      "  done: false\n",
      "  episode_len_mean: 227.43\n",
      "  episode_reward_max: 589.0345701711811\n",
      "  episode_reward_mean: 528.5338302750795\n",
      "  episode_reward_min: 472.9372316995293\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 3842\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1474.699\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.379108905792236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017055818811058998\n",
      "        policy_loss: -0.05790359154343605\n",
      "        total_loss: 15.581071853637695\n",
      "        vf_explained_var: 0.9936441779136658\n",
      "        vf_loss: 15.63129997253418\n",
      "    load_time_ms: 2.559\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 646784\n",
      "    sample_time_ms: 583.346\n",
      "    update_time_ms: 6.13\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.233333333333334\n",
      "    gpu_util_percent0: 0.35666666666666663\n",
      "    ram_util_percent: 64.53333333333333\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45376219782892796\n",
      "    mean_inference_ms: 0.9909895725762169\n",
      "    mean_processing_ms: 0.24272428794187906\n",
      "  time_since_restore: 346.8311426639557\n",
      "  time_this_iter_s: 2.0986435413360596\n",
      "  time_total_s: 346.8311426639557\n",
      "  timestamp: 1573082872\n",
      "  timesteps_since_restore: 652000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 163\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 346 s, 163 iter, 652000 ts, 529 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-27-58\n",
      "  done: false\n",
      "  episode_len_mean: 228.88\n",
      "  episode_reward_max: 589.0345701711811\n",
      "  episode_reward_mean: 534.9812658457151\n",
      "  episode_reward_min: 482.18219822337034\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 3896\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1461.769\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.338105201721191\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01541986782103777\n",
      "        policy_loss: -0.05223962292075157\n",
      "        total_loss: 16.0956974029541\n",
      "        vf_explained_var: 0.9935400485992432\n",
      "        vf_loss: 16.14099884033203\n",
      "    load_time_ms: 2.392\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 658688\n",
      "    sample_time_ms: 583.316\n",
      "    update_time_ms: 6.3\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.63333333333333\n",
      "    gpu_util_percent0: 0.38000000000000006\n",
      "    ram_util_percent: 64.6\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4531224359827641\n",
      "    mean_inference_ms: 0.9856482110696295\n",
      "    mean_processing_ms: 0.24176035955342987\n",
      "  time_since_restore: 353.056693315506\n",
      "  time_this_iter_s: 2.095308542251587\n",
      "  time_total_s: 353.056693315506\n",
      "  timestamp: 1573082878\n",
      "  timesteps_since_restore: 664000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 353 s, 166 iter, 664000 ts, 535 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-28-04\n",
      "  done: false\n",
      "  episode_len_mean: 227.64\n",
      "  episode_reward_max: 599.3703282915199\n",
      "  episode_reward_mean: 535.9949290179951\n",
      "  episode_reward_min: 462.62943112201583\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 3949\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1472.411\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.357506275177002\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0152587303891778\n",
      "        policy_loss: -0.05518944188952446\n",
      "        total_loss: 10.770485877990723\n",
      "        vf_explained_var: 0.9959591031074524\n",
      "        vf_loss: 10.818808555603027\n",
      "    load_time_ms: 2.312\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 670592\n",
      "    sample_time_ms: 588.015\n",
      "    update_time_ms: 6.24\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.96666666666666\n",
      "    gpu_util_percent0: 0.3833333333333333\n",
      "    ram_util_percent: 64.6\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4525624011814676\n",
      "    mean_inference_ms: 0.9842146256601023\n",
      "    mean_processing_ms: 0.24141734024206735\n",
      "  time_since_restore: 359.3667962551117\n",
      "  time_this_iter_s: 2.144538640975952\n",
      "  time_total_s: 359.3667962551117\n",
      "  timestamp: 1573082884\n",
      "  timesteps_since_restore: 676000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 169\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 359 s, 169 iter, 676000 ts, 536 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-28-10\n",
      "  done: false\n",
      "  episode_len_mean: 227.79\n",
      "  episode_reward_max: 599.3703282915199\n",
      "  episode_reward_mean: 539.4647378138541\n",
      "  episode_reward_min: 462.62943112201583\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 4003\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1472.219\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.251546859741211\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014748604036867619\n",
      "        policy_loss: -0.0559956394135952\n",
      "        total_loss: 9.635045051574707\n",
      "        vf_explained_var: 0.9962582588195801\n",
      "        vf_loss: 9.684403419494629\n",
      "    load_time_ms: 2.363\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 682496\n",
      "    sample_time_ms: 592.78\n",
      "    update_time_ms: 5.881\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.1\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.46666666666667\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45283585300245854\n",
      "    mean_inference_ms: 0.9838168313473166\n",
      "    mean_processing_ms: 0.24144984017242294\n",
      "  time_since_restore: 365.65600085258484\n",
      "  time_this_iter_s: 2.08813738822937\n",
      "  time_total_s: 365.65600085258484\n",
      "  timestamp: 1573082890\n",
      "  timesteps_since_restore: 688000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 365 s, 172 iter, 688000 ts, 539 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-28-17\n",
      "  done: false\n",
      "  episode_len_mean: 228.83\n",
      "  episode_reward_max: 589.0878649457338\n",
      "  episode_reward_mean: 543.4384526396206\n",
      "  episode_reward_min: 495.4713486639847\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 4053\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1473.169\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.207535743713379\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01554871629923582\n",
      "        policy_loss: -0.05653556063771248\n",
      "        total_loss: 9.82988166809082\n",
      "        vf_explained_var: 0.9960277676582336\n",
      "        vf_loss: 9.87942123413086\n",
      "    load_time_ms: 2.328\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 694400\n",
      "    sample_time_ms: 598.221\n",
      "    update_time_ms: 5.757\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.333333333333336\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4530622178342235\n",
      "    mean_inference_ms: 0.9837805941652067\n",
      "    mean_processing_ms: 0.24145044819634523\n",
      "  time_since_restore: 371.9475827217102\n",
      "  time_this_iter_s: 2.108307123184204\n",
      "  time_total_s: 371.9475827217102\n",
      "  timestamp: 1573082897\n",
      "  timesteps_since_restore: 700000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 175\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 371 s, 175 iter, 700000 ts, 543 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-28-23\n",
      "  done: false\n",
      "  episode_len_mean: 228.84\n",
      "  episode_reward_max: 587.6026657927599\n",
      "  episode_reward_mean: 545.0294310510957\n",
      "  episode_reward_min: 421.0408958484962\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 4107\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1474.022\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.248839855194092\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017560599371790886\n",
      "        policy_loss: -0.05563164874911308\n",
      "        total_loss: 9.222660064697266\n",
      "        vf_explained_var: 0.9963861107826233\n",
      "        vf_loss: 9.27038860321045\n",
      "    load_time_ms: 2.288\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 706304\n",
      "    sample_time_ms: 598.393\n",
      "    update_time_ms: 5.589\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.166666666666664\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 63.56666666666666\n",
      "    vram_util_percent0: 0.17020576131687246\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45330656586779766\n",
      "    mean_inference_ms: 0.9844479486287185\n",
      "    mean_processing_ms: 0.24134739536944752\n",
      "  time_since_restore: 378.21363377571106\n",
      "  time_this_iter_s: 2.0914969444274902\n",
      "  time_total_s: 378.21363377571106\n",
      "  timestamp: 1573082903\n",
      "  timesteps_since_restore: 712000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 378 s, 178 iter, 712000 ts, 545 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 228.3\n",
      "  episode_reward_max: 587.6026657927599\n",
      "  episode_reward_mean: 547.8789203274366\n",
      "  episode_reward_min: 421.0408958484962\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 4156\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1466.811\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.155396461486816\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015123378485441208\n",
      "        policy_loss: -0.050019580870866776\n",
      "        total_loss: 6.154925346374512\n",
      "        vf_explained_var: 0.9974777698516846\n",
      "        vf_loss: 6.1981401443481445\n",
      "    load_time_ms: 2.191\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 718208\n",
      "    sample_time_ms: 587.536\n",
      "    update_time_ms: 5.585\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.900000000000006\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 63.5\n",
      "    vram_util_percent0: 0.17020576131687243\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4536711903953162\n",
      "    mean_inference_ms: 0.985263515545728\n",
      "    mean_processing_ms: 0.241524477133387\n",
      "  time_since_restore: 384.3761270046234\n",
      "  time_this_iter_s: 2.0181210041046143\n",
      "  time_total_s: 384.3761270046234\n",
      "  timestamp: 1573082909\n",
      "  timesteps_since_restore: 724000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 181\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 384 s, 181 iter, 724000 ts, 548 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-28-36\n",
      "  done: false\n",
      "  episode_len_mean: 228.68\n",
      "  episode_reward_max: 586.5003838561682\n",
      "  episode_reward_mean: 551.7449214627683\n",
      "  episode_reward_min: 509.3908980351904\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 4209\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1476.952\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.167240142822266\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015440665185451508\n",
      "        policy_loss: -0.0547960065305233\n",
      "        total_loss: 7.347255229949951\n",
      "        vf_explained_var: 0.99690181016922\n",
      "        vf_loss: 7.39510440826416\n",
      "    load_time_ms: 2.236\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 730112\n",
      "    sample_time_ms: 581.518\n",
      "    update_time_ms: 5.885\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.43333333333333\n",
      "    gpu_util_percent0: 0.11333333333333333\n",
      "    ram_util_percent: 63.6\n",
      "    vram_util_percent0: 0.17020576131687246\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.453293923230093\n",
      "    mean_inference_ms: 0.9836870948876042\n",
      "    mean_processing_ms: 0.24139682082799063\n",
      "  time_since_restore: 390.6883454322815\n",
      "  time_this_iter_s: 2.1398255825042725\n",
      "  time_total_s: 390.6883454322815\n",
      "  timestamp: 1573082916\n",
      "  timesteps_since_restore: 736000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 184\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 390 s, 184 iter, 736000 ts, 552 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-28-42\n",
      "  done: false\n",
      "  episode_len_mean: 225.76\n",
      "  episode_reward_max: 586.5003838561682\n",
      "  episode_reward_mean: 548.6445166596801\n",
      "  episode_reward_min: 447.8247366170078\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 4264\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1476.586\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.0872087478637695\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011647081933915615\n",
      "        policy_loss: -0.04182903841137886\n",
      "        total_loss: 61.888633728027344\n",
      "        vf_explained_var: 0.9766613245010376\n",
      "        vf_loss: 61.92521667480469\n",
      "    load_time_ms: 2.258\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 742016\n",
      "    sample_time_ms: 580.687\n",
      "    update_time_ms: 5.959\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.76666666666666\n",
      "    gpu_util_percent0: 0.24\n",
      "    ram_util_percent: 63.70000000000001\n",
      "    vram_util_percent0: 0.17020576131687246\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45321084021342395\n",
      "    mean_inference_ms: 0.9849362768827176\n",
      "    mean_processing_ms: 0.24145417642511066\n",
      "  time_since_restore: 396.9550201892853\n",
      "  time_this_iter_s: 2.079167127609253\n",
      "  time_total_s: 396.9550201892853\n",
      "  timestamp: 1573082922\n",
      "  timesteps_since_restore: 748000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 187\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 396 s, 187 iter, 748000 ts, 549 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-28-48\n",
      "  done: false\n",
      "  episode_len_mean: 225.65\n",
      "  episode_reward_max: 593.7576280953787\n",
      "  episode_reward_mean: 551.3750272896262\n",
      "  episode_reward_min: 447.8247366170078\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 4317\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1480.148\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.057082176208496\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01646292768418789\n",
      "        policy_loss: -0.05519402399659157\n",
      "        total_loss: 9.371709823608398\n",
      "        vf_explained_var: 0.9965428709983826\n",
      "        vf_loss: 9.419498443603516\n",
      "    load_time_ms: 2.3\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 753920\n",
      "    sample_time_ms: 580.035\n",
      "    update_time_ms: 6.136\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.900000000000006\n",
      "    gpu_util_percent0: 0.38\n",
      "    ram_util_percent: 63.7\n",
      "    vram_util_percent0: 0.17020576131687243\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45346031731683495\n",
      "    mean_inference_ms: 0.9860805044052495\n",
      "    mean_processing_ms: 0.24156333539744032\n",
      "  time_since_restore: 403.23502016067505\n",
      "  time_this_iter_s: 2.0652947425842285\n",
      "  time_total_s: 403.23502016067505\n",
      "  timestamp: 1573082928\n",
      "  timesteps_since_restore: 760000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 190\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 403 s, 190 iter, 760000 ts, 551 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-28-54\n",
      "  done: false\n",
      "  episode_len_mean: 226.35\n",
      "  episode_reward_max: 593.7576280953787\n",
      "  episode_reward_mean: 553.941272251358\n",
      "  episode_reward_min: 462.66960724659685\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 4372\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.301\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.982024192810059\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015384688042104244\n",
      "        policy_loss: -0.05335429683327675\n",
      "        total_loss: 6.87284517288208\n",
      "        vf_explained_var: 0.9974443912506104\n",
      "        vf_loss: 6.919276714324951\n",
      "    load_time_ms: 2.426\n",
      "    num_steps_sampled: 772000\n",
      "    num_steps_trained: 765824\n",
      "    sample_time_ms: 586.108\n",
      "    update_time_ms: 5.612\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.93333333333334\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 63.70000000000001\n",
      "    vram_util_percent0: 0.17020576131687246\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45300288432217056\n",
      "    mean_inference_ms: 0.9839775743049749\n",
      "    mean_processing_ms: 0.2410661698902719\n",
      "  time_since_restore: 409.5051851272583\n",
      "  time_this_iter_s: 2.052371025085449\n",
      "  time_total_s: 409.5051851272583\n",
      "  timestamp: 1573082934\n",
      "  timesteps_since_restore: 772000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 772000\n",
      "  training_iteration: 193\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 409 s, 193 iter, 772000 ts, 554 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-29-01\n",
      "  done: false\n",
      "  episode_len_mean: 222.82\n",
      "  episode_reward_max: 589.7888296414143\n",
      "  episode_reward_mean: 549.1078752214783\n",
      "  episode_reward_min: 462.66960724659685\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 4425\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1475.2\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.943891525268555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015239888802170753\n",
      "        policy_loss: -0.05688859149813652\n",
      "        total_loss: 8.361283302307129\n",
      "        vf_explained_var: 0.9968451261520386\n",
      "        vf_loss: 8.411314010620117\n",
      "    load_time_ms: 2.247\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 777728\n",
      "    sample_time_ms: 585.495\n",
      "    update_time_ms: 5.487\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.766666666666666\n",
      "    gpu_util_percent0: 0.05333333333333334\n",
      "    ram_util_percent: 63.73333333333333\n",
      "    vram_util_percent0: 0.16910836762688616\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45281806171491906\n",
      "    mean_inference_ms: 0.9832650910355626\n",
      "    mean_processing_ms: 0.24096507220463517\n",
      "  time_since_restore: 415.758722782135\n",
      "  time_this_iter_s: 2.1206953525543213\n",
      "  time_total_s: 415.758722782135\n",
      "  timestamp: 1573082941\n",
      "  timesteps_since_restore: 784000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 196\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 415 s, 196 iter, 784000 ts, 549 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-29-07\n",
      "  done: false\n",
      "  episode_len_mean: 220.68\n",
      "  episode_reward_max: 591.6382942067262\n",
      "  episode_reward_mean: 546.7384769954799\n",
      "  episode_reward_min: 489.26851930857714\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 4480\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1470.56\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.921377182006836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016130806878209114\n",
      "        policy_loss: -0.05364001914858818\n",
      "        total_loss: 8.303971290588379\n",
      "        vf_explained_var: 0.9970651268959045\n",
      "        vf_loss: 8.35035228729248\n",
      "    load_time_ms: 2.283\n",
      "    num_steps_sampled: 796000\n",
      "    num_steps_trained: 789632\n",
      "    sample_time_ms: 589.343\n",
      "    update_time_ms: 5.677\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.43333333333334\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 63.9\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45249896540421536\n",
      "    mean_inference_ms: 0.981221332204813\n",
      "    mean_processing_ms: 0.2407708587501151\n",
      "  time_since_restore: 422.04381251335144\n",
      "  time_this_iter_s: 2.0990188121795654\n",
      "  time_total_s: 422.04381251335144\n",
      "  timestamp: 1573082947\n",
      "  timesteps_since_restore: 796000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 796000\n",
      "  training_iteration: 199\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 422 s, 199 iter, 796000 ts, 547 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 220.78\n",
      "  episode_reward_max: 591.6382942067262\n",
      "  episode_reward_mean: 547.0534515873768\n",
      "  episode_reward_min: 483.3893290231914\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 4532\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1466.815\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.866274356842041\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017533574253320694\n",
      "        policy_loss: -0.05857398733496666\n",
      "        total_loss: 5.731973171234131\n",
      "        vf_explained_var: 0.9979261159896851\n",
      "        vf_loss: 5.782657146453857\n",
      "    load_time_ms: 2.127\n",
      "    num_steps_sampled: 808000\n",
      "    num_steps_trained: 801536\n",
      "    sample_time_ms: 594.454\n",
      "    update_time_ms: 5.926\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.46666666666666\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 63.9\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4522919269330993\n",
      "    mean_inference_ms: 0.9788634663222602\n",
      "    mean_processing_ms: 0.2404006163088102\n",
      "  time_since_restore: 428.3405749797821\n",
      "  time_this_iter_s: 2.1326968669891357\n",
      "  time_total_s: 428.3405749797821\n",
      "  timestamp: 1573082953\n",
      "  timesteps_since_restore: 808000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 808000\n",
      "  training_iteration: 202\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 428 s, 202 iter, 808000 ts, 547 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-29-20\n",
      "  done: false\n",
      "  episode_len_mean: 220.35\n",
      "  episode_reward_max: 601.678966994567\n",
      "  episode_reward_mean: 547.3964080979296\n",
      "  episode_reward_min: 483.3893290231914\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 4588\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1470.89\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.853590965270996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016213592141866684\n",
      "        policy_loss: -0.05787977948784828\n",
      "        total_loss: 8.401036262512207\n",
      "        vf_explained_var: 0.9971002340316772\n",
      "        vf_loss: 8.451620101928711\n",
      "    load_time_ms: 2.032\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 813440\n",
      "    sample_time_ms: 597.587\n",
      "    update_time_ms: 5.909\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.333333333333336\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 63.76666666666667\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4530094656356539\n",
      "    mean_inference_ms: 0.9820464237547428\n",
      "    mean_processing_ms: 0.24090304626078105\n",
      "  time_since_restore: 434.5963580608368\n",
      "  time_this_iter_s: 2.06721830368042\n",
      "  time_total_s: 434.5963580608368\n",
      "  timestamp: 1573082960\n",
      "  timesteps_since_restore: 820000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 205\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 434 s, 205 iter, 820000 ts, 547 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-29-26\n",
      "  done: false\n",
      "  episode_len_mean: 219.24\n",
      "  episode_reward_max: 601.678966994567\n",
      "  episode_reward_mean: 549.2039870251488\n",
      "  episode_reward_min: 485.5793495111634\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 4644\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1470.232\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.749393463134766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020269418135285378\n",
      "        policy_loss: -0.05708157643675804\n",
      "        total_loss: 5.459432601928711\n",
      "        vf_explained_var: 0.9980522394180298\n",
      "        vf_loss: 5.5073933601379395\n",
      "    load_time_ms: 1.935\n",
      "    num_steps_sampled: 832000\n",
      "    num_steps_trained: 825344\n",
      "    sample_time_ms: 589.211\n",
      "    update_time_ms: 6.111\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.00000000000001\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 63.70000000000001\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4525870424361211\n",
      "    mean_inference_ms: 0.9801693531766332\n",
      "    mean_processing_ms: 0.24068266142665803\n",
      "  time_since_restore: 440.8058786392212\n",
      "  time_this_iter_s: 2.0558745861053467\n",
      "  time_total_s: 440.8058786392212\n",
      "  timestamp: 1573082966\n",
      "  timesteps_since_restore: 832000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 832000\n",
      "  training_iteration: 208\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 440 s, 208 iter, 832000 ts, 549 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-29-32\n",
      "  done: false\n",
      "  episode_len_mean: 220.4\n",
      "  episode_reward_max: 602.4237386363477\n",
      "  episode_reward_mean: 552.8464001655543\n",
      "  episode_reward_min: 485.5793495111634\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 4697\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1477.422\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.653934955596924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013617212884128094\n",
      "        policy_loss: -0.05341748520731926\n",
      "        total_loss: 4.496492385864258\n",
      "        vf_explained_var: 0.9983513355255127\n",
      "        vf_loss: 4.540717601776123\n",
      "    load_time_ms: 1.736\n",
      "    num_steps_sampled: 844000\n",
      "    num_steps_trained: 837248\n",
      "    sample_time_ms: 583.697\n",
      "    update_time_ms: 6.096\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.50000000000001\n",
      "    gpu_util_percent0: 0.07333333333333333\n",
      "    ram_util_percent: 63.70000000000001\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4524012947531191\n",
      "    mean_inference_ms: 0.9805459575841264\n",
      "    mean_processing_ms: 0.2405192102918746\n",
      "  time_since_restore: 447.0819058418274\n",
      "  time_this_iter_s: 2.0919642448425293\n",
      "  time_total_s: 447.0819058418274\n",
      "  timestamp: 1573082972\n",
      "  timesteps_since_restore: 844000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 844000\n",
      "  training_iteration: 211\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 447 s, 211 iter, 844000 ts, 553 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-29-39\n",
      "  done: false\n",
      "  episode_len_mean: 221.11\n",
      "  episode_reward_max: 602.4237386363477\n",
      "  episode_reward_mean: 554.7225689508858\n",
      "  episode_reward_min: 496.65954109907625\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 4752\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1487.65\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.661442279815674\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013973007909953594\n",
      "        policy_loss: -0.05421488359570503\n",
      "        total_loss: 6.714016437530518\n",
      "        vf_explained_var: 0.9977044463157654\n",
      "        vf_loss: 6.7587995529174805\n",
      "    load_time_ms: 1.932\n",
      "    num_steps_sampled: 856000\n",
      "    num_steps_trained: 849152\n",
      "    sample_time_ms: 583.194\n",
      "    update_time_ms: 6.196\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.93333333333333\n",
      "    gpu_util_percent0: 0.22666666666666668\n",
      "    ram_util_percent: 63.79999999999999\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4528737249970294\n",
      "    mean_inference_ms: 0.9821178864897944\n",
      "    mean_processing_ms: 0.24077200841575874\n",
      "  time_since_restore: 453.5030174255371\n",
      "  time_this_iter_s: 2.1260526180267334\n",
      "  time_total_s: 453.5030174255371\n",
      "  timestamp: 1573082979\n",
      "  timesteps_since_restore: 856000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 856000\n",
      "  training_iteration: 214\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 453 s, 214 iter, 856000 ts, 555 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-29-45\n",
      "  done: false\n",
      "  episode_len_mean: 222.0\n",
      "  episode_reward_max: 601.3999909947461\n",
      "  episode_reward_mean: 557.9887597550336\n",
      "  episode_reward_min: 518.4909536461874\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 4806\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1500.493\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.59578800201416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014669756405055523\n",
      "        policy_loss: -0.05825004354119301\n",
      "        total_loss: 6.697597503662109\n",
      "        vf_explained_var: 0.9976101517677307\n",
      "        vf_loss: 6.745946407318115\n",
      "    load_time_ms: 2.18\n",
      "    num_steps_sampled: 868000\n",
      "    num_steps_trained: 861056\n",
      "    sample_time_ms: 581.003\n",
      "    update_time_ms: 6.361\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.6\n",
      "    gpu_util_percent0: 0.3833333333333333\n",
      "    ram_util_percent: 63.79999999999999\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45335251101573293\n",
      "    mean_inference_ms: 0.983026032087072\n",
      "    mean_processing_ms: 0.2410025964686177\n",
      "  time_since_restore: 459.83612155914307\n",
      "  time_this_iter_s: 2.1376256942749023\n",
      "  time_total_s: 459.83612155914307\n",
      "  timestamp: 1573082985\n",
      "  timesteps_since_restore: 868000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 868000\n",
      "  training_iteration: 217\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 459 s, 217 iter, 868000 ts, 558 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-29-51\n",
      "  done: false\n",
      "  episode_len_mean: 221.97\n",
      "  episode_reward_max: 601.3999909947461\n",
      "  episode_reward_mean: 558.9224719919242\n",
      "  episode_reward_min: 504.42345127839326\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 4861\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1503.947\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.554245471954346\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013432098552584648\n",
      "        policy_loss: -0.05139538273215294\n",
      "        total_loss: 10.157157897949219\n",
      "        vf_explained_var: 0.9964689612388611\n",
      "        vf_loss: 10.199483871459961\n",
      "    load_time_ms: 2.134\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 872960\n",
      "    sample_time_ms: 583.817\n",
      "    update_time_ms: 6.152\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.96666666666667\n",
      "    gpu_util_percent0: 0.06666666666666667\n",
      "    ram_util_percent: 63.86666666666667\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4533238700298042\n",
      "    mean_inference_ms: 0.9833277431572094\n",
      "    mean_processing_ms: 0.24096862044808684\n",
      "  time_since_restore: 466.1372661590576\n",
      "  time_this_iter_s: 2.083918333053589\n",
      "  time_total_s: 466.1372661590576\n",
      "  timestamp: 1573082991\n",
      "  timesteps_since_restore: 880000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 220\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 466 s, 220 iter, 880000 ts, 559 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-29-58\n",
      "  done: false\n",
      "  episode_len_mean: 219.51\n",
      "  episode_reward_max: 595.112344864393\n",
      "  episode_reward_mean: 554.9570305243169\n",
      "  episode_reward_min: 502.5816735974273\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 4915\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1495.667\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.519517421722412\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013779036700725555\n",
      "        policy_loss: -0.0528968945145607\n",
      "        total_loss: 6.935375213623047\n",
      "        vf_explained_var: 0.9975359439849854\n",
      "        vf_loss: 6.978970527648926\n",
      "    load_time_ms: 2.161\n",
      "    num_steps_sampled: 892000\n",
      "    num_steps_trained: 884864\n",
      "    sample_time_ms: 579.723\n",
      "    update_time_ms: 5.767\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.833333333333336\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 63.9\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4528507428910633\n",
      "    mean_inference_ms: 0.9816451613471397\n",
      "    mean_processing_ms: 0.24063346641646127\n",
      "  time_since_restore: 472.3964514732361\n",
      "  time_this_iter_s: 2.0945658683776855\n",
      "  time_total_s: 472.3964514732361\n",
      "  timestamp: 1573082998\n",
      "  timesteps_since_restore: 892000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 892000\n",
      "  training_iteration: 223\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 472 s, 223 iter, 892000 ts, 555 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-30-04\n",
      "  done: false\n",
      "  episode_len_mean: 220.74\n",
      "  episode_reward_max: 599.6422002817878\n",
      "  episode_reward_mean: 559.0458749335316\n",
      "  episode_reward_min: 502.5816735974273\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 4968\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1493.52\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.500713348388672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015528446063399315\n",
      "        policy_loss: -0.05280853435397148\n",
      "        total_loss: 6.437788963317871\n",
      "        vf_explained_var: 0.9977021217346191\n",
      "        vf_loss: 6.48011589050293\n",
      "    load_time_ms: 2.124\n",
      "    num_steps_sampled: 904000\n",
      "    num_steps_trained: 896768\n",
      "    sample_time_ms: 579.052\n",
      "    update_time_ms: 5.381\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.36666666666667\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 63.9\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45256135795371066\n",
      "    mean_inference_ms: 0.9794667553507491\n",
      "    mean_processing_ms: 0.24028737416285\n",
      "  time_since_restore: 478.682284116745\n",
      "  time_this_iter_s: 2.0972225666046143\n",
      "  time_total_s: 478.682284116745\n",
      "  timestamp: 1573083004\n",
      "  timesteps_since_restore: 904000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 904000\n",
      "  training_iteration: 226\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 478 s, 226 iter, 904000 ts, 559 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-30-10\n",
      "  done: false\n",
      "  episode_len_mean: 220.72\n",
      "  episode_reward_max: 603.2979339756603\n",
      "  episode_reward_mean: 561.5380512245417\n",
      "  episode_reward_min: 507.1145352692041\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 5025\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1478.127\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.423467636108398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013830328360199928\n",
      "        policy_loss: -0.04917948693037033\n",
      "        total_loss: 6.743114948272705\n",
      "        vf_explained_var: 0.9977480173110962\n",
      "        vf_loss: 6.782958984375\n",
      "    load_time_ms: 2.115\n",
      "    num_steps_sampled: 916000\n",
      "    num_steps_trained: 908672\n",
      "    sample_time_ms: 584.399\n",
      "    update_time_ms: 5.27\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.233333333333334\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.0\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4527676457811398\n",
      "    mean_inference_ms: 0.9791975532531688\n",
      "    mean_processing_ms: 0.24035098266974736\n",
      "  time_since_restore: 484.9386923313141\n",
      "  time_this_iter_s: 2.062343120574951\n",
      "  time_total_s: 484.9386923313141\n",
      "  timestamp: 1573083010\n",
      "  timesteps_since_restore: 916000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 916000\n",
      "  training_iteration: 229\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 484 s, 229 iter, 916000 ts, 562 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-30-17\n",
      "  done: false\n",
      "  episode_len_mean: 220.1\n",
      "  episode_reward_max: 605.4835773239506\n",
      "  episode_reward_mean: 561.0119712688224\n",
      "  episode_reward_min: 452.70983291637964\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 5079\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.427\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.345202922821045\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013989109545946121\n",
      "        policy_loss: -0.05014204978942871\n",
      "        total_loss: 7.9378981590271\n",
      "        vf_explained_var: 0.9972444176673889\n",
      "        vf_loss: 7.978597640991211\n",
      "    load_time_ms: 2.189\n",
      "    num_steps_sampled: 928000\n",
      "    num_steps_trained: 920576\n",
      "    sample_time_ms: 586.503\n",
      "    update_time_ms: 5.483\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.7\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.0\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45276021342489975\n",
      "    mean_inference_ms: 0.9790130093656555\n",
      "    mean_processing_ms: 0.24035830272873415\n",
      "  time_since_restore: 491.25409603118896\n",
      "  time_this_iter_s: 2.1090526580810547\n",
      "  time_total_s: 491.25409603118896\n",
      "  timestamp: 1573083017\n",
      "  timesteps_since_restore: 928000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 928000\n",
      "  training_iteration: 232\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 491 s, 232 iter, 928000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-30-23\n",
      "  done: false\n",
      "  episode_len_mean: 219.38\n",
      "  episode_reward_max: 607.4806853308103\n",
      "  episode_reward_mean: 559.9275065807834\n",
      "  episode_reward_min: 452.70983291637964\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5133\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1479.91\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.396657466888428\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015485649928450584\n",
      "        policy_loss: -0.059328604489564896\n",
      "        total_loss: 5.305958271026611\n",
      "        vf_explained_var: 0.9982201457023621\n",
      "        vf_loss: 5.354833126068115\n",
      "    load_time_ms: 2.386\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 932480\n",
      "    sample_time_ms: 594.302\n",
      "    update_time_ms: 5.501\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.833333333333336\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 63.96666666666667\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4524645656697534\n",
      "    mean_inference_ms: 0.9782460035596117\n",
      "    mean_processing_ms: 0.2401575555604059\n",
      "  time_since_restore: 497.59864807128906\n",
      "  time_this_iter_s: 2.123892307281494\n",
      "  time_total_s: 497.59864807128906\n",
      "  timestamp: 1573083023\n",
      "  timesteps_since_restore: 940000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 235\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 497 s, 235 iter, 940000 ts, 560 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-30-29\n",
      "  done: false\n",
      "  episode_len_mean: 218.67\n",
      "  episode_reward_max: 607.4806853308103\n",
      "  episode_reward_mean: 561.2997854218852\n",
      "  episode_reward_min: 476.6790846509233\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5190\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1481.863\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.3749823570251465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014031573198735714\n",
      "        policy_loss: -0.04821431636810303\n",
      "        total_loss: 3.955700159072876\n",
      "        vf_explained_var: 0.9987260103225708\n",
      "        vf_loss: 3.9944427013397217\n",
      "    load_time_ms: 2.241\n",
      "    num_steps_sampled: 952000\n",
      "    num_steps_trained: 944384\n",
      "    sample_time_ms: 585.303\n",
      "    update_time_ms: 5.587\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.5\n",
      "    gpu_util_percent0: 0.055\n",
      "    ram_util_percent: 63.9\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45267448607253913\n",
      "    mean_inference_ms: 0.9790436127700655\n",
      "    mean_processing_ms: 0.24021661013580395\n",
      "  time_since_restore: 503.8138689994812\n",
      "  time_this_iter_s: 2.031981945037842\n",
      "  time_total_s: 503.8138689994812\n",
      "  timestamp: 1573083029\n",
      "  timesteps_since_restore: 952000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 952000\n",
      "  training_iteration: 238\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 503 s, 238 iter, 952000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-30-36\n",
      "  done: false\n",
      "  episode_len_mean: 218.28\n",
      "  episode_reward_max: 600.4671807460192\n",
      "  episode_reward_mean: 561.6584352068202\n",
      "  episode_reward_min: 490.57598694424615\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 5244\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1486.351\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.258265495300293\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01621660217642784\n",
      "        policy_loss: -0.05402227118611336\n",
      "        total_loss: 3.8766520023345947\n",
      "        vf_explained_var: 0.9986129403114319\n",
      "        vf_loss: 3.9197278022766113\n",
      "    load_time_ms: 2.421\n",
      "    num_steps_sampled: 964000\n",
      "    num_steps_trained: 956288\n",
      "    sample_time_ms: 589.206\n",
      "    update_time_ms: 5.584\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.5\n",
      "    gpu_util_percent0: 0.24666666666666667\n",
      "    ram_util_percent: 63.96666666666667\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4521659048125729\n",
      "    mean_inference_ms: 0.9786282750624034\n",
      "    mean_processing_ms: 0.24002071954393972\n",
      "  time_since_restore: 510.16454434394836\n",
      "  time_this_iter_s: 2.150804281234741\n",
      "  time_total_s: 510.16454434394836\n",
      "  timestamp: 1573083036\n",
      "  timesteps_since_restore: 964000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 964000\n",
      "  training_iteration: 241\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 510 s, 241 iter, 964000 ts, 562 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-30-42\n",
      "  done: false\n",
      "  episode_len_mean: 216.94\n",
      "  episode_reward_max: 595.2033985710987\n",
      "  episode_reward_mean: 559.9441677517719\n",
      "  episode_reward_min: 452.9994500590396\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 5297\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1481.626\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.2331647872924805\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015249431133270264\n",
      "        policy_loss: -0.05538351461291313\n",
      "        total_loss: 6.344162940979004\n",
      "        vf_explained_var: 0.9978097081184387\n",
      "        vf_loss: 6.389253616333008\n",
      "    load_time_ms: 2.264\n",
      "    num_steps_sampled: 976000\n",
      "    num_steps_trained: 968192\n",
      "    sample_time_ms: 585.988\n",
      "    update_time_ms: 5.762\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.63333333333333\n",
      "    gpu_util_percent0: 0.22333333333333336\n",
      "    ram_util_percent: 64.0\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45216224081089496\n",
      "    mean_inference_ms: 0.9791512009339045\n",
      "    mean_processing_ms: 0.2401951958962014\n",
      "  time_since_restore: 516.4176609516144\n",
      "  time_this_iter_s: 2.085406541824341\n",
      "  time_total_s: 516.4176609516144\n",
      "  timestamp: 1573083042\n",
      "  timesteps_since_restore: 976000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 976000\n",
      "  training_iteration: 244\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 516 s, 244 iter, 976000 ts, 560 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-30-48\n",
      "  done: false\n",
      "  episode_len_mean: 215.73\n",
      "  episode_reward_max: 599.3921564079037\n",
      "  episode_reward_mean: 558.3545059239443\n",
      "  episode_reward_min: 408.79968641900996\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5354\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1481.666\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.2376909255981445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012105719186365604\n",
      "        policy_loss: -0.05339452996850014\n",
      "        total_loss: 10.756064414978027\n",
      "        vf_explained_var: 0.9966006875038147\n",
      "        vf_loss: 10.801288604736328\n",
      "    load_time_ms: 2.175\n",
      "    num_steps_sampled: 988000\n",
      "    num_steps_trained: 980096\n",
      "    sample_time_ms: 585.365\n",
      "    update_time_ms: 5.627\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.35\n",
      "    gpu_util_percent0: 0.23\n",
      "    ram_util_percent: 64.0\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45270364704756133\n",
      "    mean_inference_ms: 0.9816588267281402\n",
      "    mean_processing_ms: 0.24071368498829016\n",
      "  time_since_restore: 522.7209990024567\n",
      "  time_this_iter_s: 2.1147518157958984\n",
      "  time_total_s: 522.7209990024567\n",
      "  timestamp: 1573083048\n",
      "  timesteps_since_restore: 988000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 988000\n",
      "  training_iteration: 247\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 522 s, 247 iter, 988000 ts, 558 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-30-55\n",
      "  done: false\n",
      "  episode_len_mean: 217.1\n",
      "  episode_reward_max: 609.6037435378023\n",
      "  episode_reward_mean: 561.559869903697\n",
      "  episode_reward_min: 408.79968641900996\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5411\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1485.665\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.098174571990967\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015396912582218647\n",
      "        policy_loss: -0.05319773405790329\n",
      "        total_loss: 4.907292366027832\n",
      "        vf_explained_var: 0.9984033107757568\n",
      "        vf_loss: 4.95009708404541\n",
      "    load_time_ms: 2.265\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 992000\n",
      "    sample_time_ms: 590.341\n",
      "    update_time_ms: 5.654\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.93333333333334\n",
      "    gpu_util_percent0: 0.22333333333333336\n",
      "    ram_util_percent: 64.0\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45254386789675377\n",
      "    mean_inference_ms: 0.980446176324345\n",
      "    mean_processing_ms: 0.24037700968412629\n",
      "  time_since_restore: 529.0428898334503\n",
      "  time_this_iter_s: 2.1048431396484375\n",
      "  time_total_s: 529.0428898334503\n",
      "  timestamp: 1573083055\n",
      "  timesteps_since_restore: 1000000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 250\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 529 s, 250 iter, 1000000 ts, 562 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-31-01\n",
      "  done: false\n",
      "  episode_len_mean: 217.05\n",
      "  episode_reward_max: 609.6037435378023\n",
      "  episode_reward_mean: 563.3948868679267\n",
      "  episode_reward_min: 482.6044152872415\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 5464\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1498.278\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.067082405090332\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01264992170035839\n",
      "        policy_loss: -0.045677557587623596\n",
      "        total_loss: 4.004701614379883\n",
      "        vf_explained_var: 0.9986637830734253\n",
      "        vf_loss: 4.041840553283691\n",
      "    load_time_ms: 2.305\n",
      "    num_steps_sampled: 1012000\n",
      "    num_steps_trained: 1003904\n",
      "    sample_time_ms: 586.377\n",
      "    update_time_ms: 5.433\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.733333333333334\n",
      "    gpu_util_percent0: 0.22999999999999998\n",
      "    ram_util_percent: 64.0\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4520740492280737\n",
      "    mean_inference_ms: 0.9774030075177538\n",
      "    mean_processing_ms: 0.23987826156153605\n",
      "  time_since_restore: 535.4458940029144\n",
      "  time_this_iter_s: 2.1434457302093506\n",
      "  time_total_s: 535.4458940029144\n",
      "  timestamp: 1573083061\n",
      "  timesteps_since_restore: 1012000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1012000\n",
      "  training_iteration: 253\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 535 s, 253 iter, 1012000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-31-07\n",
      "  done: false\n",
      "  episode_len_mean: 216.56\n",
      "  episode_reward_max: 607.6122829617291\n",
      "  episode_reward_mean: 563.9982811942625\n",
      "  episode_reward_min: 482.6044152872415\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 5519\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1499.773\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.0276618003845215\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012617815285921097\n",
      "        policy_loss: -0.041947416961193085\n",
      "        total_loss: 4.791377544403076\n",
      "        vf_explained_var: 0.998327910900116\n",
      "        vf_loss: 4.824808120727539\n",
      "    load_time_ms: 2.447\n",
      "    num_steps_sampled: 1024000\n",
      "    num_steps_trained: 1015808\n",
      "    sample_time_ms: 583.973\n",
      "    update_time_ms: 5.727\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.03333333333334\n",
      "    gpu_util_percent0: 0.22666666666666668\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45179621390682867\n",
      "    mean_inference_ms: 0.9778274779173072\n",
      "    mean_processing_ms: 0.2398977472851304\n",
      "  time_since_restore: 541.7089247703552\n",
      "  time_this_iter_s: 2.0686941146850586\n",
      "  time_total_s: 541.7089247703552\n",
      "  timestamp: 1573083067\n",
      "  timesteps_since_restore: 1024000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1024000\n",
      "  training_iteration: 256\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 541 s, 256 iter, 1024000 ts, 564 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-31-14\n",
      "  done: false\n",
      "  episode_len_mean: 217.61\n",
      "  episode_reward_max: 611.8395039933943\n",
      "  episode_reward_mean: 566.5324432638362\n",
      "  episode_reward_min: 485.17607047349816\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 5575\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1498.238\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.014347553253174\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014578187838196754\n",
      "        policy_loss: -0.049235206097364426\n",
      "        total_loss: 4.870715618133545\n",
      "        vf_explained_var: 0.9984496235847473\n",
      "        vf_loss: 4.9101104736328125\n",
      "    load_time_ms: 2.304\n",
      "    num_steps_sampled: 1036000\n",
      "    num_steps_trained: 1027712\n",
      "    sample_time_ms: 579.06\n",
      "    update_time_ms: 5.855\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.833333333333336\n",
      "    gpu_util_percent0: 0.22333333333333336\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45249770524574495\n",
      "    mean_inference_ms: 0.979303564624634\n",
      "    mean_processing_ms: 0.240065692389988\n",
      "  time_since_restore: 547.9776089191437\n",
      "  time_this_iter_s: 2.0853078365325928\n",
      "  time_total_s: 547.9776089191437\n",
      "  timestamp: 1573083074\n",
      "  timesteps_since_restore: 1036000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1036000\n",
      "  training_iteration: 259\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 547 s, 259 iter, 1036000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-31-20\n",
      "  done: false\n",
      "  episode_len_mean: 216.83\n",
      "  episode_reward_max: 615.8048179457509\n",
      "  episode_reward_mean: 566.0932451082779\n",
      "  episode_reward_min: 485.17607047349816\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 5631\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1487.602\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.9124019145965576\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013337169773876667\n",
      "        policy_loss: -0.04717689007520676\n",
      "        total_loss: 2.825054883956909\n",
      "        vf_explained_var: 0.9990766644477844\n",
      "        vf_loss: 2.8632290363311768\n",
      "    load_time_ms: 2.18\n",
      "    num_steps_sampled: 1048000\n",
      "    num_steps_trained: 1039616\n",
      "    sample_time_ms: 579.2\n",
      "    update_time_ms: 5.673\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.800000000000004\n",
      "    gpu_util_percent0: 0.22333333333333336\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45280627858085654\n",
      "    mean_inference_ms: 0.9794265101670819\n",
      "    mean_processing_ms: 0.24030015062099008\n",
      "  time_since_restore: 554.2298431396484\n",
      "  time_this_iter_s: 2.078918933868408\n",
      "  time_total_s: 554.2298431396484\n",
      "  timestamp: 1573083080\n",
      "  timesteps_since_restore: 1048000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1048000\n",
      "  training_iteration: 262\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 554 s, 262 iter, 1048000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-31-26\n",
      "  done: false\n",
      "  episode_len_mean: 218.13\n",
      "  episode_reward_max: 630.7234231735631\n",
      "  episode_reward_mean: 570.6533586971266\n",
      "  episode_reward_min: 529.3321770535533\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 5683\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1478.684\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.9435718059539795\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0156979002058506\n",
      "        policy_loss: -0.04918348789215088\n",
      "        total_loss: 3.2348833084106445\n",
      "        vf_explained_var: 0.9989279508590698\n",
      "        vf_loss: 3.273470163345337\n",
      "    load_time_ms: 2.08\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1051520\n",
      "    sample_time_ms: 582.603\n",
      "    update_time_ms: 5.355\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.13333333333333\n",
      "    gpu_util_percent0: 0.31666666666666665\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45208109834037535\n",
      "    mean_inference_ms: 0.9758742450255656\n",
      "    mean_processing_ms: 0.23958309196628197\n",
      "  time_since_restore: 560.5072658061981\n",
      "  time_this_iter_s: 2.0977978706359863\n",
      "  time_total_s: 560.5072658061981\n",
      "  timestamp: 1573083086\n",
      "  timesteps_since_restore: 1060000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 265\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 560 s, 265 iter, 1060000 ts, 571 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-31-33\n",
      "  done: false\n",
      "  episode_len_mean: 219.18\n",
      "  episode_reward_max: 630.7234231735631\n",
      "  episode_reward_mean: 573.6213298007714\n",
      "  episode_reward_min: 516.4833646040998\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 5740\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1489.055\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.8513810634613037\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01466086320579052\n",
      "        policy_loss: -0.04841480776667595\n",
      "        total_loss: 4.092024803161621\n",
      "        vf_explained_var: 0.9986984133720398\n",
      "        vf_loss: 4.130543231964111\n",
      "    load_time_ms: 2.134\n",
      "    num_steps_sampled: 1072000\n",
      "    num_steps_trained: 1063424\n",
      "    sample_time_ms: 583.347\n",
      "    update_time_ms: 5.323\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.766666666666666\n",
      "    gpu_util_percent0: 0.36999999999999994\n",
      "    ram_util_percent: 64.0\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4523641340457574\n",
      "    mean_inference_ms: 0.9773872282390164\n",
      "    mean_processing_ms: 0.2397186082926405\n",
      "  time_since_restore: 566.871657371521\n",
      "  time_this_iter_s: 2.0925347805023193\n",
      "  time_total_s: 566.871657371521\n",
      "  timestamp: 1573083093\n",
      "  timesteps_since_restore: 1072000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1072000\n",
      "  training_iteration: 268\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 566 s, 268 iter, 1072000 ts, 574 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-31-39\n",
      "  done: false\n",
      "  episode_len_mean: 216.49\n",
      "  episode_reward_max: 615.1978294784511\n",
      "  episode_reward_mean: 569.8506411511203\n",
      "  episode_reward_min: 516.4833646040998\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5796\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1488.503\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.8515117168426514\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016745125874876976\n",
      "        policy_loss: -0.05140318721532822\n",
      "        total_loss: 3.528661012649536\n",
      "        vf_explained_var: 0.998885989189148\n",
      "        vf_loss: 3.5687615871429443\n",
      "    load_time_ms: 2.072\n",
      "    num_steps_sampled: 1084000\n",
      "    num_steps_trained: 1075328\n",
      "    sample_time_ms: 585.751\n",
      "    update_time_ms: 5.475\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.46666666666667\n",
      "    gpu_util_percent0: 0.056666666666666664\n",
      "    ram_util_percent: 64.1\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.452214759666394\n",
      "    mean_inference_ms: 0.978616965093196\n",
      "    mean_processing_ms: 0.2399121590282676\n",
      "  time_since_restore: 573.1496775150299\n",
      "  time_this_iter_s: 2.08986234664917\n",
      "  time_total_s: 573.1496775150299\n",
      "  timestamp: 1573083099\n",
      "  timesteps_since_restore: 1084000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1084000\n",
      "  training_iteration: 271\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 573 s, 271 iter, 1084000 ts, 570 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-31-45\n",
      "  done: false\n",
      "  episode_len_mean: 215.17\n",
      "  episode_reward_max: 628.2186207302723\n",
      "  episode_reward_mean: 568.7039322818313\n",
      "  episode_reward_min: 526.2047804787529\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 5852\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1491.831\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.810857057571411\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01462977658957243\n",
      "        policy_loss: -0.048031218349933624\n",
      "        total_loss: 2.561208963394165\n",
      "        vf_explained_var: 0.9991921782493591\n",
      "        vf_loss: 2.599365234375\n",
      "    load_time_ms: 2.444\n",
      "    num_steps_sampled: 1096000\n",
      "    num_steps_trained: 1087232\n",
      "    sample_time_ms: 584.606\n",
      "    update_time_ms: 5.51\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.86666666666667\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.451648690642826\n",
      "    mean_inference_ms: 0.9757275132710653\n",
      "    mean_processing_ms: 0.23956536133176543\n",
      "  time_since_restore: 579.4439866542816\n",
      "  time_this_iter_s: 2.0877068042755127\n",
      "  time_total_s: 579.4439866542816\n",
      "  timestamp: 1573083105\n",
      "  timesteps_since_restore: 1096000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1096000\n",
      "  training_iteration: 274\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 579 s, 274 iter, 1096000 ts, 569 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-31-52\n",
      "  done: false\n",
      "  episode_len_mean: 215.02\n",
      "  episode_reward_max: 602.9919031221497\n",
      "  episode_reward_mean: 569.1056453680671\n",
      "  episode_reward_min: 504.7375265131879\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 5908\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1481.255\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.809054136276245\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015550332143902779\n",
      "        policy_loss: -0.050749171525239944\n",
      "        total_loss: 2.7167038917541504\n",
      "        vf_explained_var: 0.9991285800933838\n",
      "        vf_loss: 2.7569563388824463\n",
      "    load_time_ms: 2.459\n",
      "    num_steps_sampled: 1108000\n",
      "    num_steps_trained: 1099136\n",
      "    sample_time_ms: 582.701\n",
      "    update_time_ms: 5.806\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.199999999999996\n",
      "    gpu_util_percent0: 0.05333333333333334\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45241790842948676\n",
      "    mean_inference_ms: 0.9774666386129574\n",
      "    mean_processing_ms: 0.23982736357574413\n",
      "  time_since_restore: 585.6967051029205\n",
      "  time_this_iter_s: 2.085765838623047\n",
      "  time_total_s: 585.6967051029205\n",
      "  timestamp: 1573083112\n",
      "  timesteps_since_restore: 1108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1108000\n",
      "  training_iteration: 277\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 585 s, 277 iter, 1108000 ts, 569 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-31-58\n",
      "  done: false\n",
      "  episode_len_mean: 214.16\n",
      "  episode_reward_max: 613.8613198569582\n",
      "  episode_reward_mean: 567.6454161022584\n",
      "  episode_reward_min: 504.7375265131879\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 5965\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1473.497\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.774794578552246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016310544684529305\n",
      "        policy_loss: -0.04751908406615257\n",
      "        total_loss: 4.07625675201416\n",
      "        vf_explained_var: 0.9987491369247437\n",
      "        vf_loss: 4.112766742706299\n",
      "    load_time_ms: 2.326\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1111040\n",
      "    sample_time_ms: 581.504\n",
      "    update_time_ms: 5.817\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.199999999999996\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4521864040906063\n",
      "    mean_inference_ms: 0.9766603540067917\n",
      "    mean_processing_ms: 0.23954802414895454\n",
      "  time_since_restore: 591.8850381374359\n",
      "  time_this_iter_s: 2.0266246795654297\n",
      "  time_total_s: 591.8850381374359\n",
      "  timestamp: 1573083118\n",
      "  timesteps_since_restore: 1120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 280\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 591 s, 280 iter, 1120000 ts, 568 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-32-04\n",
      "  done: false\n",
      "  episode_len_mean: 212.86\n",
      "  episode_reward_max: 613.8613198569582\n",
      "  episode_reward_mean: 566.1784681095082\n",
      "  episode_reward_min: 508.92416745786466\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 6020\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1474.093\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.71921968460083\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013716478832066059\n",
      "        policy_loss: -0.04491373151540756\n",
      "        total_loss: 5.145101070404053\n",
      "        vf_explained_var: 0.9984549283981323\n",
      "        vf_loss: 5.180756568908691\n",
      "    load_time_ms: 2.188\n",
      "    num_steps_sampled: 1132000\n",
      "    num_steps_trained: 1122944\n",
      "    sample_time_ms: 580.316\n",
      "    update_time_ms: 5.915\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.03333333333333\n",
      "    gpu_util_percent0: 0.04666666666666667\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4515980750023141\n",
      "    mean_inference_ms: 0.9755001722410173\n",
      "    mean_processing_ms: 0.2395190676150928\n",
      "  time_since_restore: 598.1907317638397\n",
      "  time_this_iter_s: 2.1228818893432617\n",
      "  time_total_s: 598.1907317638397\n",
      "  timestamp: 1573083124\n",
      "  timesteps_since_restore: 1132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1132000\n",
      "  training_iteration: 283\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 598 s, 283 iter, 1132000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-32-10\n",
      "  done: false\n",
      "  episode_len_mean: 212.24\n",
      "  episode_reward_max: 628.5369183840342\n",
      "  episode_reward_mean: 565.5464792745252\n",
      "  episode_reward_min: 498.79952066717976\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6078\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1470.788\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.7408857345581055\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013846189714968204\n",
      "        policy_loss: -0.047327857464551926\n",
      "        total_loss: 2.906191349029541\n",
      "        vf_explained_var: 0.9990736246109009\n",
      "        vf_loss: 2.944173574447632\n",
      "    load_time_ms: 2.098\n",
      "    num_steps_sampled: 1144000\n",
      "    num_steps_trained: 1134848\n",
      "    sample_time_ms: 583.859\n",
      "    update_time_ms: 6.59\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.00000000000001\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4517961524720935\n",
      "    mean_inference_ms: 0.9764436302463134\n",
      "    mean_processing_ms: 0.2397463023360147\n",
      "  time_since_restore: 604.4426167011261\n",
      "  time_this_iter_s: 2.0372629165649414\n",
      "  time_total_s: 604.4426167011261\n",
      "  timestamp: 1573083130\n",
      "  timesteps_since_restore: 1144000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1144000\n",
      "  training_iteration: 286\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 604 s, 286 iter, 1144000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-32-17\n",
      "  done: false\n",
      "  episode_len_mean: 213.08\n",
      "  episode_reward_max: 614.8865286114027\n",
      "  episode_reward_mean: 567.7872554652283\n",
      "  episode_reward_min: 515.4639898721985\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 6135\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1489.995\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.648141622543335\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016237495467066765\n",
      "        policy_loss: -0.04688393697142601\n",
      "        total_loss: 4.135147571563721\n",
      "        vf_explained_var: 0.9987289905548096\n",
      "        vf_loss: 4.1710710525512695\n",
      "    load_time_ms: 2.444\n",
      "    num_steps_sampled: 1156000\n",
      "    num_steps_trained: 1146752\n",
      "    sample_time_ms: 581.909\n",
      "    update_time_ms: 6.515\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.9\n",
      "    gpu_util_percent0: 0.11666666666666665\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4522294991024774\n",
      "    mean_inference_ms: 0.9772021031443922\n",
      "    mean_processing_ms: 0.23998991792363417\n",
      "  time_since_restore: 610.8702902793884\n",
      "  time_this_iter_s: 2.0992696285247803\n",
      "  time_total_s: 610.8702902793884\n",
      "  timestamp: 1573083137\n",
      "  timesteps_since_restore: 1156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1156000\n",
      "  training_iteration: 289\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 610 s, 289 iter, 1156000 ts, 568 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-32-23\n",
      "  done: false\n",
      "  episode_len_mean: 213.17\n",
      "  episode_reward_max: 614.8865286114027\n",
      "  episode_reward_mean: 568.5736479155149\n",
      "  episode_reward_min: 522.6467562011048\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 6191\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1509.416\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5470025539398193\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016368305310606956\n",
      "        policy_loss: -0.05046403780579567\n",
      "        total_loss: 2.5960323810577393\n",
      "        vf_explained_var: 0.9991687536239624\n",
      "        vf_loss: 2.6354482173919678\n",
      "    load_time_ms: 2.558\n",
      "    num_steps_sampled: 1168000\n",
      "    num_steps_trained: 1158656\n",
      "    sample_time_ms: 590.705\n",
      "    update_time_ms: 6.406\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.666666666666664\n",
      "    gpu_util_percent0: 0.2833333333333333\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4523500316157779\n",
      "    mean_inference_ms: 0.9793220010179939\n",
      "    mean_processing_ms: 0.24008678489761745\n",
      "  time_since_restore: 617.3626067638397\n",
      "  time_this_iter_s: 2.1719350814819336\n",
      "  time_total_s: 617.3626067638397\n",
      "  timestamp: 1573083143\n",
      "  timesteps_since_restore: 1168000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1168000\n",
      "  training_iteration: 292\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 617 s, 292 iter, 1168000 ts, 569 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-32-30\n",
      "  done: false\n",
      "  episode_len_mean: 211.63\n",
      "  episode_reward_max: 620.6245739988548\n",
      "  episode_reward_mean: 564.8505756792651\n",
      "  episode_reward_min: 497.4777449202316\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6246\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1512.0\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5579206943511963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015868959948420525\n",
      "        policy_loss: -0.051646795123815536\n",
      "        total_loss: 4.11116886138916\n",
      "        vf_explained_var: 0.9987037777900696\n",
      "        vf_loss: 4.152104377746582\n",
      "    load_time_ms: 2.555\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1170560\n",
      "    sample_time_ms: 584.77\n",
      "    update_time_ms: 5.557\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.666666666666664\n",
      "    gpu_util_percent0: 0.3833333333333333\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4518037132088872\n",
      "    mean_inference_ms: 0.9771227234792653\n",
      "    mean_processing_ms: 0.23962182449779804\n",
      "  time_since_restore: 623.648814201355\n",
      "  time_this_iter_s: 2.1209139823913574\n",
      "  time_total_s: 623.648814201355\n",
      "  timestamp: 1573083150\n",
      "  timesteps_since_restore: 1180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 295\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 623 s, 295 iter, 1180000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-32-36\n",
      "  done: false\n",
      "  episode_len_mean: 211.06\n",
      "  episode_reward_max: 620.6245739988548\n",
      "  episode_reward_mean: 563.9999228174985\n",
      "  episode_reward_min: 480.5836733929995\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 6303\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1505.223\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5611937046051025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010451463982462883\n",
      "        policy_loss: -0.05073301121592522\n",
      "        total_loss: 5.300771713256836\n",
      "        vf_explained_var: 0.9983648657798767\n",
      "        vf_loss: 5.340922832489014\n",
      "    load_time_ms: 2.428\n",
      "    num_steps_sampled: 1192000\n",
      "    num_steps_trained: 1182464\n",
      "    sample_time_ms: 590.251\n",
      "    update_time_ms: 6.305\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.43333333333334\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45164894453596605\n",
      "    mean_inference_ms: 0.97682203309409\n",
      "    mean_processing_ms: 0.23992916735780864\n",
      "  time_since_restore: 630.0081770420074\n",
      "  time_this_iter_s: 2.1978530883789062\n",
      "  time_total_s: 630.0081770420074\n",
      "  timestamp: 1573083156\n",
      "  timesteps_since_restore: 1192000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1192000\n",
      "  training_iteration: 298\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 630 s, 298 iter, 1192000 ts, 564 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-32-42\n",
      "  done: false\n",
      "  episode_len_mean: 210.04\n",
      "  episode_reward_max: 612.0627325189071\n",
      "  episode_reward_mean: 562.6682027878104\n",
      "  episode_reward_min: 480.5836733929995\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 6360\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1490.359\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.51640248298645\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012242613360285759\n",
      "        policy_loss: -0.03972822055220604\n",
      "        total_loss: 3.4670355319976807\n",
      "        vf_explained_var: 0.998868465423584\n",
      "        vf_loss: 3.494368076324463\n",
      "    load_time_ms: 2.205\n",
      "    num_steps_sampled: 1204000\n",
      "    num_steps_trained: 1194368\n",
      "    sample_time_ms: 586.484\n",
      "    update_time_ms: 6.262\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.03333333333334\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4518096583146414\n",
      "    mean_inference_ms: 0.9763198227971875\n",
      "    mean_processing_ms: 0.2398310142397599\n",
      "  time_since_restore: 636.2404117584229\n",
      "  time_this_iter_s: 2.06516695022583\n",
      "  time_total_s: 636.2404117584229\n",
      "  timestamp: 1573083162\n",
      "  timesteps_since_restore: 1204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1204000\n",
      "  training_iteration: 301\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 636 s, 301 iter, 1204000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-32-49\n",
      "  done: false\n",
      "  episode_len_mean: 208.59\n",
      "  episode_reward_max: 617.8034951062443\n",
      "  episode_reward_mean: 560.6263031658513\n",
      "  episode_reward_min: 498.157100340964\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6418\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.526\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5119967460632324\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011792805977165699\n",
      "        policy_loss: -0.046299371868371964\n",
      "        total_loss: 3.112388849258423\n",
      "        vf_explained_var: 0.9990456700325012\n",
      "        vf_loss: 3.1467480659484863\n",
      "    load_time_ms: 2.359\n",
      "    num_steps_sampled: 1216000\n",
      "    num_steps_trained: 1206272\n",
      "    sample_time_ms: 585.414\n",
      "    update_time_ms: 5.936\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.9\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45177645371528813\n",
      "    mean_inference_ms: 0.9757091658130079\n",
      "    mean_processing_ms: 0.23970269357772572\n",
      "  time_since_restore: 642.4876053333282\n",
      "  time_this_iter_s: 2.090022325515747\n",
      "  time_total_s: 642.4876053333282\n",
      "  timestamp: 1573083169\n",
      "  timesteps_since_restore: 1216000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1216000\n",
      "  training_iteration: 304\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 642 s, 304 iter, 1216000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-32-55\n",
      "  done: false\n",
      "  episode_len_mean: 208.36\n",
      "  episode_reward_max: 616.9594971634962\n",
      "  episode_reward_mean: 560.0040091648448\n",
      "  episode_reward_min: 499.35937384744926\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 6476\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.987\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4524002075195312\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011578126810491085\n",
      "        policy_loss: -0.04634983465075493\n",
      "        total_loss: 2.873546838760376\n",
      "        vf_explained_var: 0.9991105794906616\n",
      "        vf_loss: 2.9081737995147705\n",
      "    load_time_ms: 2.348\n",
      "    num_steps_sampled: 1228000\n",
      "    num_steps_trained: 1218176\n",
      "    sample_time_ms: 586.446\n",
      "    update_time_ms: 5.408\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.300000000000004\n",
      "    gpu_util_percent0: 0.05333333333333334\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4521098870325259\n",
      "    mean_inference_ms: 0.9768429003311202\n",
      "    mean_processing_ms: 0.23987154112908407\n",
      "  time_since_restore: 648.7829096317291\n",
      "  time_this_iter_s: 2.1169252395629883\n",
      "  time_total_s: 648.7829096317291\n",
      "  timestamp: 1573083175\n",
      "  timesteps_since_restore: 1228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1228000\n",
      "  training_iteration: 307\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 648 s, 307 iter, 1228000 ts, 560 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-33-01\n",
      "  done: false\n",
      "  episode_len_mean: 206.7\n",
      "  episode_reward_max: 598.1836504034367\n",
      "  episode_reward_mean: 556.8939758093076\n",
      "  episode_reward_min: 491.9467105017255\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6535\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1481.983\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4723901748657227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01438450813293457\n",
      "        policy_loss: -0.0430176742374897\n",
      "        total_loss: 3.013181209564209\n",
      "        vf_explained_var: 0.9990788698196411\n",
      "        vf_loss: 3.0416343212127686\n",
      "    load_time_ms: 2.474\n",
      "    num_steps_sampled: 1240000\n",
      "    num_steps_trained: 1230080\n",
      "    sample_time_ms: 580.727\n",
      "    update_time_ms: 5.659\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.03333333333333\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4521119669586161\n",
      "    mean_inference_ms: 0.9763933270355817\n",
      "    mean_processing_ms: 0.23986047104286243\n",
      "  time_since_restore: 655.0762116909027\n",
      "  time_this_iter_s: 2.106879234313965\n",
      "  time_total_s: 655.0762116909027\n",
      "  timestamp: 1573083181\n",
      "  timesteps_since_restore: 1240000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1240000\n",
      "  training_iteration: 310\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 655 s, 310 iter, 1240000 ts, 557 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-33-08\n",
      "  done: false\n",
      "  episode_len_mean: 205.02\n",
      "  episode_reward_max: 597.5019938989476\n",
      "  episode_reward_mean: 553.568141828618\n",
      "  episode_reward_min: 457.6972806237901\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 6595\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1491.806\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.477139949798584\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012399660423398018\n",
      "        policy_loss: -0.05029606819152832\n",
      "        total_loss: 3.830313205718994\n",
      "        vf_explained_var: 0.9988635182380676\n",
      "        vf_loss: 3.8680543899536133\n",
      "    load_time_ms: 2.364\n",
      "    num_steps_sampled: 1252000\n",
      "    num_steps_trained: 1241984\n",
      "    sample_time_ms: 582.366\n",
      "    update_time_ms: 6.057\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.3\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4516042740213762\n",
      "    mean_inference_ms: 0.9749474616080924\n",
      "    mean_processing_ms: 0.23978791956301504\n",
      "  time_since_restore: 661.4198052883148\n",
      "  time_this_iter_s: 2.081061840057373\n",
      "  time_total_s: 661.4198052883148\n",
      "  timestamp: 1573083188\n",
      "  timesteps_since_restore: 1252000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1252000\n",
      "  training_iteration: 313\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 661 s, 313 iter, 1252000 ts, 554 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-33-14\n",
      "  done: false\n",
      "  episode_len_mean: 205.93\n",
      "  episode_reward_max: 610.7729929834678\n",
      "  episode_reward_mean: 556.2120712217167\n",
      "  episode_reward_min: 486.3903443292478\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 6654\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1492.135\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4179418087005615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012803575955331326\n",
      "        policy_loss: -0.04857780039310455\n",
      "        total_loss: 2.6149673461914062\n",
      "        vf_explained_var: 0.9992110133171082\n",
      "        vf_loss: 2.6505820751190186\n",
      "    load_time_ms: 2.379\n",
      "    num_steps_sampled: 1264000\n",
      "    num_steps_trained: 1253888\n",
      "    sample_time_ms: 581.091\n",
      "    update_time_ms: 6.097\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.65\n",
      "    gpu_util_percent0: 0.11499999999999999\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45173137681735526\n",
      "    mean_inference_ms: 0.9761710996687566\n",
      "    mean_processing_ms: 0.23999012382774856\n",
      "  time_since_restore: 667.6723778247833\n",
      "  time_this_iter_s: 2.0651886463165283\n",
      "  time_total_s: 667.6723778247833\n",
      "  timestamp: 1573083194\n",
      "  timesteps_since_restore: 1264000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1264000\n",
      "  training_iteration: 316\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 667 s, 316 iter, 1264000 ts, 556 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-33-20\n",
      "  done: false\n",
      "  episode_len_mean: 207.95\n",
      "  episode_reward_max: 610.7729929834678\n",
      "  episode_reward_mean: 561.1220912998594\n",
      "  episode_reward_min: 486.3903443292478\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 6708\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1485.656\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3871614933013916\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013505333103239536\n",
      "        policy_loss: -0.04871140420436859\n",
      "        total_loss: 2.8861312866210938\n",
      "        vf_explained_var: 0.9991468787193298\n",
      "        vf_loss: 2.9211678504943848\n",
      "    load_time_ms: 2.334\n",
      "    num_steps_sampled: 1276000\n",
      "    num_steps_trained: 1265792\n",
      "    sample_time_ms: 582.137\n",
      "    update_time_ms: 5.48\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.53333333333334\n",
      "    gpu_util_percent0: 0.2333333333333333\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45135422733657043\n",
      "    mean_inference_ms: 0.9750528994958411\n",
      "    mean_processing_ms: 0.23965394242078417\n",
      "  time_since_restore: 673.9214818477631\n",
      "  time_this_iter_s: 2.0410494804382324\n",
      "  time_total_s: 673.9214818477631\n",
      "  timestamp: 1573083200\n",
      "  timesteps_since_restore: 1276000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1276000\n",
      "  training_iteration: 319\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 673 s, 319 iter, 1276000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-33-27\n",
      "  done: false\n",
      "  episode_len_mean: 207.62\n",
      "  episode_reward_max: 608.8065620075331\n",
      "  episode_reward_mean: 560.87286838631\n",
      "  episode_reward_min: 513.023636402601\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 6766\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1478.364\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3266892433166504\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014175359159708023\n",
      "        policy_loss: -0.04949023574590683\n",
      "        total_loss: 2.6619699001312256\n",
      "        vf_explained_var: 0.9991576075553894\n",
      "        vf_loss: 2.6971075534820557\n",
      "    load_time_ms: 2.322\n",
      "    num_steps_sampled: 1288000\n",
      "    num_steps_trained: 1277696\n",
      "    sample_time_ms: 582.141\n",
      "    update_time_ms: 5.313\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.300000000000004\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45162306388411994\n",
      "    mean_inference_ms: 0.9755899524847591\n",
      "    mean_processing_ms: 0.23978245482523597\n",
      "  time_since_restore: 680.2211513519287\n",
      "  time_this_iter_s: 2.093904733657837\n",
      "  time_total_s: 680.2211513519287\n",
      "  timestamp: 1573083207\n",
      "  timesteps_since_restore: 1288000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1288000\n",
      "  training_iteration: 322\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 680 s, 322 iter, 1288000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-33-33\n",
      "  done: false\n",
      "  episode_len_mean: 207.28\n",
      "  episode_reward_max: 600.4597732779197\n",
      "  episode_reward_mean: 561.1967278625982\n",
      "  episode_reward_min: 420.1861286963796\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6825\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1474.917\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.361057996749878\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014786319807171822\n",
      "        policy_loss: -0.04723239690065384\n",
      "        total_loss: 2.806220531463623\n",
      "        vf_explained_var: 0.9991612434387207\n",
      "        vf_loss: 2.8384814262390137\n",
      "    load_time_ms: 2.294\n",
      "    num_steps_sampled: 1300000\n",
      "    num_steps_trained: 1289600\n",
      "    sample_time_ms: 581.616\n",
      "    update_time_ms: 5.337\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.3\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.55\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45195404443186654\n",
      "    mean_inference_ms: 0.9773286252432182\n",
      "    mean_processing_ms: 0.2399885169294608\n",
      "  time_since_restore: 686.4602735042572\n",
      "  time_this_iter_s: 2.0582973957061768\n",
      "  time_total_s: 686.4602735042572\n",
      "  timestamp: 1573083213\n",
      "  timesteps_since_restore: 1300000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1300000\n",
      "  training_iteration: 325\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 686 s, 325 iter, 1300000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-33-39\n",
      "  done: false\n",
      "  episode_len_mean: 206.36\n",
      "  episode_reward_max: 613.2857070233873\n",
      "  episode_reward_mean: 559.4322555726628\n",
      "  episode_reward_min: 420.1861286963796\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6884\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1475.649\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3453783988952637\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013075371272861958\n",
      "        policy_loss: -0.05218519642949104\n",
      "        total_loss: 3.8661949634552\n",
      "        vf_explained_var: 0.9988528490066528\n",
      "        vf_loss: 3.9051411151885986\n",
      "    load_time_ms: 2.094\n",
      "    num_steps_sampled: 1312000\n",
      "    num_steps_trained: 1301504\n",
      "    sample_time_ms: 579.652\n",
      "    update_time_ms: 5.906\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.6\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.6\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45106051191625796\n",
      "    mean_inference_ms: 0.9747137730697193\n",
      "    mean_processing_ms: 0.239338526893498\n",
      "  time_since_restore: 692.7196800708771\n",
      "  time_this_iter_s: 2.1079142093658447\n",
      "  time_total_s: 692.7196800708771\n",
      "  timestamp: 1573083219\n",
      "  timesteps_since_restore: 1312000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1312000\n",
      "  training_iteration: 328\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 692 s, 328 iter, 1312000 ts, 559 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-33-45\n",
      "  done: false\n",
      "  episode_len_mean: 207.37\n",
      "  episode_reward_max: 608.0574862376438\n",
      "  episode_reward_mean: 561.8866628960853\n",
      "  episode_reward_min: 505.42336667707275\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6942\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1478.909\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2949161529541016\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012158434838056564\n",
      "        policy_loss: -0.04739028960466385\n",
      "        total_loss: 3.5808029174804688\n",
      "        vf_explained_var: 0.9989076852798462\n",
      "        vf_loss: 3.6158831119537354\n",
      "    load_time_ms: 2.165\n",
      "    num_steps_sampled: 1324000\n",
      "    num_steps_trained: 1313408\n",
      "    sample_time_ms: 581.023\n",
      "    update_time_ms: 6.169\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.266666666666666\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.46666666666667\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4519313932061715\n",
      "    mean_inference_ms: 0.9779672709886988\n",
      "    mean_processing_ms: 0.2400666317890794\n",
      "  time_since_restore: 699.0102751255035\n",
      "  time_this_iter_s: 2.1358489990234375\n",
      "  time_total_s: 699.0102751255035\n",
      "  timestamp: 1573083225\n",
      "  timesteps_since_restore: 1324000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1324000\n",
      "  training_iteration: 331\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 699 s, 331 iter, 1324000 ts, 562 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-33-52\n",
      "  done: false\n",
      "  episode_len_mean: 207.83\n",
      "  episode_reward_max: 621.8651098679721\n",
      "  episode_reward_mean: 563.0373501056071\n",
      "  episode_reward_min: 483.61642599457025\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6999\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1477.509\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3027327060699463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011907096020877361\n",
      "        policy_loss: -0.0464017428457737\n",
      "        total_loss: 3.870232582092285\n",
      "        vf_explained_var: 0.9988418221473694\n",
      "        vf_loss: 3.9045779705047607\n",
      "    load_time_ms: 2.25\n",
      "    num_steps_sampled: 1336000\n",
      "    num_steps_trained: 1325312\n",
      "    sample_time_ms: 581.567\n",
      "    update_time_ms: 6.037\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.63333333333333\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4517855067230907\n",
      "    mean_inference_ms: 0.9775837227689438\n",
      "    mean_processing_ms: 0.24012276695416168\n",
      "  time_since_restore: 705.2677135467529\n",
      "  time_this_iter_s: 2.095299482345581\n",
      "  time_total_s: 705.2677135467529\n",
      "  timestamp: 1573083232\n",
      "  timesteps_since_restore: 1336000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1336000\n",
      "  training_iteration: 334\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 705 s, 334 iter, 1336000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-33-58\n",
      "  done: false\n",
      "  episode_len_mean: 207.29\n",
      "  episode_reward_max: 621.8651098679721\n",
      "  episode_reward_mean: 561.6090060666234\n",
      "  episode_reward_min: 483.61642599457025\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 7058\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1481.395\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3071393966674805\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013920217752456665\n",
      "        policy_loss: -0.048398446291685104\n",
      "        total_loss: 3.139531373977661\n",
      "        vf_explained_var: 0.9990637302398682\n",
      "        vf_loss: 3.1738357543945312\n",
      "    load_time_ms: 2.337\n",
      "    num_steps_sampled: 1348000\n",
      "    num_steps_trained: 1337216\n",
      "    sample_time_ms: 585.951\n",
      "    update_time_ms: 5.92\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.300000000000004\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45152178393457626\n",
      "    mean_inference_ms: 0.9770597718447195\n",
      "    mean_processing_ms: 0.23980924445656562\n",
      "  time_since_restore: 711.5601398944855\n",
      "  time_this_iter_s: 2.103379249572754\n",
      "  time_total_s: 711.5601398944855\n",
      "  timestamp: 1573083238\n",
      "  timesteps_since_restore: 1348000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1348000\n",
      "  training_iteration: 337\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 711 s, 337 iter, 1348000 ts, 562 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-34-04\n",
      "  done: false\n",
      "  episode_len_mean: 205.49\n",
      "  episode_reward_max: 605.0582086530433\n",
      "  episode_reward_mean: 558.6186295105122\n",
      "  episode_reward_min: 494.24564442541555\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7115\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1477.36\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.326322555541992\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012845400720834732\n",
      "        policy_loss: -0.05047877877950668\n",
      "        total_loss: 3.684910535812378\n",
      "        vf_explained_var: 0.9989100098609924\n",
      "        vf_loss: 3.7223830223083496\n",
      "    load_time_ms: 2.531\n",
      "    num_steps_sampled: 1360000\n",
      "    num_steps_trained: 1349120\n",
      "    sample_time_ms: 589.495\n",
      "    update_time_ms: 5.785\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.99999999999999\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4517898536277139\n",
      "    mean_inference_ms: 0.978157687540884\n",
      "    mean_processing_ms: 0.24016620253854945\n",
      "  time_since_restore: 717.8168613910675\n",
      "  time_this_iter_s: 2.08478045463562\n",
      "  time_total_s: 717.8168613910675\n",
      "  timestamp: 1573083244\n",
      "  timesteps_since_restore: 1360000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1360000\n",
      "  training_iteration: 340\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 717 s, 340 iter, 1360000 ts, 559 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-34-11\n",
      "  done: false\n",
      "  episode_len_mean: 205.89\n",
      "  episode_reward_max: 607.8713021803574\n",
      "  episode_reward_mean: 560.4614955200886\n",
      "  episode_reward_min: 494.24564442541555\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7173\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1485.57\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2680978775024414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012154956348240376\n",
      "        policy_loss: -0.04402758926153183\n",
      "        total_loss: 4.2469611167907715\n",
      "        vf_explained_var: 0.9986703991889954\n",
      "        vf_loss: 4.27868127822876\n",
      "    load_time_ms: 2.463\n",
      "    num_steps_sampled: 1372000\n",
      "    num_steps_trained: 1361024\n",
      "    sample_time_ms: 597.153\n",
      "    update_time_ms: 5.772\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.6\n",
      "    gpu_util_percent0: 0.14666666666666667\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4512565871900648\n",
      "    mean_inference_ms: 0.9752390602706177\n",
      "    mean_processing_ms: 0.2396275991400041\n",
      "  time_since_restore: 724.2816271781921\n",
      "  time_this_iter_s: 2.0967695713043213\n",
      "  time_total_s: 724.2816271781921\n",
      "  timestamp: 1573083251\n",
      "  timesteps_since_restore: 1372000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1372000\n",
      "  training_iteration: 343\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 724 s, 343 iter, 1372000 ts, 560 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-34-17\n",
      "  done: false\n",
      "  episode_len_mean: 206.18\n",
      "  episode_reward_max: 621.0572176610034\n",
      "  episode_reward_mean: 561.2657468518174\n",
      "  episode_reward_min: 499.42603165139553\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7233\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1491.622\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3019285202026367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013422761112451553\n",
      "        policy_loss: -0.047936636954545975\n",
      "        total_loss: 2.4163005352020264\n",
      "        vf_explained_var: 0.9992877840995789\n",
      "        vf_loss: 2.45064640045166\n",
      "    load_time_ms: 2.604\n",
      "    num_steps_sampled: 1384000\n",
      "    num_steps_trained: 1372928\n",
      "    sample_time_ms: 597.993\n",
      "    update_time_ms: 5.807\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.666666666666664\n",
      "    gpu_util_percent0: 0.27\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45158329634177025\n",
      "    mean_inference_ms: 0.9771715429131501\n",
      "    mean_processing_ms: 0.23976616682098498\n",
      "  time_since_restore: 730.6421213150024\n",
      "  time_this_iter_s: 2.13905930519104\n",
      "  time_total_s: 730.6421213150024\n",
      "  timestamp: 1573083257\n",
      "  timesteps_since_restore: 1384000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1384000\n",
      "  training_iteration: 346\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 730 s, 346 iter, 1384000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-34-24\n",
      "  done: false\n",
      "  episode_len_mean: 208.63\n",
      "  episode_reward_max: 621.0572176610034\n",
      "  episode_reward_mean: 566.8355156567286\n",
      "  episode_reward_min: 500.71006927787107\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7291\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1497.826\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.253441333770752\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012353508733212948\n",
      "        policy_loss: -0.042940232902765274\n",
      "        total_loss: 4.026658535003662\n",
      "        vf_explained_var: 0.998815655708313\n",
      "        vf_loss: 4.057090759277344\n",
      "    load_time_ms: 2.496\n",
      "    num_steps_sampled: 1396000\n",
      "    num_steps_trained: 1384832\n",
      "    sample_time_ms: 601.581\n",
      "    update_time_ms: 5.913\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.56666666666667\n",
      "    gpu_util_percent0: 0.37333333333333335\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4514996476655213\n",
      "    mean_inference_ms: 0.9768393248866251\n",
      "    mean_processing_ms: 0.23971207608477724\n",
      "  time_since_restore: 737.0163164138794\n",
      "  time_this_iter_s: 2.1281533241271973\n",
      "  time_total_s: 737.0163164138794\n",
      "  timestamp: 1573083264\n",
      "  timesteps_since_restore: 1396000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1396000\n",
      "  training_iteration: 349\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 737 s, 349 iter, 1396000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-34-30\n",
      "  done: false\n",
      "  episode_len_mean: 207.35\n",
      "  episode_reward_max: 614.1328141941691\n",
      "  episode_reward_mean: 564.6064979483778\n",
      "  episode_reward_min: 507.5459322271669\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7350\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1488.778\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.229912519454956\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011178060434758663\n",
      "        policy_loss: -0.04051068052649498\n",
      "        total_loss: 3.885131597518921\n",
      "        vf_explained_var: 0.9988304376602173\n",
      "        vf_loss: 3.9143247604370117\n",
      "    load_time_ms: 2.278\n",
      "    num_steps_sampled: 1408000\n",
      "    num_steps_trained: 1396736\n",
      "    sample_time_ms: 593.011\n",
      "    update_time_ms: 5.777\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.53333333333333\n",
      "    gpu_util_percent0: 0.36000000000000004\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45159881854859635\n",
      "    mean_inference_ms: 0.9776112135615367\n",
      "    mean_processing_ms: 0.2399266159311561\n",
      "  time_since_restore: 743.2898015975952\n",
      "  time_this_iter_s: 2.0843396186828613\n",
      "  time_total_s: 743.2898015975952\n",
      "  timestamp: 1573083270\n",
      "  timesteps_since_restore: 1408000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1408000\n",
      "  training_iteration: 352\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 743 s, 352 iter, 1408000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-34-36\n",
      "  done: false\n",
      "  episode_len_mean: 205.55\n",
      "  episode_reward_max: 599.5676544580366\n",
      "  episode_reward_mean: 561.069911748786\n",
      "  episode_reward_min: 507.5459322271669\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 7408\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1488.73\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.166874408721924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013297193683683872\n",
      "        policy_loss: -0.051089126616716385\n",
      "        total_loss: 2.0717480182647705\n",
      "        vf_explained_var: 0.9993913173675537\n",
      "        vf_loss: 2.1093738079071045\n",
      "    load_time_ms: 2.491\n",
      "    num_steps_sampled: 1420000\n",
      "    num_steps_trained: 1408640\n",
      "    sample_time_ms: 599.002\n",
      "    update_time_ms: 5.772\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.166666666666664\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45120603530780484\n",
      "    mean_inference_ms: 0.9766232924843088\n",
      "    mean_processing_ms: 0.23988363582255892\n",
      "  time_since_restore: 749.6574904918671\n",
      "  time_this_iter_s: 2.1559340953826904\n",
      "  time_total_s: 749.6574904918671\n",
      "  timestamp: 1573083276\n",
      "  timesteps_since_restore: 1420000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1420000\n",
      "  training_iteration: 355\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 749 s, 355 iter, 1420000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-34-43\n",
      "  done: false\n",
      "  episode_len_mean: 205.38\n",
      "  episode_reward_max: 600.2218831699647\n",
      "  episode_reward_mean: 560.567273765044\n",
      "  episode_reward_min: 506.7271663076865\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7463\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1485.329\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1885125637054443\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012083735316991806\n",
      "        policy_loss: -0.049422286450862885\n",
      "        total_loss: 2.7929253578186035\n",
      "        vf_explained_var: 0.9991701245307922\n",
      "        vf_loss: 2.830112934112549\n",
      "    load_time_ms: 2.207\n",
      "    num_steps_sampled: 1432000\n",
      "    num_steps_trained: 1420544\n",
      "    sample_time_ms: 591.383\n",
      "    update_time_ms: 5.776\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.233333333333334\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45087200152310486\n",
      "    mean_inference_ms: 0.9747862247914392\n",
      "    mean_processing_ms: 0.2396857392851594\n",
      "  time_since_restore: 755.9317858219147\n",
      "  time_this_iter_s: 2.10905122756958\n",
      "  time_total_s: 755.9317858219147\n",
      "  timestamp: 1573083283\n",
      "  timesteps_since_restore: 1432000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1432000\n",
      "  training_iteration: 358\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 755 s, 358 iter, 1432000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-34-49\n",
      "  done: false\n",
      "  episode_len_mean: 205.9\n",
      "  episode_reward_max: 600.7952133775544\n",
      "  episode_reward_mean: 562.8435206768065\n",
      "  episode_reward_min: 506.7271663076865\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7521\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1480.518\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.109046697616577\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013492057099938393\n",
      "        policy_loss: -0.04698583483695984\n",
      "        total_loss: 2.231823682785034\n",
      "        vf_explained_var: 0.9993305802345276\n",
      "        vf_loss: 2.2651493549346924\n",
      "    load_time_ms: 2.436\n",
      "    num_steps_sampled: 1444000\n",
      "    num_steps_trained: 1432448\n",
      "    sample_time_ms: 588.328\n",
      "    update_time_ms: 5.675\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.96666666666667\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45042173208279174\n",
      "    mean_inference_ms: 0.972773453352021\n",
      "    mean_processing_ms: 0.23933569229724774\n",
      "  time_since_restore: 762.1674704551697\n",
      "  time_this_iter_s: 2.0620131492614746\n",
      "  time_total_s: 762.1674704551697\n",
      "  timestamp: 1573083289\n",
      "  timesteps_since_restore: 1444000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1444000\n",
      "  training_iteration: 361\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 762 s, 361 iter, 1444000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-34-55\n",
      "  done: false\n",
      "  episode_len_mean: 204.57\n",
      "  episode_reward_max: 601.2671790574774\n",
      "  episode_reward_mean: 561.1492933154906\n",
      "  episode_reward_min: 498.72839995188\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7581\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1478.115\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1324121952056885\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012812293134629726\n",
      "        policy_loss: -0.04243893921375275\n",
      "        total_loss: 4.160747528076172\n",
      "        vf_explained_var: 0.9987890124320984\n",
      "        vf_loss: 4.190213680267334\n",
      "    load_time_ms: 2.314\n",
      "    num_steps_sampled: 1456000\n",
      "    num_steps_trained: 1444352\n",
      "    sample_time_ms: 591.015\n",
      "    update_time_ms: 5.487\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.06666666666667\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45078879946367034\n",
      "    mean_inference_ms: 0.9743918274607141\n",
      "    mean_processing_ms: 0.23947452821881207\n",
      "  time_since_restore: 768.4643578529358\n",
      "  time_this_iter_s: 2.0926644802093506\n",
      "  time_total_s: 768.4643578529358\n",
      "  timestamp: 1573083295\n",
      "  timesteps_since_restore: 1456000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1456000\n",
      "  training_iteration: 364\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 768 s, 364 iter, 1456000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-35-02\n",
      "  done: false\n",
      "  episode_len_mean: 202.78\n",
      "  episode_reward_max: 601.2671790574774\n",
      "  episode_reward_mean: 556.8152973237952\n",
      "  episode_reward_min: 501.2370141793804\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7640\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1483.16\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.093496561050415\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0131577979773283\n",
      "        policy_loss: -0.050575535744428635\n",
      "        total_loss: 3.016808032989502\n",
      "        vf_explained_var: 0.9990988373756409\n",
      "        vf_loss: 3.0540614128112793\n",
      "    load_time_ms: 2.506\n",
      "    num_steps_sampled: 1468000\n",
      "    num_steps_trained: 1456256\n",
      "    sample_time_ms: 589.345\n",
      "    update_time_ms: 5.233\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.833333333333336\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4509860910390758\n",
      "    mean_inference_ms: 0.9755132227995744\n",
      "    mean_processing_ms: 0.23957771612389184\n",
      "  time_since_restore: 774.8201565742493\n",
      "  time_this_iter_s: 2.111591100692749\n",
      "  time_total_s: 774.8201565742493\n",
      "  timestamp: 1573083302\n",
      "  timesteps_since_restore: 1468000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1468000\n",
      "  training_iteration: 367\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 774 s, 367 iter, 1468000 ts, 557 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-35-08\n",
      "  done: false\n",
      "  episode_len_mean: 202.9\n",
      "  episode_reward_max: 611.9891741772113\n",
      "  episode_reward_mean: 557.0754237627177\n",
      "  episode_reward_min: 509.72405019906864\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7700\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1477.767\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1292707920074463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014050251804292202\n",
      "        policy_loss: -0.05107571929693222\n",
      "        total_loss: 3.634937047958374\n",
      "        vf_explained_var: 0.9989371299743652\n",
      "        vf_loss: 3.6717875003814697\n",
      "    load_time_ms: 2.498\n",
      "    num_steps_sampled: 1480000\n",
      "    num_steps_trained: 1468160\n",
      "    sample_time_ms: 596.085\n",
      "    update_time_ms: 5.007\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.333333333333336\n",
      "    gpu_util_percent0: 0.18333333333333335\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45113555633476693\n",
      "    mean_inference_ms: 0.9751699916389132\n",
      "    mean_processing_ms: 0.23967644278232958\n",
      "  time_since_restore: 781.1162352561951\n",
      "  time_this_iter_s: 2.077664613723755\n",
      "  time_total_s: 781.1162352561951\n",
      "  timestamp: 1573083308\n",
      "  timesteps_since_restore: 1480000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1480000\n",
      "  training_iteration: 370\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 781 s, 370 iter, 1480000 ts, 557 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-35-14\n",
      "  done: false\n",
      "  episode_len_mean: 203.38\n",
      "  episode_reward_max: 594.0337521650199\n",
      "  episode_reward_mean: 558.091194613902\n",
      "  episode_reward_min: 504.26808675830654\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7760\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1478.999\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.036649465560913\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013657324947416782\n",
      "        policy_loss: -0.04531446471810341\n",
      "        total_loss: 2.2627975940704346\n",
      "        vf_explained_var: 0.9993536472320557\n",
      "        vf_loss: 2.294283628463745\n",
      "    load_time_ms: 2.525\n",
      "    num_steps_sampled: 1492000\n",
      "    num_steps_trained: 1480064\n",
      "    sample_time_ms: 591.557\n",
      "    update_time_ms: 4.954\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.73333333333333\n",
      "    gpu_util_percent0: 0.3633333333333333\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45134979534033953\n",
      "    mean_inference_ms: 0.97518176030469\n",
      "    mean_processing_ms: 0.23974358632776707\n",
      "  time_since_restore: 787.3521106243134\n",
      "  time_this_iter_s: 2.085239887237549\n",
      "  time_total_s: 787.3521106243134\n",
      "  timestamp: 1573083314\n",
      "  timesteps_since_restore: 1492000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1492000\n",
      "  training_iteration: 373\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 787 s, 373 iter, 1492000 ts, 558 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-35-21\n",
      "  done: false\n",
      "  episode_len_mean: 204.04\n",
      "  episode_reward_max: 628.3012821135164\n",
      "  episode_reward_mean: 560.6668359772431\n",
      "  episode_reward_min: 520.4345777630956\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 7820\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1473.865\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0378148555755615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015254261903464794\n",
      "        policy_loss: -0.045199036598205566\n",
      "        total_loss: 4.3847551345825195\n",
      "        vf_explained_var: 0.9987673759460449\n",
      "        vf_loss: 4.414509296417236\n",
      "    load_time_ms: 2.359\n",
      "    num_steps_sampled: 1504000\n",
      "    num_steps_trained: 1491968\n",
      "    sample_time_ms: 591.054\n",
      "    update_time_ms: 4.901\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.2\n",
      "    gpu_util_percent0: 0.22333333333333336\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4511721730939637\n",
      "    mean_inference_ms: 0.9758841461194074\n",
      "    mean_processing_ms: 0.23973241595207917\n",
      "  time_since_restore: 793.6318082809448\n",
      "  time_this_iter_s: 2.096270799636841\n",
      "  time_total_s: 793.6318082809448\n",
      "  timestamp: 1573083321\n",
      "  timesteps_since_restore: 1504000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1504000\n",
      "  training_iteration: 376\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 793 s, 376 iter, 1504000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-35-27\n",
      "  done: false\n",
      "  episode_len_mean: 205.08\n",
      "  episode_reward_max: 628.3012821135164\n",
      "  episode_reward_mean: 561.5713120600333\n",
      "  episode_reward_min: 509.695440682841\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7876\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1475.524\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.923734188079834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012651341035962105\n",
      "        policy_loss: -0.04822240769863129\n",
      "        total_loss: 4.303329944610596\n",
      "        vf_explained_var: 0.9986817836761475\n",
      "        vf_loss: 4.338742733001709\n",
      "    load_time_ms: 2.122\n",
      "    num_steps_sampled: 1516000\n",
      "    num_steps_trained: 1503872\n",
      "    sample_time_ms: 577.411\n",
      "    update_time_ms: 5.423\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.96666666666666\n",
      "    gpu_util_percent0: 0.22666666666666668\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4505476344368676\n",
      "    mean_inference_ms: 0.9740866371251526\n",
      "    mean_processing_ms: 0.23937373465200754\n",
      "  time_since_restore: 799.8367903232574\n",
      "  time_this_iter_s: 2.0817723274230957\n",
      "  time_total_s: 799.8367903232574\n",
      "  timestamp: 1573083327\n",
      "  timesteps_since_restore: 1516000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1516000\n",
      "  training_iteration: 379\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 799 s, 379 iter, 1516000 ts, 562 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-35-33\n",
      "  done: false\n",
      "  episode_len_mean: 204.71\n",
      "  episode_reward_max: 610.5350777776933\n",
      "  episode_reward_mean: 560.1137964208486\n",
      "  episode_reward_min: 492.00900918381546\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 7936\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1458.851\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.967841386795044\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013873983174562454\n",
      "        policy_loss: -0.04652348905801773\n",
      "        total_loss: 3.1709604263305664\n",
      "        vf_explained_var: 0.9990746378898621\n",
      "        vf_loss: 3.2034361362457275\n",
      "    load_time_ms: 2.115\n",
      "    num_steps_sampled: 1528000\n",
      "    num_steps_trained: 1515776\n",
      "    sample_time_ms: 583.398\n",
      "    update_time_ms: 5.411\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.7\n",
      "    gpu_util_percent0: 0.23\n",
      "    ram_util_percent: 64.05\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45115228330909574\n",
      "    mean_inference_ms: 0.9763042237479422\n",
      "    mean_processing_ms: 0.239826985234275\n",
      "  time_since_restore: 805.9508142471313\n",
      "  time_this_iter_s: 1.9957754611968994\n",
      "  time_total_s: 805.9508142471313\n",
      "  timestamp: 1573083333\n",
      "  timesteps_since_restore: 1528000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1528000\n",
      "  training_iteration: 382\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 805 s, 382 iter, 1528000 ts, 560 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-35-39\n",
      "  done: false\n",
      "  episode_len_mean: 203.59\n",
      "  episode_reward_max: 600.3130427302361\n",
      "  episode_reward_mean: 557.907114562156\n",
      "  episode_reward_min: 492.00900918381546\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7993\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1465.854\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.945666790008545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011803687550127506\n",
      "        policy_loss: -0.0363437794148922\n",
      "        total_loss: 3.873501777648926\n",
      "        vf_explained_var: 0.9988816380500793\n",
      "        vf_loss: 3.8978943824768066\n",
      "    load_time_ms: 2.051\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1527680\n",
      "    sample_time_ms: 591.194\n",
      "    update_time_ms: 5.768\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.56666666666667\n",
      "    gpu_util_percent0: 0.056666666666666664\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45099963605545557\n",
      "    mean_inference_ms: 0.9764006501395971\n",
      "    mean_processing_ms: 0.23978868812189447\n",
      "  time_since_restore: 812.3700947761536\n",
      "  time_this_iter_s: 2.1483304500579834\n",
      "  time_total_s: 812.3700947761536\n",
      "  timestamp: 1573083339\n",
      "  timesteps_since_restore: 1540000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 385\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 812 s, 385 iter, 1540000 ts, 558 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-35-46\n",
      "  done: false\n",
      "  episode_len_mean: 203.14\n",
      "  episode_reward_max: 599.9332698721281\n",
      "  episode_reward_mean: 557.7802945464273\n",
      "  episode_reward_min: 491.94763993998396\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 8053\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1471.918\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8904101848602295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012631896883249283\n",
      "        policy_loss: -0.03940623253583908\n",
      "        total_loss: 5.208819389343262\n",
      "        vf_explained_var: 0.9984392523765564\n",
      "        vf_loss: 5.23543643951416\n",
      "    load_time_ms: 2.087\n",
      "    num_steps_sampled: 1552000\n",
      "    num_steps_trained: 1539584\n",
      "    sample_time_ms: 597.645\n",
      "    update_time_ms: 5.661\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.63333333333333\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.0\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4511222361891293\n",
      "    mean_inference_ms: 0.9764313839466484\n",
      "    mean_processing_ms: 0.23983045372082631\n",
      "  time_since_restore: 818.7211761474609\n",
      "  time_this_iter_s: 2.116588830947876\n",
      "  time_total_s: 818.7211761474609\n",
      "  timestamp: 1573083346\n",
      "  timesteps_since_restore: 1552000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1552000\n",
      "  training_iteration: 388\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 818 s, 388 iter, 1552000 ts, 558 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-35-52\n",
      "  done: false\n",
      "  episode_len_mean: 203.45\n",
      "  episode_reward_max: 598.3386269242744\n",
      "  episode_reward_mean: 558.9279785561884\n",
      "  episode_reward_min: 464.1500332897157\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 8111\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1477.159\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8603718280792236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013498424552381039\n",
      "        policy_loss: -0.0486893355846405\n",
      "        total_loss: 3.1334595680236816\n",
      "        vf_explained_var: 0.9990723729133606\n",
      "        vf_loss: 3.1684823036193848\n",
      "    load_time_ms: 2.126\n",
      "    num_steps_sampled: 1564000\n",
      "    num_steps_trained: 1551488\n",
      "    sample_time_ms: 598.006\n",
      "    update_time_ms: 5.723\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.55\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.0\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45088443059533634\n",
      "    mean_inference_ms: 0.9753752041828696\n",
      "    mean_processing_ms: 0.2397213892941134\n",
      "  time_since_restore: 824.9738206863403\n",
      "  time_this_iter_s: 2.10308837890625\n",
      "  time_total_s: 824.9738206863403\n",
      "  timestamp: 1573083352\n",
      "  timesteps_since_restore: 1564000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1564000\n",
      "  training_iteration: 391\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 824 s, 391 iter, 1564000 ts, 559 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-35-58\n",
      "  done: false\n",
      "  episode_len_mean: 204.35\n",
      "  episode_reward_max: 606.7271237849124\n",
      "  episode_reward_mean: 560.666499371993\n",
      "  episode_reward_min: 464.1500332897157\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 8169\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.823\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8814404010772705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012614996172487736\n",
      "        policy_loss: -0.047011908143758774\n",
      "        total_loss: 1.787825345993042\n",
      "        vf_explained_var: 0.9994895458221436\n",
      "        vf_loss: 1.822064757347107\n",
      "    load_time_ms: 2.339\n",
      "    num_steps_sampled: 1576000\n",
      "    num_steps_trained: 1563392\n",
      "    sample_time_ms: 594.193\n",
      "    update_time_ms: 5.829\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.56666666666667\n",
      "    gpu_util_percent0: 0.04666666666666667\n",
      "    ram_util_percent: 64.0\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4503864235659283\n",
      "    mean_inference_ms: 0.9731149785806514\n",
      "    mean_processing_ms: 0.23941441329816782\n",
      "  time_since_restore: 831.2678959369659\n",
      "  time_this_iter_s: 2.101752281188965\n",
      "  time_total_s: 831.2678959369659\n",
      "  timestamp: 1573083358\n",
      "  timesteps_since_restore: 1576000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1576000\n",
      "  training_iteration: 394\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 831 s, 394 iter, 1576000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-36-05\n",
      "  done: false\n",
      "  episode_len_mean: 204.93\n",
      "  episode_reward_max: 611.8420478656426\n",
      "  episode_reward_mean: 561.5372968086889\n",
      "  episode_reward_min: 516.5234425333398\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 8228\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1474.979\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.826897382736206\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014705597423017025\n",
      "        policy_loss: -0.04986261576414108\n",
      "        total_loss: 2.084681272506714\n",
      "        vf_explained_var: 0.9993770122528076\n",
      "        vf_loss: 2.119654417037964\n",
      "    load_time_ms: 2.187\n",
      "    num_steps_sampled: 1588000\n",
      "    num_steps_trained: 1575296\n",
      "    sample_time_ms: 593.185\n",
      "    update_time_ms: 6.042\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.0\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.06666666666666\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4502360518290414\n",
      "    mean_inference_ms: 0.9731244232291281\n",
      "    mean_processing_ms: 0.2394293623221095\n",
      "  time_since_restore: 837.5556724071503\n",
      "  time_this_iter_s: 2.109313488006592\n",
      "  time_total_s: 837.5556724071503\n",
      "  timestamp: 1573083365\n",
      "  timesteps_since_restore: 1588000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1588000\n",
      "  training_iteration: 397\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 837 s, 397 iter, 1588000 ts, 562 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-36-11\n",
      "  done: false\n",
      "  episode_len_mean: 205.3\n",
      "  episode_reward_max: 611.8420478656426\n",
      "  episode_reward_mean: 562.1819196419987\n",
      "  episode_reward_min: 500.4553257699181\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8287\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1477.025\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.871882438659668\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014080743305385113\n",
      "        policy_loss: -0.04582999646663666\n",
      "        total_loss: 3.6030964851379395\n",
      "        vf_explained_var: 0.9989634156227112\n",
      "        vf_loss: 3.634669542312622\n",
      "    load_time_ms: 2.217\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1587200\n",
      "    sample_time_ms: 593.16\n",
      "    update_time_ms: 5.821\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.7\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4503620846629255\n",
      "    mean_inference_ms: 0.9743458197661647\n",
      "    mean_processing_ms: 0.23953204644848508\n",
      "  time_since_restore: 843.8460502624512\n",
      "  time_this_iter_s: 2.1038243770599365\n",
      "  time_total_s: 843.8460502624512\n",
      "  timestamp: 1573083371\n",
      "  timesteps_since_restore: 1600000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 400\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 843 s, 400 iter, 1600000 ts, 562 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-36-17\n",
      "  done: false\n",
      "  episode_len_mean: 204.68\n",
      "  episode_reward_max: 591.8817166208061\n",
      "  episode_reward_mean: 562.1217579406465\n",
      "  episode_reward_min: 525.0644033294598\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 8344\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1471.992\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.830199718475342\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013333035632967949\n",
      "        policy_loss: -0.04880130663514137\n",
      "        total_loss: 2.512855052947998\n",
      "        vf_explained_var: 0.9992528557777405\n",
      "        vf_loss: 2.54815673828125\n",
      "    load_time_ms: 2.258\n",
      "    num_steps_sampled: 1612000\n",
      "    num_steps_trained: 1599104\n",
      "    sample_time_ms: 593.14\n",
      "    update_time_ms: 5.651\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.4\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45059247200528796\n",
      "    mean_inference_ms: 0.9758813484190495\n",
      "    mean_processing_ms: 0.239807144862242\n",
      "  time_since_restore: 850.0887715816498\n",
      "  time_this_iter_s: 2.070702075958252\n",
      "  time_total_s: 850.0887715816498\n",
      "  timestamp: 1573083377\n",
      "  timesteps_since_restore: 1612000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1612000\n",
      "  training_iteration: 403\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 850 s, 403 iter, 1612000 ts, 562 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-36-23\n",
      "  done: false\n",
      "  episode_len_mean: 205.02\n",
      "  episode_reward_max: 602.8944231155626\n",
      "  episode_reward_mean: 563.0525872557319\n",
      "  episode_reward_min: 516.3119967329449\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8403\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1463.889\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8064403533935547\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013315500691533089\n",
      "        policy_loss: -0.04819105565547943\n",
      "        total_loss: 2.4430923461914062\n",
      "        vf_explained_var: 0.999274492263794\n",
      "        vf_loss: 2.4778013229370117\n",
      "    load_time_ms: 2.133\n",
      "    num_steps_sampled: 1624000\n",
      "    num_steps_trained: 1611008\n",
      "    sample_time_ms: 589.802\n",
      "    update_time_ms: 5.606\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.5\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.45048745838548016\n",
      "    mean_inference_ms: 0.9754250466880658\n",
      "    mean_processing_ms: 0.2396907195053905\n",
      "  time_since_restore: 856.2564990520477\n",
      "  time_this_iter_s: 2.0362985134124756\n",
      "  time_total_s: 856.2564990520477\n",
      "  timestamp: 1573083383\n",
      "  timesteps_since_restore: 1624000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1624000\n",
      "  training_iteration: 406\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 856 s, 406 iter, 1624000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-36-30\n",
      "  done: false\n",
      "  episode_len_mean: 203.78\n",
      "  episode_reward_max: 624.7231642905465\n",
      "  episode_reward_mean: 560.89375132866\n",
      "  episode_reward_min: 511.215463749375\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8463\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1453.366\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.792936086654663\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014757692813873291\n",
      "        policy_loss: -0.05620608106255531\n",
      "        total_loss: 2.335732936859131\n",
      "        vf_explained_var: 0.999315619468689\n",
      "        vf_loss: 2.3769967555999756\n",
      "    load_time_ms: 2.374\n",
      "    num_steps_sampled: 1636000\n",
      "    num_steps_trained: 1622912\n",
      "    sample_time_ms: 587.174\n",
      "    update_time_ms: 5.473\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.13333333333333\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4501031777159894\n",
      "    mean_inference_ms: 0.9747939674110258\n",
      "    mean_processing_ms: 0.23953725009295027\n",
      "  time_since_restore: 862.4292438030243\n",
      "  time_this_iter_s: 2.0320920944213867\n",
      "  time_total_s: 862.4292438030243\n",
      "  timestamp: 1573083390\n",
      "  timesteps_since_restore: 1636000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1636000\n",
      "  training_iteration: 409\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 862 s, 409 iter, 1636000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-36-36\n",
      "  done: false\n",
      "  episode_len_mean: 201.71\n",
      "  episode_reward_max: 624.7231642905465\n",
      "  episode_reward_mean: 556.6383945824075\n",
      "  episode_reward_min: 511.215463749375\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8523\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1455.337\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.81978440284729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013368577696383\n",
      "        policy_loss: -0.0452248677611351\n",
      "        total_loss: 2.374242067337036\n",
      "        vf_explained_var: 0.9992996454238892\n",
      "        vf_loss: 2.4059314727783203\n",
      "    load_time_ms: 2.193\n",
      "    num_steps_sampled: 1648000\n",
      "    num_steps_trained: 1634816\n",
      "    sample_time_ms: 587.635\n",
      "    update_time_ms: 5.657\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.9\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.36666666666666\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44996560252537565\n",
      "    mean_inference_ms: 0.9734569557805337\n",
      "    mean_processing_ms: 0.2394644619004087\n",
      "  time_since_restore: 868.7251844406128\n",
      "  time_this_iter_s: 2.162726402282715\n",
      "  time_total_s: 868.7251844406128\n",
      "  timestamp: 1573083396\n",
      "  timesteps_since_restore: 1648000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1648000\n",
      "  training_iteration: 412\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 868 s, 412 iter, 1648000 ts, 557 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-36-42\n",
      "  done: false\n",
      "  episode_len_mean: 202.93\n",
      "  episode_reward_max: 614.3858087417225\n",
      "  episode_reward_mean: 559.0465824187766\n",
      "  episode_reward_min: 516.5663493379963\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8583\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1451.095\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.769528865814209\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014286252669990063\n",
      "        policy_loss: -0.05117599666118622\n",
      "        total_loss: 2.8226640224456787\n",
      "        vf_explained_var: 0.9991728067398071\n",
      "        vf_loss: 2.859375476837158\n",
      "    load_time_ms: 2.938\n",
      "    num_steps_sampled: 1660000\n",
      "    num_steps_trained: 1646720\n",
      "    sample_time_ms: 588.544\n",
      "    update_time_ms: 5.69\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.8\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.449957189189446\n",
      "    mean_inference_ms: 0.9731165798437691\n",
      "    mean_processing_ms: 0.2394484463921643\n",
      "  time_since_restore: 874.9108040332794\n",
      "  time_this_iter_s: 2.0405006408691406\n",
      "  time_total_s: 874.9108040332794\n",
      "  timestamp: 1573083402\n",
      "  timesteps_since_restore: 1660000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1660000\n",
      "  training_iteration: 415\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 874 s, 415 iter, 1660000 ts, 559 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-36-48\n",
      "  done: false\n",
      "  episode_len_mean: 201.87\n",
      "  episode_reward_max: 599.2349286952273\n",
      "  episode_reward_mean: 556.4137278381046\n",
      "  episode_reward_min: 424.54856514124094\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 8641\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1456.431\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7292821407318115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014251240529119968\n",
      "        policy_loss: -0.043081134557724\n",
      "        total_loss: 1.7842997312545776\n",
      "        vf_explained_var: 0.999480664730072\n",
      "        vf_loss: 1.8129515647888184\n",
      "    load_time_ms: 2.819\n",
      "    num_steps_sampled: 1672000\n",
      "    num_steps_trained: 1658624\n",
      "    sample_time_ms: 588.6\n",
      "    update_time_ms: 6.139\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.13333333333333\n",
      "    gpu_util_percent0: 0.07666666666666666\n",
      "    ram_util_percent: 64.36666666666667\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4497250149397666\n",
      "    mean_inference_ms: 0.972565133441828\n",
      "    mean_processing_ms: 0.23932933882643728\n",
      "  time_since_restore: 881.136034488678\n",
      "  time_this_iter_s: 2.0868618488311768\n",
      "  time_total_s: 881.136034488678\n",
      "  timestamp: 1573083408\n",
      "  timesteps_since_restore: 1672000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1672000\n",
      "  training_iteration: 418\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 881 s, 418 iter, 1672000 ts, 556 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-36-55\n",
      "  done: false\n",
      "  episode_len_mean: 203.13\n",
      "  episode_reward_max: 609.0613890424692\n",
      "  episode_reward_mean: 560.1225159552083\n",
      "  episode_reward_min: 503.7074599364526\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8701\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1460.577\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7764503955841064\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012028155848383904\n",
      "        policy_loss: -0.0400659404695034\n",
      "        total_loss: 9.163468360900879\n",
      "        vf_explained_var: 0.9973663687705994\n",
      "        vf_loss: 9.191354751586914\n",
      "    load_time_ms: 2.712\n",
      "    num_steps_sampled: 1684000\n",
      "    num_steps_trained: 1670528\n",
      "    sample_time_ms: 593.141\n",
      "    update_time_ms: 5.981\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.13333333333333\n",
      "    gpu_util_percent0: 0.2333333333333333\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44961962058974536\n",
      "    mean_inference_ms: 0.9731635929875486\n",
      "    mean_processing_ms: 0.23940594977171425\n",
      "  time_since_restore: 887.381432056427\n",
      "  time_this_iter_s: 2.049485921859741\n",
      "  time_total_s: 887.381432056427\n",
      "  timestamp: 1573083415\n",
      "  timesteps_since_restore: 1684000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1684000\n",
      "  training_iteration: 421\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 887 s, 421 iter, 1684000 ts, 560 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-37-01\n",
      "  done: false\n",
      "  episode_len_mean: 204.43\n",
      "  episode_reward_max: 609.8310811947123\n",
      "  episode_reward_mean: 562.4699948095844\n",
      "  episode_reward_min: 506.7582393362247\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 8758\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1451.019\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7178192138671875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01471477746963501\n",
      "        policy_loss: -0.05134458839893341\n",
      "        total_loss: 2.6902832984924316\n",
      "        vf_explained_var: 0.9991976618766785\n",
      "        vf_loss: 2.726729154586792\n",
      "    load_time_ms: 2.495\n",
      "    num_steps_sampled: 1696000\n",
      "    num_steps_trained: 1682432\n",
      "    sample_time_ms: 590.591\n",
      "    update_time_ms: 5.881\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.46666666666666\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44922747189722423\n",
      "    mean_inference_ms: 0.9718606815932246\n",
      "    mean_processing_ms: 0.23920427522707818\n",
      "  time_since_restore: 893.55752825737\n",
      "  time_this_iter_s: 2.0728397369384766\n",
      "  time_total_s: 893.55752825737\n",
      "  timestamp: 1573083421\n",
      "  timesteps_since_restore: 1696000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1696000\n",
      "  training_iteration: 424\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 893 s, 424 iter, 1696000 ts, 562 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-37-07\n",
      "  done: false\n",
      "  episode_len_mean: 205.74\n",
      "  episode_reward_max: 609.8310811947123\n",
      "  episode_reward_mean: 565.0334837620715\n",
      "  episode_reward_min: 496.1334849395003\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8817\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1461.035\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7658183574676514\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014065402559936047\n",
      "        policy_loss: -0.04755311831831932\n",
      "        total_loss: 2.305266857147217\n",
      "        vf_explained_var: 0.9993354678153992\n",
      "        vf_loss: 2.338578462600708\n",
      "    load_time_ms: 2.076\n",
      "    num_steps_sampled: 1708000\n",
      "    num_steps_trained: 1694336\n",
      "    sample_time_ms: 594.36\n",
      "    update_time_ms: 5.448\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.333333333333336\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44925489599125457\n",
      "    mean_inference_ms: 0.9713595250773696\n",
      "    mean_processing_ms: 0.23916470825282152\n",
      "  time_since_restore: 899.8575143814087\n",
      "  time_this_iter_s: 2.1008076667785645\n",
      "  time_total_s: 899.8575143814087\n",
      "  timestamp: 1573083427\n",
      "  timesteps_since_restore: 1708000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1708000\n",
      "  training_iteration: 427\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 899 s, 427 iter, 1708000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-37-13\n",
      "  done: false\n",
      "  episode_len_mean: 205.56\n",
      "  episode_reward_max: 602.8399568841029\n",
      "  episode_reward_mean: 565.5018343535054\n",
      "  episode_reward_min: 496.1334849395003\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 8876\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1451.477\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.730736255645752\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013942589983344078\n",
      "        policy_loss: -0.04771982133388519\n",
      "        total_loss: 1.9640730619430542\n",
      "        vf_explained_var: 0.9994344115257263\n",
      "        vf_loss: 1.997676134109497\n",
      "    load_time_ms: 2.197\n",
      "    num_steps_sampled: 1720000\n",
      "    num_steps_trained: 1706240\n",
      "    sample_time_ms: 595.41\n",
      "    update_time_ms: 5.418\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.333333333333336\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4492872330407954\n",
      "    mean_inference_ms: 0.9710895427800044\n",
      "    mean_processing_ms: 0.2391279805576092\n",
      "  time_since_restore: 906.0684008598328\n",
      "  time_this_iter_s: 2.088961124420166\n",
      "  time_total_s: 906.0684008598328\n",
      "  timestamp: 1573083433\n",
      "  timesteps_since_restore: 1720000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1720000\n",
      "  training_iteration: 430\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 906 s, 430 iter, 1720000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-37-20\n",
      "  done: false\n",
      "  episode_len_mean: 204.36\n",
      "  episode_reward_max: 607.5175621036728\n",
      "  episode_reward_mean: 563.1358812466557\n",
      "  episode_reward_min: 512.6415560033583\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 8935\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1471.248\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7033636569976807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014510218985378742\n",
      "        policy_loss: -0.045956045389175415\n",
      "        total_loss: 1.601146936416626\n",
      "        vf_explained_var: 0.9995359778404236\n",
      "        vf_loss: 1.6324115991592407\n",
      "    load_time_ms: 2.453\n",
      "    num_steps_sampled: 1732000\n",
      "    num_steps_trained: 1718144\n",
      "    sample_time_ms: 591.819\n",
      "    update_time_ms: 5.375\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.56666666666667\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44987674165597796\n",
      "    mean_inference_ms: 0.972857897383643\n",
      "    mean_processing_ms: 0.23950764602045704\n",
      "  time_since_restore: 912.3820536136627\n",
      "  time_this_iter_s: 2.0985116958618164\n",
      "  time_total_s: 912.3820536136627\n",
      "  timestamp: 1573083440\n",
      "  timesteps_since_restore: 1732000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1732000\n",
      "  training_iteration: 433\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 912 s, 433 iter, 1732000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-37-26\n",
      "  done: false\n",
      "  episode_len_mean: 203.95\n",
      "  episode_reward_max: 607.5175621036728\n",
      "  episode_reward_mean: 563.2484060472818\n",
      "  episode_reward_min: 521.9336500013025\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8994\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1464.023\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.719980239868164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012557705864310265\n",
      "        policy_loss: -0.05228940397500992\n",
      "        total_loss: 2.46744966506958\n",
      "        vf_explained_var: 0.9992793202400208\n",
      "        vf_loss: 2.5070245265960693\n",
      "    load_time_ms: 2.294\n",
      "    num_steps_sampled: 1744000\n",
      "    num_steps_trained: 1730048\n",
      "    sample_time_ms: 588.835\n",
      "    update_time_ms: 5.4\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.0\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4494920260634549\n",
      "    mean_inference_ms: 0.9723412229602331\n",
      "    mean_processing_ms: 0.2393701167156679\n",
      "  time_since_restore: 918.5583975315094\n",
      "  time_this_iter_s: 2.0511765480041504\n",
      "  time_total_s: 918.5583975315094\n",
      "  timestamp: 1573083446\n",
      "  timesteps_since_restore: 1744000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1744000\n",
      "  training_iteration: 436\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 918 s, 436 iter, 1744000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-37-32\n",
      "  done: false\n",
      "  episode_len_mean: 204.78\n",
      "  episode_reward_max: 600.0468568489317\n",
      "  episode_reward_mean: 564.5939587202333\n",
      "  episode_reward_min: 517.759247723298\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 9053\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1480.052\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.665646553039551\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014732688665390015\n",
      "        policy_loss: -0.046144116669893265\n",
      "        total_loss: 3.363089084625244\n",
      "        vf_explained_var: 0.9990183711051941\n",
      "        vf_loss: 3.394317150115967\n",
      "    load_time_ms: 2.419\n",
      "    num_steps_sampled: 1756000\n",
      "    num_steps_trained: 1741952\n",
      "    sample_time_ms: 582.593\n",
      "    update_time_ms: 5.461\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.93333333333334\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44886182771851435\n",
      "    mean_inference_ms: 0.9708517947170564\n",
      "    mean_processing_ms: 0.23893977415793718\n",
      "  time_since_restore: 924.8993101119995\n",
      "  time_this_iter_s: 2.1262664794921875\n",
      "  time_total_s: 924.8993101119995\n",
      "  timestamp: 1573083452\n",
      "  timesteps_since_restore: 1756000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1756000\n",
      "  training_iteration: 439\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 924 s, 439 iter, 1756000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-37-39\n",
      "  done: false\n",
      "  episode_len_mean: 205.62\n",
      "  episode_reward_max: 610.5651970714206\n",
      "  episode_reward_mean: 565.8697783706465\n",
      "  episode_reward_min: 517.759247723298\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9110\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.063\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.683863639831543\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014954643324017525\n",
      "        policy_loss: -0.049028050154447556\n",
      "        total_loss: 2.1880857944488525\n",
      "        vf_explained_var: 0.9993617534637451\n",
      "        vf_loss: 2.2219724655151367\n",
      "    load_time_ms: 2.344\n",
      "    num_steps_sampled: 1768000\n",
      "    num_steps_trained: 1753856\n",
      "    sample_time_ms: 581.639\n",
      "    update_time_ms: 5.634\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.06666666666666\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4491328835776463\n",
      "    mean_inference_ms: 0.9707943933830703\n",
      "    mean_processing_ms: 0.23916872151305837\n",
      "  time_since_restore: 931.2015025615692\n",
      "  time_this_iter_s: 2.0686209201812744\n",
      "  time_total_s: 931.2015025615692\n",
      "  timestamp: 1573083459\n",
      "  timesteps_since_restore: 1768000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1768000\n",
      "  training_iteration: 442\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 931 s, 442 iter, 1768000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-37-45\n",
      "  done: false\n",
      "  episode_len_mean: 204.38\n",
      "  episode_reward_max: 611.4004270617318\n",
      "  episode_reward_mean: 563.9300096458926\n",
      "  episode_reward_min: 498.29296658105267\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 9171\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1488.998\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6786110401153564\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013813001103699207\n",
      "        policy_loss: -0.051467183977365494\n",
      "        total_loss: 4.125757217407227\n",
      "        vf_explained_var: 0.9988126158714294\n",
      "        vf_loss: 4.163238525390625\n",
      "    load_time_ms: 2.432\n",
      "    num_steps_sampled: 1780000\n",
      "    num_steps_trained: 1765760\n",
      "    sample_time_ms: 586.724\n",
      "    update_time_ms: 5.711\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.5\n",
      "    gpu_util_percent0: 0.12666666666666668\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44945589708000566\n",
      "    mean_inference_ms: 0.9716657773900534\n",
      "    mean_processing_ms: 0.23936651811735454\n",
      "  time_since_restore: 937.5430219173431\n",
      "  time_this_iter_s: 2.1324315071105957\n",
      "  time_total_s: 937.5430219173431\n",
      "  timestamp: 1573083465\n",
      "  timesteps_since_restore: 1780000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1780000\n",
      "  training_iteration: 445\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 937 s, 445 iter, 1780000 ts, 564 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-37-52\n",
      "  done: false\n",
      "  episode_len_mean: 204.56\n",
      "  episode_reward_max: 611.4004270617318\n",
      "  episode_reward_mean: 564.530131596541\n",
      "  episode_reward_min: 498.29296658105267\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 9226\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1495.946\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6441614627838135\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013053219765424728\n",
      "        policy_loss: -0.048336245119571686\n",
      "        total_loss: 2.3932032585144043\n",
      "        vf_explained_var: 0.9993078112602234\n",
      "        vf_loss: 2.428323268890381\n",
      "    load_time_ms: 2.404\n",
      "    num_steps_sampled: 1792000\n",
      "    num_steps_trained: 1777664\n",
      "    sample_time_ms: 589.234\n",
      "    update_time_ms: 6.171\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.86666666666667\n",
      "    gpu_util_percent0: 0.26999999999999996\n",
      "    ram_util_percent: 64.6\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4491571883016722\n",
      "    mean_inference_ms: 0.971694992087778\n",
      "    mean_processing_ms: 0.23923682993288167\n",
      "  time_since_restore: 943.9037711620331\n",
      "  time_this_iter_s: 2.115602970123291\n",
      "  time_total_s: 943.9037711620331\n",
      "  timestamp: 1573083472\n",
      "  timesteps_since_restore: 1792000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1792000\n",
      "  training_iteration: 448\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 943 s, 448 iter, 1792000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-37-58\n",
      "  done: false\n",
      "  episode_len_mean: 203.37\n",
      "  episode_reward_max: 609.8596027809006\n",
      "  episode_reward_mean: 563.1007112476236\n",
      "  episode_reward_min: 519.002026845358\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9286\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1499.388\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6474695205688477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013782811351120472\n",
      "        policy_loss: -0.0515056811273098\n",
      "        total_loss: 3.868880033493042\n",
      "        vf_explained_var: 0.9988887906074524\n",
      "        vf_loss: 3.90643048286438\n",
      "    load_time_ms: 2.485\n",
      "    num_steps_sampled: 1804000\n",
      "    num_steps_trained: 1789568\n",
      "    sample_time_ms: 590.558\n",
      "    update_time_ms: 6.273\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.300000000000004\n",
      "    gpu_util_percent0: 0.2333333333333333\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4487512248097905\n",
      "    mean_inference_ms: 0.9712238652403085\n",
      "    mean_processing_ms: 0.2390346804812814\n",
      "  time_since_restore: 950.3020925521851\n",
      "  time_this_iter_s: 2.1464438438415527\n",
      "  time_total_s: 950.3020925521851\n",
      "  timestamp: 1573083478\n",
      "  timesteps_since_restore: 1804000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1804000\n",
      "  training_iteration: 451\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 950 s, 451 iter, 1804000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-38-04\n",
      "  done: false\n",
      "  episode_len_mean: 202.64\n",
      "  episode_reward_max: 606.484165973706\n",
      "  episode_reward_mean: 561.4006667994272\n",
      "  episode_reward_min: 518.83457279696\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 9344\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1510.744\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.604841709136963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01289071049541235\n",
      "        policy_loss: -0.04752982780337334\n",
      "        total_loss: 3.582482099533081\n",
      "        vf_explained_var: 0.9989102482795715\n",
      "        vf_loss: 3.616960048675537\n",
      "    load_time_ms: 2.027\n",
      "    num_steps_sampled: 1816000\n",
      "    num_steps_trained: 1801472\n",
      "    sample_time_ms: 589.868\n",
      "    update_time_ms: 6.081\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.166666666666664\n",
      "    gpu_util_percent0: 0.2333333333333333\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4486014949059771\n",
      "    mean_inference_ms: 0.9696608165947056\n",
      "    mean_processing_ms: 0.23871263847257676\n",
      "  time_since_restore: 956.7048482894897\n",
      "  time_this_iter_s: 2.15698504447937\n",
      "  time_total_s: 956.7048482894897\n",
      "  timestamp: 1573083484\n",
      "  timesteps_since_restore: 1816000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1816000\n",
      "  training_iteration: 454\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 956 s, 454 iter, 1816000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-38-11\n",
      "  done: false\n",
      "  episode_len_mean: 203.29\n",
      "  episode_reward_max: 604.192161156978\n",
      "  episode_reward_mean: 563.2367849642466\n",
      "  episode_reward_min: 518.83457279696\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9405\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1510.458\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.636589765548706\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01392523292452097\n",
      "        policy_loss: -0.05326707661151886\n",
      "        total_loss: 2.4332940578460693\n",
      "        vf_explained_var: 0.9993030428886414\n",
      "        vf_loss: 2.4724619388580322\n",
      "    load_time_ms: 2.067\n",
      "    num_steps_sampled: 1828000\n",
      "    num_steps_trained: 1813376\n",
      "    sample_time_ms: 594.223\n",
      "    update_time_ms: 6.341\n",
      "  iterations_since_restore: 457\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.53333333333333\n",
      "    gpu_util_percent0: 0.22333333333333336\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4489456289462984\n",
      "    mean_inference_ms: 0.9698299705684107\n",
      "    mean_processing_ms: 0.23885032077297616\n",
      "  time_since_restore: 963.1269197463989\n",
      "  time_this_iter_s: 2.1355531215667725\n",
      "  time_total_s: 963.1269197463989\n",
      "  timestamp: 1573083491\n",
      "  timesteps_since_restore: 1828000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1828000\n",
      "  training_iteration: 457\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 963 s, 457 iter, 1828000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-38-17\n",
      "  done: false\n",
      "  episode_len_mean: 203.17\n",
      "  episode_reward_max: 617.9292624074842\n",
      "  episode_reward_mean: 562.9924195174402\n",
      "  episode_reward_min: 488.63634923051603\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9463\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1510.164\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6146087646484375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015013269148766994\n",
      "        policy_loss: -0.048117175698280334\n",
      "        total_loss: 3.005281925201416\n",
      "        vf_explained_var: 0.999122142791748\n",
      "        vf_loss: 3.038198232650757\n",
      "    load_time_ms: 1.973\n",
      "    num_steps_sampled: 1840000\n",
      "    num_steps_trained: 1825280\n",
      "    sample_time_ms: 590.087\n",
      "    update_time_ms: 5.811\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.56666666666667\n",
      "    gpu_util_percent0: 0.22666666666666668\n",
      "    ram_util_percent: 64.56666666666666\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4494158632139436\n",
      "    mean_inference_ms: 0.9726185196292044\n",
      "    mean_processing_ms: 0.23931771989139783\n",
      "  time_since_restore: 969.4476685523987\n",
      "  time_this_iter_s: 2.1007587909698486\n",
      "  time_total_s: 969.4476685523987\n",
      "  timestamp: 1573083497\n",
      "  timesteps_since_restore: 1840000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1840000\n",
      "  training_iteration: 460\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 969 s, 460 iter, 1840000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-38-24\n",
      "  done: false\n",
      "  episode_len_mean: 201.99\n",
      "  episode_reward_max: 617.9292624074842\n",
      "  episode_reward_mean: 560.4103539351435\n",
      "  episode_reward_min: 476.2562726436792\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 9522\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1506.213\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6314001083374023\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012466099113225937\n",
      "        policy_loss: -0.04479086026549339\n",
      "        total_loss: 4.857539176940918\n",
      "        vf_explained_var: 0.9986485838890076\n",
      "        vf_loss: 4.889708518981934\n",
      "    load_time_ms: 2.197\n",
      "    num_steps_sampled: 1852000\n",
      "    num_steps_trained: 1837184\n",
      "    sample_time_ms: 589.148\n",
      "    update_time_ms: 5.874\n",
      "  iterations_since_restore: 463\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.43333333333334\n",
      "    gpu_util_percent0: 0.22666666666666668\n",
      "    ram_util_percent: 64.6\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4491791000588124\n",
      "    mean_inference_ms: 0.9718789367655052\n",
      "    mean_processing_ms: 0.2392735061699634\n",
      "  time_since_restore: 975.7774660587311\n",
      "  time_this_iter_s: 2.1404435634613037\n",
      "  time_total_s: 975.7774660587311\n",
      "  timestamp: 1573083504\n",
      "  timesteps_since_restore: 1852000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1852000\n",
      "  training_iteration: 463\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 975 s, 463 iter, 1852000 ts, 560 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-38-30\n",
      "  done: false\n",
      "  episode_len_mean: 202.81\n",
      "  episode_reward_max: 607.5290127585545\n",
      "  episode_reward_mean: 562.4266805001671\n",
      "  episode_reward_min: 476.2562726436792\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9582\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1503.315\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.603834390640259\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014064901508390903\n",
      "        policy_loss: -0.050630439072847366\n",
      "        total_loss: 1.9874109029769897\n",
      "        vf_explained_var: 0.9994281530380249\n",
      "        vf_loss: 2.0238006114959717\n",
      "    load_time_ms: 2.251\n",
      "    num_steps_sampled: 1864000\n",
      "    num_steps_trained: 1849088\n",
      "    sample_time_ms: 581.68\n",
      "    update_time_ms: 6.089\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.7\n",
      "    gpu_util_percent0: 0.225\n",
      "    ram_util_percent: 64.6\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44937792951361\n",
      "    mean_inference_ms: 0.9719978155265296\n",
      "    mean_processing_ms: 0.23935335887272874\n",
      "  time_since_restore: 982.1072309017181\n",
      "  time_this_iter_s: 2.101747989654541\n",
      "  time_total_s: 982.1072309017181\n",
      "  timestamp: 1573083510\n",
      "  timesteps_since_restore: 1864000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1864000\n",
      "  training_iteration: 466\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 982 s, 466 iter, 1864000 ts, 562 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-38-36\n",
      "  done: false\n",
      "  episode_len_mean: 202.72\n",
      "  episode_reward_max: 596.0798771536845\n",
      "  episode_reward_mean: 562.588237304212\n",
      "  episode_reward_min: 503.5546389659706\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 9641\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1505.404\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5643692016601562\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01499369740486145\n",
      "        policy_loss: -0.04906422644853592\n",
      "        total_loss: 2.4923036098480225\n",
      "        vf_explained_var: 0.9992600083351135\n",
      "        vf_loss: 2.526186227798462\n",
      "    load_time_ms: 2.097\n",
      "    num_steps_sampled: 1876000\n",
      "    num_steps_trained: 1860992\n",
      "    sample_time_ms: 588.477\n",
      "    update_time_ms: 6.388\n",
      "  iterations_since_restore: 469\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.833333333333336\n",
      "    gpu_util_percent0: 0.23666666666666666\n",
      "    ram_util_percent: 64.66666666666667\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44971816967665895\n",
      "    mean_inference_ms: 0.9739868447576722\n",
      "    mean_processing_ms: 0.23958081955245553\n",
      "  time_since_restore: 988.5551555156708\n",
      "  time_this_iter_s: 2.157416343688965\n",
      "  time_total_s: 988.5551555156708\n",
      "  timestamp: 1573083516\n",
      "  timesteps_since_restore: 1876000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1876000\n",
      "  training_iteration: 469\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 988 s, 469 iter, 1876000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-38-43\n",
      "  done: false\n",
      "  episode_len_mean: 204.02\n",
      "  episode_reward_max: 616.2131874896426\n",
      "  episode_reward_mean: 565.5321907359896\n",
      "  episode_reward_min: 517.7970518607867\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 9698\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1507.194\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.574477195739746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014437778852880001\n",
      "        policy_loss: -0.05010131001472473\n",
      "        total_loss: 2.4411637783050537\n",
      "        vf_explained_var: 0.9992983341217041\n",
      "        vf_loss: 2.476647138595581\n",
      "    load_time_ms: 2.007\n",
      "    num_steps_sampled: 1888000\n",
      "    num_steps_trained: 1872896\n",
      "    sample_time_ms: 588.048\n",
      "    update_time_ms: 6.604\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.400000000000006\n",
      "    gpu_util_percent0: 0.36000000000000004\n",
      "    ram_util_percent: 64.7\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44907762232159115\n",
      "    mean_inference_ms: 0.9720670574467519\n",
      "    mean_processing_ms: 0.23916339703256553\n",
      "  time_since_restore: 994.8688321113586\n",
      "  time_this_iter_s: 2.0802416801452637\n",
      "  time_total_s: 994.8688321113586\n",
      "  timestamp: 1573083523\n",
      "  timesteps_since_restore: 1888000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1888000\n",
      "  training_iteration: 472\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 994 s, 472 iter, 1888000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-38-49\n",
      "  done: false\n",
      "  episode_len_mean: 203.0\n",
      "  episode_reward_max: 595.9109878114378\n",
      "  episode_reward_mean: 563.0644526405968\n",
      "  episode_reward_min: 505.8808122693105\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9758\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1498.745\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5370874404907227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014430753886699677\n",
      "        policy_loss: -0.053554341197013855\n",
      "        total_loss: 3.516613483428955\n",
      "        vf_explained_var: 0.9990023970603943\n",
      "        vf_loss: 3.5555567741394043\n",
      "    load_time_ms: 1.789\n",
      "    num_steps_sampled: 1900000\n",
      "    num_steps_trained: 1884800\n",
      "    sample_time_ms: 590.423\n",
      "    update_time_ms: 6.778\n",
      "  iterations_since_restore: 475\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.46666666666667\n",
      "    gpu_util_percent0: 0.3666666666666667\n",
      "    ram_util_percent: 64.7\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44873570524762507\n",
      "    mean_inference_ms: 0.9705058189458025\n",
      "    mean_processing_ms: 0.23898069798796262\n",
      "  time_since_restore: 1001.175990819931\n",
      "  time_this_iter_s: 2.1157140731811523\n",
      "  time_total_s: 1001.175990819931\n",
      "  timestamp: 1573083529\n",
      "  timesteps_since_restore: 1900000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1900000\n",
      "  training_iteration: 475\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1001 s, 475 iter, 1900000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-38-55\n",
      "  done: false\n",
      "  episode_len_mean: 203.36\n",
      "  episode_reward_max: 607.4441476975911\n",
      "  episode_reward_mean: 563.7608954046551\n",
      "  episode_reward_min: 505.8808122693105\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9818\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1489.205\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.583120346069336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01335621066391468\n",
      "        policy_loss: -0.04582705348730087\n",
      "        total_loss: 3.8223824501037598\n",
      "        vf_explained_var: 0.9988962411880493\n",
      "        vf_loss: 3.8546864986419678\n",
      "    load_time_ms: 2.002\n",
      "    num_steps_sampled: 1912000\n",
      "    num_steps_trained: 1896704\n",
      "    sample_time_ms: 592.249\n",
      "    update_time_ms: 6.257\n",
      "  iterations_since_restore: 478\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.400000000000006\n",
      "    gpu_util_percent0: 0.365\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4492239251380038\n",
      "    mean_inference_ms: 0.9706262053143441\n",
      "    mean_processing_ms: 0.2392300927544366\n",
      "  time_since_restore: 1007.4857749938965\n",
      "  time_this_iter_s: 2.0845766067504883\n",
      "  time_total_s: 1007.4857749938965\n",
      "  timestamp: 1573083535\n",
      "  timesteps_since_restore: 1912000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1912000\n",
      "  training_iteration: 478\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1007 s, 478 iter, 1912000 ts, 564 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-39-02\n",
      "  done: false\n",
      "  episode_len_mean: 203.06\n",
      "  episode_reward_max: 607.4441476975911\n",
      "  episode_reward_mean: 564.1377944844614\n",
      "  episode_reward_min: 507.6701428678178\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9878\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1488.677\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.565155267715454\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018004752695560455\n",
      "        policy_loss: -0.049366045743227005\n",
      "        total_loss: 2.8110461235046387\n",
      "        vf_explained_var: 0.999212384223938\n",
      "        vf_loss: 2.842181921005249\n",
      "    load_time_ms: 2.215\n",
      "    num_steps_sampled: 1924000\n",
      "    num_steps_trained: 1908608\n",
      "    sample_time_ms: 589.374\n",
      "    update_time_ms: 6.056\n",
      "  iterations_since_restore: 481\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.76666666666667\n",
      "    gpu_util_percent0: 0.3666666666666667\n",
      "    ram_util_percent: 64.2\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44924718297807914\n",
      "    mean_inference_ms: 0.9711419823586014\n",
      "    mean_processing_ms: 0.23921980882517577\n",
      "  time_since_restore: 1013.8404717445374\n",
      "  time_this_iter_s: 2.13126540184021\n",
      "  time_total_s: 1013.8404717445374\n",
      "  timestamp: 1573083542\n",
      "  timesteps_since_restore: 1924000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1924000\n",
      "  training_iteration: 481\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.0/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1013 s, 481 iter, 1924000 ts, 564 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-39-08\n",
      "  done: false\n",
      "  episode_len_mean: 201.56\n",
      "  episode_reward_max: 606.701764504735\n",
      "  episode_reward_mean: 560.3275694365524\n",
      "  episode_reward_min: 396.67947271917393\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9938\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1499.984\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.585176706314087\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013677043840289116\n",
      "        policy_loss: -0.04641101509332657\n",
      "        total_loss: 7.3519392013549805\n",
      "        vf_explained_var: 0.9978580474853516\n",
      "        vf_loss: 7.3845014572143555\n",
      "    load_time_ms: 2.274\n",
      "    num_steps_sampled: 1936000\n",
      "    num_steps_trained: 1920512\n",
      "    sample_time_ms: 592.519\n",
      "    update_time_ms: 5.899\n",
      "  iterations_since_restore: 484\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.233333333333334\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44897264324616737\n",
      "    mean_inference_ms: 0.9692679609424053\n",
      "    mean_processing_ms: 0.23894402333559878\n",
      "  time_since_restore: 1020.2614591121674\n",
      "  time_this_iter_s: 2.134395122528076\n",
      "  time_total_s: 1020.2614591121674\n",
      "  timestamp: 1573083548\n",
      "  timesteps_since_restore: 1936000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1936000\n",
      "  training_iteration: 484\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1020 s, 484 iter, 1936000 ts, 560 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-39-15\n",
      "  done: false\n",
      "  episode_len_mean: 200.61\n",
      "  episode_reward_max: 606.701764504735\n",
      "  episode_reward_mean: 559.6990348229417\n",
      "  episode_reward_min: 491.1976250027252\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 9996\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1500.421\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5995142459869385\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0155869722366333\n",
      "        policy_loss: -0.046860333532094955\n",
      "        total_loss: 2.417818069458008\n",
      "        vf_explained_var: 0.9993134140968323\n",
      "        vf_loss: 2.4488964080810547\n",
      "    load_time_ms: 2.314\n",
      "    num_steps_sampled: 1948000\n",
      "    num_steps_trained: 1932416\n",
      "    sample_time_ms: 587.882\n",
      "    update_time_ms: 5.236\n",
      "  iterations_since_restore: 487\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.699999999999996\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4484563287210903\n",
      "    mean_inference_ms: 0.9690210204678334\n",
      "    mean_processing_ms: 0.23885679411652458\n",
      "  time_since_restore: 1026.5561256408691\n",
      "  time_this_iter_s: 2.089520215988159\n",
      "  time_total_s: 1026.5561256408691\n",
      "  timestamp: 1573083555\n",
      "  timesteps_since_restore: 1948000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1948000\n",
      "  training_iteration: 487\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1026 s, 487 iter, 1948000 ts, 560 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-39-21\n",
      "  done: false\n",
      "  episode_len_mean: 201.39\n",
      "  episode_reward_max: 597.539691177643\n",
      "  episode_reward_mean: 562.3249727986649\n",
      "  episode_reward_min: 491.1976250027252\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 10056\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1500.264\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.572577476501465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013809344731271267\n",
      "        policy_loss: -0.05351897329092026\n",
      "        total_loss: 2.688647747039795\n",
      "        vf_explained_var: 0.9992405772209167\n",
      "        vf_loss: 2.7281851768493652\n",
      "    load_time_ms: 2.449\n",
      "    num_steps_sampled: 1960000\n",
      "    num_steps_trained: 1944320\n",
      "    sample_time_ms: 588.262\n",
      "    update_time_ms: 5.322\n",
      "  iterations_since_restore: 490\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.3\n",
      "    gpu_util_percent0: 0.05500000000000001\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44864616037439503\n",
      "    mean_inference_ms: 0.970161485367884\n",
      "    mean_processing_ms: 0.23895207106726535\n",
      "  time_since_restore: 1032.8725736141205\n",
      "  time_this_iter_s: 2.1090993881225586\n",
      "  time_total_s: 1032.8725736141205\n",
      "  timestamp: 1573083561\n",
      "  timesteps_since_restore: 1960000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1960000\n",
      "  training_iteration: 490\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1032 s, 490 iter, 1960000 ts, 562 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-39-27\n",
      "  done: false\n",
      "  episode_len_mean: 202.8\n",
      "  episode_reward_max: 624.4631335833254\n",
      "  episode_reward_mean: 564.9440540596514\n",
      "  episode_reward_min: 521.389882604577\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 10115\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1483.431\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.597626209259033\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014153909869492054\n",
      "        policy_loss: -0.05013659968972206\n",
      "        total_loss: 5.709681034088135\n",
      "        vf_explained_var: 0.9984034895896912\n",
      "        vf_loss: 5.745487213134766\n",
      "    load_time_ms: 2.413\n",
      "    num_steps_sampled: 1972000\n",
      "    num_steps_trained: 1956224\n",
      "    sample_time_ms: 586.231\n",
      "    update_time_ms: 5.384\n",
      "  iterations_since_restore: 493\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.06666666666667\n",
      "    gpu_util_percent0: 0.09333333333333332\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4491414405900008\n",
      "    mean_inference_ms: 0.9713030948333126\n",
      "    mean_processing_ms: 0.2391817499296806\n",
      "  time_since_restore: 1039.0978622436523\n",
      "  time_this_iter_s: 2.0275232791900635\n",
      "  time_total_s: 1039.0978622436523\n",
      "  timestamp: 1573083567\n",
      "  timesteps_since_restore: 1972000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1972000\n",
      "  training_iteration: 493\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1039 s, 493 iter, 1972000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-39-34\n",
      "  done: false\n",
      "  episode_len_mean: 202.04\n",
      "  episode_reward_max: 624.4631335833254\n",
      "  episode_reward_mean: 563.498744006791\n",
      "  episode_reward_min: 511.5530502779721\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 10175\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1491.327\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5507571697235107\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013939940370619297\n",
      "        policy_loss: -0.050110794603824615\n",
      "        total_loss: 2.4727206230163574\n",
      "        vf_explained_var: 0.9992835521697998\n",
      "        vf_loss: 2.5087172985076904\n",
      "    load_time_ms: 2.523\n",
      "    num_steps_sampled: 1984000\n",
      "    num_steps_trained: 1968128\n",
      "    sample_time_ms: 592.481\n",
      "    update_time_ms: 5.667\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.1\n",
      "    gpu_util_percent0: 0.04666666666666667\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44945921443255854\n",
      "    mean_inference_ms: 0.9722896757998604\n",
      "    mean_processing_ms: 0.2394042138480448\n",
      "  time_since_restore: 1045.5862936973572\n",
      "  time_this_iter_s: 2.1025283336639404\n",
      "  time_total_s: 1045.5862936973572\n",
      "  timestamp: 1573083574\n",
      "  timesteps_since_restore: 1984000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1984000\n",
      "  training_iteration: 496\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1045 s, 496 iter, 1984000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-39-40\n",
      "  done: false\n",
      "  episode_len_mean: 202.41\n",
      "  episode_reward_max: 617.699947140299\n",
      "  episode_reward_mean: 565.2910556612188\n",
      "  episode_reward_min: 524.9734137851682\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 10235\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1489.837\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5331568717956543\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015216252766549587\n",
      "        policy_loss: -0.055166736245155334\n",
      "        total_loss: 2.322972536087036\n",
      "        vf_explained_var: 0.9993465542793274\n",
      "        vf_loss: 2.3627328872680664\n",
      "    load_time_ms: 2.392\n",
      "    num_steps_sampled: 1996000\n",
      "    num_steps_trained: 1980032\n",
      "    sample_time_ms: 598.33\n",
      "    update_time_ms: 5.701\n",
      "  iterations_since_restore: 499\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.1\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4491694258393323\n",
      "    mean_inference_ms: 0.9711250601384924\n",
      "    mean_processing_ms: 0.2391438524052753\n",
      "  time_since_restore: 1051.915292263031\n",
      "  time_this_iter_s: 2.075207233428955\n",
      "  time_total_s: 1051.915292263031\n",
      "  timestamp: 1573083580\n",
      "  timesteps_since_restore: 1996000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 1996000\n",
      "  training_iteration: 499\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1051 s, 499 iter, 1996000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-39-46\n",
      "  done: false\n",
      "  episode_len_mean: 202.96\n",
      "  episode_reward_max: 632.3068387000677\n",
      "  episode_reward_mean: 566.899251469917\n",
      "  episode_reward_min: 525.8867024008126\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 10295\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1486.884\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5349886417388916\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01705760695040226\n",
      "        policy_loss: -0.05624287202954292\n",
      "        total_loss: 2.643604040145874\n",
      "        vf_explained_var: 0.9992550611495972\n",
      "        vf_loss: 2.6825759410858154\n",
      "    load_time_ms: 2.63\n",
      "    num_steps_sampled: 2008000\n",
      "    num_steps_trained: 1991936\n",
      "    sample_time_ms: 592.853\n",
      "    update_time_ms: 5.819\n",
      "  iterations_since_restore: 502\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.5\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4488254347668992\n",
      "    mean_inference_ms: 0.9706217337461607\n",
      "    mean_processing_ms: 0.23920780210083703\n",
      "  time_since_restore: 1058.141761302948\n",
      "  time_this_iter_s: 2.1102752685546875\n",
      "  time_total_s: 1058.141761302948\n",
      "  timestamp: 1573083586\n",
      "  timesteps_since_restore: 2008000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2008000\n",
      "  training_iteration: 502\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1058 s, 502 iter, 2008000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-39-53\n",
      "  done: false\n",
      "  episode_len_mean: 203.37\n",
      "  episode_reward_max: 632.3068387000677\n",
      "  episode_reward_mean: 567.8305773469195\n",
      "  episode_reward_min: 528.2040345316416\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 10355\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1475.114\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4660227298736572\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013830668292939663\n",
      "        policy_loss: -0.05687236785888672\n",
      "        total_loss: 1.7635927200317383\n",
      "        vf_explained_var: 0.9994972348213196\n",
      "        vf_loss: 1.8064616918563843\n",
      "    load_time_ms: 2.505\n",
      "    num_steps_sampled: 2020000\n",
      "    num_steps_trained: 2003840\n",
      "    sample_time_ms: 585.618\n",
      "    update_time_ms: 5.914\n",
      "  iterations_since_restore: 505\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.233333333333334\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44832960250191384\n",
      "    mean_inference_ms: 0.9688249076612402\n",
      "    mean_processing_ms: 0.23887507102739522\n",
      "  time_since_restore: 1064.362608909607\n",
      "  time_this_iter_s: 2.087167978286743\n",
      "  time_total_s: 1064.362608909607\n",
      "  timestamp: 1573083593\n",
      "  timesteps_since_restore: 2020000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2020000\n",
      "  training_iteration: 505\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1064 s, 505 iter, 2020000 ts, 568 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-39-59\n",
      "  done: false\n",
      "  episode_len_mean: 202.35\n",
      "  episode_reward_max: 608.5298437223845\n",
      "  episode_reward_mean: 566.5686110882134\n",
      "  episode_reward_min: 495.0520542931322\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 10415\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1465.115\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5099120140075684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01370992511510849\n",
      "        policy_loss: -0.0478382334113121\n",
      "        total_loss: 4.18822717666626\n",
      "        vf_explained_var: 0.9988552927970886\n",
      "        vf_loss: 4.222184181213379\n",
      "    load_time_ms: 2.547\n",
      "    num_steps_sampled: 2032000\n",
      "    num_steps_trained: 2015744\n",
      "    sample_time_ms: 580.512\n",
      "    update_time_ms: 5.785\n",
      "  iterations_since_restore: 508\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.1\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44883505110883254\n",
      "    mean_inference_ms: 0.9707180748038146\n",
      "    mean_processing_ms: 0.23919243061248674\n",
      "  time_since_restore: 1070.5674221515656\n",
      "  time_this_iter_s: 2.0710248947143555\n",
      "  time_total_s: 1070.5674221515656\n",
      "  timestamp: 1573083599\n",
      "  timesteps_since_restore: 2032000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2032000\n",
      "  training_iteration: 508\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1070 s, 508 iter, 2032000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-40-05\n",
      "  done: false\n",
      "  episode_len_mean: 203.08\n",
      "  episode_reward_max: 617.9263273408867\n",
      "  episode_reward_mean: 567.7229244140608\n",
      "  episode_reward_min: 485.99677915623045\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 10473\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1473.394\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4680752754211426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015378119423985481\n",
      "        policy_loss: -0.05171643942594528\n",
      "        total_loss: 3.4602653980255127\n",
      "        vf_explained_var: 0.9990177154541016\n",
      "        vf_loss: 3.4964115619659424\n",
      "    load_time_ms: 2.418\n",
      "    num_steps_sampled: 2044000\n",
      "    num_steps_trained: 2027648\n",
      "    sample_time_ms: 586.12\n",
      "    update_time_ms: 5.789\n",
      "  iterations_since_restore: 511\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.300000000000004\n",
      "    gpu_util_percent0: 0.05333333333333334\n",
      "    ram_util_percent: 64.23333333333333\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4488210384596436\n",
      "    mean_inference_ms: 0.9712389251638891\n",
      "    mean_processing_ms: 0.2393348084025037\n",
      "  time_since_restore: 1076.890706539154\n",
      "  time_this_iter_s: 2.157386541366577\n",
      "  time_total_s: 1076.890706539154\n",
      "  timestamp: 1573083605\n",
      "  timesteps_since_restore: 2044000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2044000\n",
      "  training_iteration: 511\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1076 s, 511 iter, 2044000 ts, 568 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-40-12\n",
      "  done: false\n",
      "  episode_len_mean: 202.76\n",
      "  episode_reward_max: 617.9263273408867\n",
      "  episode_reward_mean: 566.5380716845432\n",
      "  episode_reward_min: 486.4336193209736\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 10532\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1475.105\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.507875919342041\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01533493772149086\n",
      "        policy_loss: -0.049477238208055496\n",
      "        total_loss: 2.7224528789520264\n",
      "        vf_explained_var: 0.9992304444313049\n",
      "        vf_loss: 2.7564032077789307\n",
      "    load_time_ms: 2.399\n",
      "    num_steps_sampled: 2056000\n",
      "    num_steps_trained: 2039552\n",
      "    sample_time_ms: 588.563\n",
      "    update_time_ms: 5.865\n",
      "  iterations_since_restore: 514\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.166666666666664\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4488024562138072\n",
      "    mean_inference_ms: 0.9713869352642434\n",
      "    mean_processing_ms: 0.23952715200888197\n",
      "  time_since_restore: 1083.1783330440521\n",
      "  time_this_iter_s: 2.092616558074951\n",
      "  time_total_s: 1083.1783330440521\n",
      "  timestamp: 1573083612\n",
      "  timesteps_since_restore: 2056000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2056000\n",
      "  training_iteration: 514\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1083 s, 514 iter, 2056000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-40-18\n",
      "  done: false\n",
      "  episode_len_mean: 202.07\n",
      "  episode_reward_max: 612.5557870042165\n",
      "  episode_reward_mean: 565.3726220468944\n",
      "  episode_reward_min: 474.6454480324374\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 10592\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1482.517\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.491478204727173\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014343625865876675\n",
      "        policy_loss: -0.04858290031552315\n",
      "        total_loss: 4.654189586639404\n",
      "        vf_explained_var: 0.998670220375061\n",
      "        vf_loss: 4.688249588012695\n",
      "    load_time_ms: 2.424\n",
      "    num_steps_sampled: 2068000\n",
      "    num_steps_trained: 2051456\n",
      "    sample_time_ms: 588.884\n",
      "    update_time_ms: 6.204\n",
      "  iterations_since_restore: 517\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.666666666666664\n",
      "    gpu_util_percent0: 0.06\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44847408440508135\n",
      "    mean_inference_ms: 0.9714204232434329\n",
      "    mean_processing_ms: 0.2393608701569577\n",
      "  time_since_restore: 1089.4850442409515\n",
      "  time_this_iter_s: 2.104485273361206\n",
      "  time_total_s: 1089.4850442409515\n",
      "  timestamp: 1573083618\n",
      "  timesteps_since_restore: 2068000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2068000\n",
      "  training_iteration: 517\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1089 s, 517 iter, 2068000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-40-24\n",
      "  done: false\n",
      "  episode_len_mean: 203.42\n",
      "  episode_reward_max: 612.5557870042165\n",
      "  episode_reward_mean: 568.6014598887059\n",
      "  episode_reward_min: 475.70335777608625\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 10648\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1492.091\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.441657304763794\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014283014461398125\n",
      "        policy_loss: -0.04261523857712746\n",
      "        total_loss: 3.5345373153686523\n",
      "        vf_explained_var: 0.9989987015724182\n",
      "        vf_loss: 3.5626909732818604\n",
      "    load_time_ms: 2.394\n",
      "    num_steps_sampled: 2080000\n",
      "    num_steps_trained: 2063360\n",
      "    sample_time_ms: 589.902\n",
      "    update_time_ms: 6.236\n",
      "  iterations_since_restore: 520\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.766666666666666\n",
      "    gpu_util_percent0: 0.19999999999999998\n",
      "    ram_util_percent: 64.3\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44882878790519987\n",
      "    mean_inference_ms: 0.973429531146804\n",
      "    mean_processing_ms: 0.23948909222746842\n",
      "  time_since_restore: 1095.84232878685\n",
      "  time_this_iter_s: 2.1087803840637207\n",
      "  time_total_s: 1095.84232878685\n",
      "  timestamp: 1573083624\n",
      "  timesteps_since_restore: 2080000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2080000\n",
      "  training_iteration: 520\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1095 s, 520 iter, 2080000 ts, 569 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-40-31\n",
      "  done: false\n",
      "  episode_len_mean: 203.21\n",
      "  episode_reward_max: 604.5470093166674\n",
      "  episode_reward_mean: 567.9160557017773\n",
      "  episode_reward_min: 475.70335777608625\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 10707\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1487.155\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.400977611541748\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014218312688171864\n",
      "        policy_loss: -0.05645408853888512\n",
      "        total_loss: 2.178863763809204\n",
      "        vf_explained_var: 0.9993982911109924\n",
      "        vf_loss: 2.220921754837036\n",
      "    load_time_ms: 2.508\n",
      "    num_steps_sampled: 2092000\n",
      "    num_steps_trained: 2075264\n",
      "    sample_time_ms: 590.332\n",
      "    update_time_ms: 6.142\n",
      "  iterations_since_restore: 523\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.43333333333333\n",
      "    gpu_util_percent0: 0.36000000000000004\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4488777241402851\n",
      "    mean_inference_ms: 0.9726482873974992\n",
      "    mean_processing_ms: 0.23941334270590425\n",
      "  time_since_restore: 1102.1495549678802\n",
      "  time_this_iter_s: 2.105896472930908\n",
      "  time_total_s: 1102.1495549678802\n",
      "  timestamp: 1573083631\n",
      "  timesteps_since_restore: 2092000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2092000\n",
      "  training_iteration: 523\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1102 s, 523 iter, 2092000 ts, 568 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-40-37\n",
      "  done: false\n",
      "  episode_len_mean: 201.73\n",
      "  episode_reward_max: 600.0749146449718\n",
      "  episode_reward_mean: 565.6225298374534\n",
      "  episode_reward_min: 495.6159446380954\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 10765\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1490.044\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.417262554168701\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015466025099158287\n",
      "        policy_loss: -0.05590815097093582\n",
      "        total_loss: 2.643389940261841\n",
      "        vf_explained_var: 0.9992396235466003\n",
      "        vf_loss: 2.6836390495300293\n",
      "    load_time_ms: 2.242\n",
      "    num_steps_sampled: 2104000\n",
      "    num_steps_trained: 2087168\n",
      "    sample_time_ms: 595.845\n",
      "    update_time_ms: 6.285\n",
      "  iterations_since_restore: 526\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.733333333333334\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4486731486605667\n",
      "    mean_inference_ms: 0.9707316184607339\n",
      "    mean_processing_ms: 0.23916913594635314\n",
      "  time_since_restore: 1108.5235531330109\n",
      "  time_this_iter_s: 2.070413112640381\n",
      "  time_total_s: 1108.5235531330109\n",
      "  timestamp: 1573083637\n",
      "  timesteps_since_restore: 2104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2104000\n",
      "  training_iteration: 526\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1108 s, 526 iter, 2104000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-40-43\n",
      "  done: false\n",
      "  episode_len_mean: 201.57\n",
      "  episode_reward_max: 605.2186184761536\n",
      "  episode_reward_mean: 565.8563230536\n",
      "  episode_reward_min: 512.1881785985877\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 10824\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1492.28\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4243533611297607\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014972700737416744\n",
      "        policy_loss: -0.04902808740735054\n",
      "        total_loss: 2.387704610824585\n",
      "        vf_explained_var: 0.9993425607681274\n",
      "        vf_loss: 2.4215729236602783\n",
      "    load_time_ms: 2.16\n",
      "    num_steps_sampled: 2116000\n",
      "    num_steps_trained: 2099072\n",
      "    sample_time_ms: 591.147\n",
      "    update_time_ms: 6.216\n",
      "  iterations_since_restore: 529\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.86666666666667\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.46666666666667\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44858663911755264\n",
      "    mean_inference_ms: 0.9707124163220704\n",
      "    mean_processing_ms: 0.23913225121037784\n",
      "  time_since_restore: 1114.8500444889069\n",
      "  time_this_iter_s: 2.0974836349487305\n",
      "  time_total_s: 1114.8500444889069\n",
      "  timestamp: 1573083643\n",
      "  timesteps_since_restore: 2116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2116000\n",
      "  training_iteration: 529\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1114 s, 529 iter, 2116000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-40-50\n",
      "  done: false\n",
      "  episode_len_mean: 202.69\n",
      "  episode_reward_max: 609.345875044189\n",
      "  episode_reward_mean: 569.1901652720596\n",
      "  episode_reward_min: 525.6826024877041\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 10886\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1501.716\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.414950370788574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01390847098082304\n",
      "        policy_loss: -0.04465777054429054\n",
      "        total_loss: 3.82515811920166\n",
      "        vf_explained_var: 0.9989771246910095\n",
      "        vf_loss: 3.8557331562042236\n",
      "    load_time_ms: 2.031\n",
      "    num_steps_sampled: 2128000\n",
      "    num_steps_trained: 2110976\n",
      "    sample_time_ms: 588.902\n",
      "    update_time_ms: 5.957\n",
      "  iterations_since_restore: 532\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.86666666666667\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4490101106729878\n",
      "    mean_inference_ms: 0.9723631240868863\n",
      "    mean_processing_ms: 0.2393707119451257\n",
      "  time_since_restore: 1121.223482131958\n",
      "  time_this_iter_s: 2.1247470378875732\n",
      "  time_total_s: 1121.223482131958\n",
      "  timestamp: 1573083650\n",
      "  timesteps_since_restore: 2128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2128000\n",
      "  training_iteration: 532\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1121 s, 532 iter, 2128000 ts, 569 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-40-56\n",
      "  done: false\n",
      "  episode_len_mean: 201.73\n",
      "  episode_reward_max: 609.345875044189\n",
      "  episode_reward_mean: 566.961661035883\n",
      "  episode_reward_min: 480.2906167463954\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 10944\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1493.635\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3441150188446045\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014368278905749321\n",
      "        policy_loss: -0.04550746828317642\n",
      "        total_loss: 5.478509902954102\n",
      "        vf_explained_var: 0.9984633326530457\n",
      "        vf_loss: 5.509469032287598\n",
      "    load_time_ms: 2.144\n",
      "    num_steps_sampled: 2140000\n",
      "    num_steps_trained: 2122880\n",
      "    sample_time_ms: 582.195\n",
      "    update_time_ms: 5.922\n",
      "  iterations_since_restore: 535\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.06666666666667\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4489339868666665\n",
      "    mean_inference_ms: 0.9710324407809316\n",
      "    mean_processing_ms: 0.23927459291913675\n",
      "  time_since_restore: 1127.4896957874298\n",
      "  time_this_iter_s: 2.0699431896209717\n",
      "  time_total_s: 1127.4896957874298\n",
      "  timestamp: 1573083656\n",
      "  timesteps_since_restore: 2140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2140000\n",
      "  training_iteration: 535\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1127 s, 535 iter, 2140000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-41-02\n",
      "  done: false\n",
      "  episode_len_mean: 199.52\n",
      "  episode_reward_max: 623.9740590340697\n",
      "  episode_reward_mean: 561.2215707890097\n",
      "  episode_reward_min: 462.4726236685964\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 11005\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1496.009\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3915865421295166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012403939850628376\n",
      "        policy_loss: -0.05053810030221939\n",
      "        total_loss: 5.779087066650391\n",
      "        vf_explained_var: 0.9983604550361633\n",
      "        vf_loss: 5.817065715789795\n",
      "    load_time_ms: 2.221\n",
      "    num_steps_sampled: 2152000\n",
      "    num_steps_trained: 2134784\n",
      "    sample_time_ms: 581.872\n",
      "    update_time_ms: 5.657\n",
      "  iterations_since_restore: 538\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.43333333333334\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44840868029347775\n",
      "    mean_inference_ms: 0.9696078539986516\n",
      "    mean_processing_ms: 0.23907423789066243\n",
      "  time_since_restore: 1133.8082704544067\n",
      "  time_this_iter_s: 2.092087507247925\n",
      "  time_total_s: 1133.8082704544067\n",
      "  timestamp: 1573083662\n",
      "  timesteps_since_restore: 2152000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2152000\n",
      "  training_iteration: 538\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1133 s, 538 iter, 2152000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-41-09\n",
      "  done: false\n",
      "  episode_len_mean: 199.87\n",
      "  episode_reward_max: 623.9740590340697\n",
      "  episode_reward_mean: 560.9333607011623\n",
      "  episode_reward_min: 399.7657720835035\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 11066\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1496.262\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.409240245819092\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0132675189524889\n",
      "        policy_loss: -0.051886867731809616\n",
      "        total_loss: 2.3357694149017334\n",
      "        vf_explained_var: 0.9993235468864441\n",
      "        vf_loss: 2.374222755432129\n",
      "    load_time_ms: 2.369\n",
      "    num_steps_sampled: 2164000\n",
      "    num_steps_trained: 2146688\n",
      "    sample_time_ms: 581.945\n",
      "    update_time_ms: 5.531\n",
      "  iterations_since_restore: 541\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.76666666666667\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4482981077596163\n",
      "    mean_inference_ms: 0.9696893022133866\n",
      "    mean_processing_ms: 0.23894470167720994\n",
      "  time_since_restore: 1140.1521377563477\n",
      "  time_this_iter_s: 2.064868211746216\n",
      "  time_total_s: 1140.1521377563477\n",
      "  timestamp: 1573083669\n",
      "  timesteps_since_restore: 2164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2164000\n",
      "  training_iteration: 541\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1140 s, 541 iter, 2164000 ts, 561 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-41-15\n",
      "  done: false\n",
      "  episode_len_mean: 201.97\n",
      "  episode_reward_max: 625.2703865778656\n",
      "  episode_reward_mean: 566.7096668306763\n",
      "  episode_reward_min: 491.64553180738136\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 11126\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1495.442\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3800745010375977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01583782024681568\n",
      "        policy_loss: -0.0417836494743824\n",
      "        total_loss: 9.424535751342773\n",
      "        vf_explained_var: 0.9973970055580139\n",
      "        vf_loss: 9.45028305053711\n",
      "    load_time_ms: 2.431\n",
      "    num_steps_sampled: 2176000\n",
      "    num_steps_trained: 2158592\n",
      "    sample_time_ms: 584.181\n",
      "    update_time_ms: 5.84\n",
      "  iterations_since_restore: 544\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.63333333333333\n",
      "    gpu_util_percent0: 0.04666666666666667\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44805838739419257\n",
      "    mean_inference_ms: 0.9699108097353448\n",
      "    mean_processing_ms: 0.23900257967563604\n",
      "  time_since_restore: 1146.489256620407\n",
      "  time_this_iter_s: 2.1131601333618164\n",
      "  time_total_s: 1146.489256620407\n",
      "  timestamp: 1573083675\n",
      "  timesteps_since_restore: 2176000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2176000\n",
      "  training_iteration: 544\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1146 s, 544 iter, 2176000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-41-21\n",
      "  done: false\n",
      "  episode_len_mean: 201.19\n",
      "  episode_reward_max: 600.4933879455317\n",
      "  episode_reward_mean: 565.6277346410964\n",
      "  episode_reward_min: 491.64553180738136\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 11186\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1499.349\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.371000051498413\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014015709049999714\n",
      "        policy_loss: -0.0457974374294281\n",
      "        total_loss: 3.2591822147369385\n",
      "        vf_explained_var: 0.9991031289100647\n",
      "        vf_loss: 3.290788412094116\n",
      "    load_time_ms: 2.376\n",
      "    num_steps_sampled: 2188000\n",
      "    num_steps_trained: 2170496\n",
      "    sample_time_ms: 583.089\n",
      "    update_time_ms: 5.812\n",
      "  iterations_since_restore: 547\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.23333333333334\n",
      "    gpu_util_percent0: 0.22999999999999998\n",
      "    ram_util_percent: 64.4\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4480061680928709\n",
      "    mean_inference_ms: 0.969093014166954\n",
      "    mean_processing_ms: 0.23889214216248278\n",
      "  time_since_restore: 1152.8126726150513\n",
      "  time_this_iter_s: 2.1199142932891846\n",
      "  time_total_s: 1152.8126726150513\n",
      "  timestamp: 1573083681\n",
      "  timesteps_since_restore: 2188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2188000\n",
      "  training_iteration: 547\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1152 s, 547 iter, 2188000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-41-28\n",
      "  done: false\n",
      "  episode_len_mean: 202.71\n",
      "  episode_reward_max: 635.96094011025\n",
      "  episode_reward_mean: 569.1548767410387\n",
      "  episode_reward_min: 519.1895086240847\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 11245\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1499.028\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.34407639503479\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014540795236825943\n",
      "        policy_loss: -0.05062508583068848\n",
      "        total_loss: 4.278891086578369\n",
      "        vf_explained_var: 0.9988197684288025\n",
      "        vf_loss: 4.314794063568115\n",
      "    load_time_ms: 2.334\n",
      "    num_steps_sampled: 2200000\n",
      "    num_steps_trained: 2182400\n",
      "    sample_time_ms: 586.823\n",
      "    update_time_ms: 6.028\n",
      "  iterations_since_restore: 550\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.7\n",
      "    gpu_util_percent0: 0.16666666666666666\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4481669665939769\n",
      "    mean_inference_ms: 0.9690308526085528\n",
      "    mean_processing_ms: 0.23883919287406166\n",
      "  time_since_restore: 1159.2255387306213\n",
      "  time_this_iter_s: 2.1523609161376953\n",
      "  time_total_s: 1159.2255387306213\n",
      "  timestamp: 1573083688\n",
      "  timesteps_since_restore: 2200000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2200000\n",
      "  training_iteration: 550\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1159 s, 550 iter, 2200000 ts, 569 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-41-34\n",
      "  done: false\n",
      "  episode_len_mean: 202.9\n",
      "  episode_reward_max: 613.3812429364037\n",
      "  episode_reward_mean: 569.9100628195882\n",
      "  episode_reward_min: 511.14466429163906\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 11304\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1500.515\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3067898750305176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013263811357319355\n",
      "        policy_loss: -0.043127987533807755\n",
      "        total_loss: 5.993349552154541\n",
      "        vf_explained_var: 0.998269259929657\n",
      "        vf_loss: 6.0230488777160645\n",
      "    load_time_ms: 2.361\n",
      "    num_steps_sampled: 2212000\n",
      "    num_steps_trained: 2194304\n",
      "    sample_time_ms: 588.734\n",
      "    update_time_ms: 5.942\n",
      "  iterations_since_restore: 553\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.900000000000006\n",
      "    gpu_util_percent0: 0.05333333333333334\n",
      "    ram_util_percent: 64.5\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4483680605826106\n",
      "    mean_inference_ms: 0.9703011852290505\n",
      "    mean_processing_ms: 0.2391230513860977\n",
      "  time_since_restore: 1165.553136587143\n",
      "  time_this_iter_s: 2.1065096855163574\n",
      "  time_total_s: 1165.553136587143\n",
      "  timestamp: 1573083694\n",
      "  timesteps_since_restore: 2212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2212000\n",
      "  training_iteration: 553\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1165 s, 553 iter, 2212000 ts, 570 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-41-41\n",
      "  done: false\n",
      "  episode_len_mean: 202.79\n",
      "  episode_reward_max: 623.9890066367591\n",
      "  episode_reward_mean: 569.9856927053181\n",
      "  episode_reward_min: 511.14466429163906\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 11365\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1501.852\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3145923614501953\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017129836603999138\n",
      "        policy_loss: -0.050721243023872375\n",
      "        total_loss: 1.5367274284362793\n",
      "        vf_explained_var: 0.9995762705802917\n",
      "        vf_loss: 1.5701048374176025\n",
      "    load_time_ms: 2.286\n",
      "    num_steps_sampled: 2224000\n",
      "    num_steps_trained: 2206208\n",
      "    sample_time_ms: 596.241\n",
      "    update_time_ms: 5.857\n",
      "  iterations_since_restore: 556\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.25\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.6\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4485576473855726\n",
      "    mean_inference_ms: 0.9710955390159222\n",
      "    mean_processing_ms: 0.23942045339025989\n",
      "  time_since_restore: 1171.951861858368\n",
      "  time_this_iter_s: 2.1206953525543213\n",
      "  time_total_s: 1171.951861858368\n",
      "  timestamp: 1573083701\n",
      "  timesteps_since_restore: 2224000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2224000\n",
      "  training_iteration: 556\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1171 s, 556 iter, 2224000 ts, 570 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-41-47\n",
      "  done: false\n",
      "  episode_len_mean: 201.68\n",
      "  episode_reward_max: 628.4679192179455\n",
      "  episode_reward_mean: 567.6431087459707\n",
      "  episode_reward_min: 482.5534976604455\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 11422\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1493.769\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2718963623046875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01565728522837162\n",
      "        policy_loss: -0.04640424996614456\n",
      "        total_loss: 3.167029619216919\n",
      "        vf_explained_var: 0.999104380607605\n",
      "        vf_loss: 3.1975808143615723\n",
      "    load_time_ms: 2.336\n",
      "    num_steps_sampled: 2236000\n",
      "    num_steps_trained: 2218112\n",
      "    sample_time_ms: 595.362\n",
      "    update_time_ms: 5.395\n",
      "  iterations_since_restore: 559\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.699999999999996\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.7\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44864992825342254\n",
      "    mean_inference_ms: 0.9708322269886949\n",
      "    mean_processing_ms: 0.23926979873975196\n",
      "  time_since_restore: 1178.2325875759125\n",
      "  time_this_iter_s: 2.084074020385742\n",
      "  time_total_s: 1178.2325875759125\n",
      "  timestamp: 1573083707\n",
      "  timesteps_since_restore: 2236000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2236000\n",
      "  training_iteration: 559\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1178 s, 559 iter, 2236000 ts, 568 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-41-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.84\n",
      "  episode_reward_max: 628.4679192179455\n",
      "  episode_reward_mean: 566.1686996185795\n",
      "  episode_reward_min: 482.5534976604455\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 11482\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1488.66\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.276890754699707\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015370276756584644\n",
      "        policy_loss: -0.05892007425427437\n",
      "        total_loss: 3.077775716781616\n",
      "        vf_explained_var: 0.9991599321365356\n",
      "        vf_loss: 3.121133327484131\n",
      "    load_time_ms: 2.233\n",
      "    num_steps_sampled: 2248000\n",
      "    num_steps_trained: 2230016\n",
      "    sample_time_ms: 592.946\n",
      "    update_time_ms: 5.286\n",
      "  iterations_since_restore: 562\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.53333333333333\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.7\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44846225368206016\n",
      "    mean_inference_ms: 0.9690329493002342\n",
      "    mean_processing_ms: 0.23904267751762337\n",
      "  time_since_restore: 1184.5319375991821\n",
      "  time_this_iter_s: 2.1024677753448486\n",
      "  time_total_s: 1184.5319375991821\n",
      "  timestamp: 1573083713\n",
      "  timesteps_since_restore: 2248000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2248000\n",
      "  training_iteration: 562\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1184 s, 562 iter, 2248000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-42-00\n",
      "  done: false\n",
      "  episode_len_mean: 200.01\n",
      "  episode_reward_max: 616.057122911503\n",
      "  episode_reward_mean: 564.9778741164421\n",
      "  episode_reward_min: 470.4950252654224\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 11542\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1484.688\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.184821128845215\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01482443418353796\n",
      "        policy_loss: -0.05101776123046875\n",
      "        total_loss: 2.8592162132263184\n",
      "        vf_explained_var: 0.999200165271759\n",
      "        vf_loss: 2.8952245712280273\n",
      "    load_time_ms: 2.127\n",
      "    num_steps_sampled: 2260000\n",
      "    num_steps_trained: 2241920\n",
      "    sample_time_ms: 583.86\n",
      "    update_time_ms: 5.71\n",
      "  iterations_since_restore: 565\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.46666666666666\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.7\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44816793409671574\n",
      "    mean_inference_ms: 0.9687017065838015\n",
      "    mean_processing_ms: 0.2390359477125924\n",
      "  time_since_restore: 1190.7964446544647\n",
      "  time_this_iter_s: 2.116503953933716\n",
      "  time_total_s: 1190.7964446544647\n",
      "  timestamp: 1573083720\n",
      "  timesteps_since_restore: 2260000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2260000\n",
      "  training_iteration: 565\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1190 s, 565 iter, 2260000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-42-06\n",
      "  done: false\n",
      "  episode_len_mean: 200.16\n",
      "  episode_reward_max: 608.0291411304041\n",
      "  episode_reward_mean: 566.0616182761456\n",
      "  episode_reward_min: 520.4848135509534\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 11602\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1484.134\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.23307204246521\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01627468504011631\n",
      "        policy_loss: -0.05354154109954834\n",
      "        total_loss: 1.8535676002502441\n",
      "        vf_explained_var: 0.9994997978210449\n",
      "        vf_loss: 1.890631079673767\n",
      "    load_time_ms: 2.428\n",
      "    num_steps_sampled: 2272000\n",
      "    num_steps_trained: 2253824\n",
      "    sample_time_ms: 584.925\n",
      "    update_time_ms: 5.845\n",
      "  iterations_since_restore: 568\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.6\n",
      "    gpu_util_percent0: 0.05000000000000001\n",
      "    ram_util_percent: 64.76666666666667\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4480407269601808\n",
      "    mean_inference_ms: 0.9700755921119799\n",
      "    mean_processing_ms: 0.23912155482089478\n",
      "  time_since_restore: 1197.1249377727509\n",
      "  time_this_iter_s: 2.1970622539520264\n",
      "  time_total_s: 1197.1249377727509\n",
      "  timestamp: 1573083726\n",
      "  timesteps_since_restore: 2272000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2272000\n",
      "  training_iteration: 568\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1197 s, 568 iter, 2272000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-42-12\n",
      "  done: false\n",
      "  episode_len_mean: 199.76\n",
      "  episode_reward_max: 618.234812389718\n",
      "  episode_reward_mean: 564.2103107210038\n",
      "  episode_reward_min: 465.42269174727204\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 11663\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1487.291\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.282292127609253\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014678509905934334\n",
      "        policy_loss: -0.05730167776346207\n",
      "        total_loss: 9.547076225280762\n",
      "        vf_explained_var: 0.9972877502441406\n",
      "        vf_loss: 9.589515686035156\n",
      "    load_time_ms: 2.319\n",
      "    num_steps_sampled: 2284000\n",
      "    num_steps_trained: 2265728\n",
      "    sample_time_ms: 583.746\n",
      "    update_time_ms: 5.786\n",
      "  iterations_since_restore: 571\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.36666666666667\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.7\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44846854105020817\n",
      "    mean_inference_ms: 0.9714571105693848\n",
      "    mean_processing_ms: 0.23935100483970395\n",
      "  time_since_restore: 1203.4246470928192\n",
      "  time_this_iter_s: 2.0781586170196533\n",
      "  time_total_s: 1203.4246470928192\n",
      "  timestamp: 1573083732\n",
      "  timesteps_since_restore: 2284000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2284000\n",
      "  training_iteration: 571\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1203 s, 571 iter, 2284000 ts, 564 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-42-19\n",
      "  done: false\n",
      "  episode_len_mean: 199.7\n",
      "  episode_reward_max: 618.234812389718\n",
      "  episode_reward_mean: 562.7361370445337\n",
      "  episode_reward_min: 352.25911657960006\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 11722\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1499.841\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2404062747955322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00996367260813713\n",
      "        policy_loss: -0.03994503244757652\n",
      "        total_loss: 43.57978057861328\n",
      "        vf_explained_var: 0.9881793856620789\n",
      "        vf_loss: 43.6096305847168\n",
      "    load_time_ms: 2.263\n",
      "    num_steps_sampled: 2296000\n",
      "    num_steps_trained: 2277632\n",
      "    sample_time_ms: 582.184\n",
      "    update_time_ms: 5.443\n",
      "  iterations_since_restore: 574\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.13333333333333\n",
      "    gpu_util_percent0: 0.18999999999999997\n",
      "    ram_util_percent: 64.7\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44869169295709405\n",
      "    mean_inference_ms: 0.9712072951444022\n",
      "    mean_processing_ms: 0.23940720728051582\n",
      "  time_since_restore: 1209.780142068863\n",
      "  time_this_iter_s: 2.138669013977051\n",
      "  time_total_s: 1209.780142068863\n",
      "  timestamp: 1573083739\n",
      "  timesteps_since_restore: 2296000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2296000\n",
      "  training_iteration: 574\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1209 s, 574 iter, 2296000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-42-25\n",
      "  done: false\n",
      "  episode_len_mean: 200.54\n",
      "  episode_reward_max: 619.8386709362286\n",
      "  episode_reward_mean: 565.1597994991719\n",
      "  episode_reward_min: 352.25911657960006\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 11781\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1505.961\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2020161151885986\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014879774302244186\n",
      "        policy_loss: -0.048702917993068695\n",
      "        total_loss: 1.8975796699523926\n",
      "        vf_explained_var: 0.9994490742683411\n",
      "        vf_loss: 1.9312169551849365\n",
      "    load_time_ms: 2.166\n",
      "    num_steps_sampled: 2308000\n",
      "    num_steps_trained: 2289536\n",
      "    sample_time_ms: 583.264\n",
      "    update_time_ms: 5.7\n",
      "  iterations_since_restore: 577\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.26666666666666\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 64.7\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44849930144195155\n",
      "    mean_inference_ms: 0.9702760955785459\n",
      "    mean_processing_ms: 0.2392196126229534\n",
      "  time_since_restore: 1216.0901420116425\n",
      "  time_this_iter_s: 2.1032416820526123\n",
      "  time_total_s: 1216.0901420116425\n",
      "  timestamp: 1573083745\n",
      "  timesteps_since_restore: 2308000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2308000\n",
      "  training_iteration: 577\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1216 s, 577 iter, 2308000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-42-31\n",
      "  done: false\n",
      "  episode_len_mean: 200.92\n",
      "  episode_reward_max: 619.8386709362286\n",
      "  episode_reward_mean: 568.0061144148825\n",
      "  episode_reward_min: 516.8917192039914\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 11841\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1495.004\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.201411247253418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014384716749191284\n",
      "        policy_loss: -0.04676470533013344\n",
      "        total_loss: 4.285726070404053\n",
      "        vf_explained_var: 0.9988210797309875\n",
      "        vf_loss: 4.317925930023193\n",
      "    load_time_ms: 2.068\n",
      "    num_steps_sampled: 2320000\n",
      "    num_steps_trained: 2301440\n",
      "    sample_time_ms: 576.019\n",
      "    update_time_ms: 5.632\n",
      "  iterations_since_restore: 580\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.83333333333333\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 64.7\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4483867269107435\n",
      "    mean_inference_ms: 0.9706290362768906\n",
      "    mean_processing_ms: 0.23924145293868385\n",
      "  time_since_restore: 1222.3236038684845\n",
      "  time_this_iter_s: 2.0764875411987305\n",
      "  time_total_s: 1222.3236038684845\n",
      "  timestamp: 1573083751\n",
      "  timesteps_since_restore: 2320000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2320000\n",
      "  training_iteration: 580\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.1/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1222 s, 580 iter, 2320000 ts, 568 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-42-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.71\n",
      "  episode_reward_max: 621.5114979162306\n",
      "  episode_reward_mean: 567.8334625247691\n",
      "  episode_reward_min: 511.2860639404559\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 11901\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1499.592\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1818549633026123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014446062967181206\n",
      "        policy_loss: -0.054558224976062775\n",
      "        total_loss: 2.4879183769226074\n",
      "        vf_explained_var: 0.9993221759796143\n",
      "        vf_loss: 2.5278501510620117\n",
      "    load_time_ms: 2.152\n",
      "    num_steps_sampled: 2332000\n",
      "    num_steps_trained: 2313344\n",
      "    sample_time_ms: 574.269\n",
      "    update_time_ms: 5.946\n",
      "  iterations_since_restore: 583\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.93333333333333\n",
      "    gpu_util_percent0: 0.05000000000000001\n",
      "    ram_util_percent: 64.83333333333333\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44806969980849376\n",
      "    mean_inference_ms: 0.9707740885042998\n",
      "    mean_processing_ms: 0.23917627639779845\n",
      "  time_since_restore: 1228.6515941619873\n",
      "  time_this_iter_s: 2.1310689449310303\n",
      "  time_total_s: 1228.6515941619873\n",
      "  timestamp: 1573083758\n",
      "  timesteps_since_restore: 2332000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2332000\n",
      "  training_iteration: 583\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1228 s, 583 iter, 2332000 ts, 568 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-42-44\n",
      "  done: false\n",
      "  episode_len_mean: 202.07\n",
      "  episode_reward_max: 621.5114979162306\n",
      "  episode_reward_mean: 570.018695192598\n",
      "  episode_reward_min: 499.89739843283496\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 11962\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1490.655\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.275909423828125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011489668861031532\n",
      "        policy_loss: -0.05025850236415863\n",
      "        total_loss: 20.275785446166992\n",
      "        vf_explained_var: 0.9945274591445923\n",
      "        vf_loss: 20.308597564697266\n",
      "    load_time_ms: 2.347\n",
      "    num_steps_sampled: 2344000\n",
      "    num_steps_trained: 2325248\n",
      "    sample_time_ms: 579.683\n",
      "    update_time_ms: 5.619\n",
      "  iterations_since_restore: 586\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.95\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 65.15\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4479294622729724\n",
      "    mean_inference_ms: 0.9711574905997685\n",
      "    mean_processing_ms: 0.2391232714396165\n",
      "  time_since_restore: 1234.9527385234833\n",
      "  time_this_iter_s: 2.0675411224365234\n",
      "  time_total_s: 1234.9527385234833\n",
      "  timestamp: 1573083764\n",
      "  timesteps_since_restore: 2344000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2344000\n",
      "  training_iteration: 586\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1234 s, 586 iter, 2344000 ts, 570 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-42-51\n",
      "  done: false\n",
      "  episode_len_mean: 200.45\n",
      "  episode_reward_max: 618.207114460494\n",
      "  episode_reward_mean: 566.6461626380311\n",
      "  episode_reward_min: 499.89739843283496\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 12021\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1504.414\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1862311363220215\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0116439713165164\n",
      "        policy_loss: -0.05102558434009552\n",
      "        total_loss: 2.1217870712280273\n",
      "        vf_explained_var: 0.9994209408760071\n",
      "        vf_loss: 2.1551284790039062\n",
      "    load_time_ms: 2.395\n",
      "    num_steps_sampled: 2356000\n",
      "    num_steps_trained: 2337152\n",
      "    sample_time_ms: 596.06\n",
      "    update_time_ms: 5.424\n",
      "  iterations_since_restore: 589\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.76666666666667\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 65.03333333333333\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4480238420080815\n",
      "    mean_inference_ms: 0.9715223475817344\n",
      "    mean_processing_ms: 0.23918296457077723\n",
      "  time_since_restore: 1241.5126922130585\n",
      "  time_this_iter_s: 2.178947925567627\n",
      "  time_total_s: 1241.5126922130585\n",
      "  timestamp: 1573083771\n",
      "  timesteps_since_restore: 2356000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2356000\n",
      "  training_iteration: 589\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1241 s, 589 iter, 2356000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-42-57\n",
      "  done: false\n",
      "  episode_len_mean: 200.11\n",
      "  episode_reward_max: 607.7220778520467\n",
      "  episode_reward_mean: 566.2351608131944\n",
      "  episode_reward_min: 507.6407383233865\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 12080\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1533.1\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.179063320159912\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012022163718938828\n",
      "        policy_loss: -0.04383726418018341\n",
      "        total_loss: 4.477652549743652\n",
      "        vf_explained_var: 0.9987506866455078\n",
      "        vf_loss: 4.503231525421143\n",
      "    load_time_ms: 3.069\n",
      "    num_steps_sampled: 2368000\n",
      "    num_steps_trained: 2349056\n",
      "    sample_time_ms: 610.611\n",
      "    update_time_ms: 5.262\n",
      "  iterations_since_restore: 592\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.36666666666667\n",
      "    gpu_util_percent0: 0.05333333333333334\n",
      "    ram_util_percent: 65.33333333333334\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44764151288525716\n",
      "    mean_inference_ms: 0.9695076033574133\n",
      "    mean_processing_ms: 0.2389398677719098\n",
      "  time_since_restore: 1248.2303779125214\n",
      "  time_this_iter_s: 2.2617762088775635\n",
      "  time_total_s: 1248.2303779125214\n",
      "  timestamp: 1573083777\n",
      "  timesteps_since_restore: 2368000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2368000\n",
      "  training_iteration: 592\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1248 s, 592 iter, 2368000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-43-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.63\n",
      "  episode_reward_max: 605.2431578388835\n",
      "  episode_reward_mean: 566.8564277775728\n",
      "  episode_reward_min: 471.85250551277113\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 12139\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1558.487\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1725215911865234\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013995759189128876\n",
      "        policy_loss: -0.055636029690504074\n",
      "        total_loss: 3.1286966800689697\n",
      "        vf_explained_var: 0.9991215467453003\n",
      "        vf_loss: 3.163076162338257\n",
      "    load_time_ms: 3.013\n",
      "    num_steps_sampled: 2380000\n",
      "    num_steps_trained: 2360960\n",
      "    sample_time_ms: 633.461\n",
      "    update_time_ms: 5.552\n",
      "  iterations_since_restore: 595\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.5\n",
      "    gpu_util_percent0: 0.05000000000000001\n",
      "    ram_util_percent: 65.56666666666666\n",
      "    vram_util_percent0: 0.16855967078189302\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4479679326021157\n",
      "    mean_inference_ms: 0.9710727141324343\n",
      "    mean_processing_ms: 0.23921157361600073\n",
      "  time_since_restore: 1255.0955374240875\n",
      "  time_this_iter_s: 2.273564338684082\n",
      "  time_total_s: 1255.0955374240875\n",
      "  timestamp: 1573083784\n",
      "  timesteps_since_restore: 2380000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2380000\n",
      "  training_iteration: 595\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1255 s, 595 iter, 2380000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-43-11\n",
      "  done: false\n",
      "  episode_len_mean: 202.05\n",
      "  episode_reward_max: 615.5196456985074\n",
      "  episode_reward_mean: 570.3644304973008\n",
      "  episode_reward_min: 471.85250551277113\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 12199\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1569.957\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1625826358795166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012214042246341705\n",
      "        policy_loss: -0.045942481607198715\n",
      "        total_loss: 4.413064002990723\n",
      "        vf_explained_var: 0.9987980127334595\n",
      "        vf_loss: 4.440456390380859\n",
      "    load_time_ms: 3.252\n",
      "    num_steps_sampled: 2392000\n",
      "    num_steps_trained: 2372864\n",
      "    sample_time_ms: 642.786\n",
      "    update_time_ms: 5.848\n",
      "  iterations_since_restore: 598\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.699999999999996\n",
      "    gpu_util_percent0: 0.15666666666666665\n",
      "    ram_util_percent: 66.16666666666666\n",
      "    vram_util_percent0: 0.16691358024691358\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44810717616429174\n",
      "    mean_inference_ms: 0.971876482358671\n",
      "    mean_processing_ms: 0.23940079731039554\n",
      "  time_since_restore: 1261.761525630951\n",
      "  time_this_iter_s: 2.21689510345459\n",
      "  time_total_s: 1261.761525630951\n",
      "  timestamp: 1573083791\n",
      "  timesteps_since_restore: 2392000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2392000\n",
      "  training_iteration: 598\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1261 s, 598 iter, 2392000 ts, 570 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-43-18\n",
      "  done: false\n",
      "  episode_len_mean: 201.46\n",
      "  episode_reward_max: 618.8121164305583\n",
      "  episode_reward_mean: 567.1626523467835\n",
      "  episode_reward_min: 400.93184158693487\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 12258\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1560.658\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1546478271484375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008747338317334652\n",
      "        policy_loss: -0.04259675368666649\n",
      "        total_loss: 10.594210624694824\n",
      "        vf_explained_var: 0.9971680641174316\n",
      "        vf_loss: 10.623523712158203\n",
      "    load_time_ms: 3.309\n",
      "    num_steps_sampled: 2404000\n",
      "    num_steps_trained: 2384768\n",
      "    sample_time_ms: 646.15\n",
      "    update_time_ms: 6.077\n",
      "  iterations_since_restore: 601\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.73333333333334\n",
      "    gpu_util_percent0: 0.26333333333333336\n",
      "    ram_util_percent: 65.9\n",
      "    vram_util_percent0: 0.16910836762688616\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44775344943914425\n",
      "    mean_inference_ms: 0.9712661491607132\n",
      "    mean_processing_ms: 0.23916540796387548\n",
      "  time_since_restore: 1268.3395354747772\n",
      "  time_this_iter_s: 2.226295232772827\n",
      "  time_total_s: 1268.3395354747772\n",
      "  timestamp: 1573083798\n",
      "  timesteps_since_restore: 2404000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2404000\n",
      "  training_iteration: 601\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1268 s, 601 iter, 2404000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-43-24\n",
      "  done: false\n",
      "  episode_len_mean: 199.51\n",
      "  episode_reward_max: 618.8121164305583\n",
      "  episode_reward_mean: 563.3105439399744\n",
      "  episode_reward_min: 400.93184158693487\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 12318\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1535.396\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0830800533294678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012577093206346035\n",
      "        policy_loss: -0.04887606203556061\n",
      "        total_loss: 2.4808247089385986\n",
      "        vf_explained_var: 0.9993212223052979\n",
      "        vf_loss: 2.510599374771118\n",
      "    load_time_ms: 2.722\n",
      "    num_steps_sampled: 2416000\n",
      "    num_steps_trained: 2396672\n",
      "    sample_time_ms: 623.985\n",
      "    update_time_ms: 6.357\n",
      "  iterations_since_restore: 604\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.26666666666666\n",
      "    gpu_util_percent0: 0.11\n",
      "    ram_util_percent: 65.9\n",
      "    vram_util_percent0: 0.16954732510288065\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44786787631779035\n",
      "    mean_inference_ms: 0.9717508630202206\n",
      "    mean_processing_ms: 0.2390786510125882\n",
      "  time_since_restore: 1274.7058861255646\n",
      "  time_this_iter_s: 2.138538122177124\n",
      "  time_total_s: 1274.7058861255646\n",
      "  timestamp: 1573083804\n",
      "  timesteps_since_restore: 2416000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2416000\n",
      "  training_iteration: 604\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1274 s, 604 iter, 2416000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-43-30\n",
      "  done: false\n",
      "  episode_len_mean: 199.88\n",
      "  episode_reward_max: 608.1646339677641\n",
      "  episode_reward_mean: 565.0995577150187\n",
      "  episode_reward_min: 510.52785087290647\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 12378\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1511.28\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1634795665740967\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01183430664241314\n",
      "        policy_loss: -0.04606085270643234\n",
      "        total_loss: 4.977720737457275\n",
      "        vf_explained_var: 0.998647928237915\n",
      "        vf_loss: 5.005807876586914\n",
      "    load_time_ms: 2.715\n",
      "    num_steps_sampled: 2428000\n",
      "    num_steps_trained: 2408576\n",
      "    sample_time_ms: 607.034\n",
      "    update_time_ms: 6.207\n",
      "  iterations_since_restore: 607\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.36666666666667\n",
      "    gpu_util_percent0: 0.11\n",
      "    ram_util_percent: 65.2\n",
      "    vram_util_percent0: 0.16954732510288065\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44812338675952273\n",
      "    mean_inference_ms: 0.9727755211993258\n",
      "    mean_processing_ms: 0.23923116187327925\n",
      "  time_since_restore: 1281.0127911567688\n",
      "  time_this_iter_s: 2.079514741897583\n",
      "  time_total_s: 1281.0127911567688\n",
      "  timestamp: 1573083810\n",
      "  timesteps_since_restore: 2428000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2428000\n",
      "  training_iteration: 607\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1281 s, 607 iter, 2428000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-43-37\n",
      "  done: false\n",
      "  episode_len_mean: 201.07\n",
      "  episode_reward_max: 613.0221848773566\n",
      "  episode_reward_mean: 567.1679366555773\n",
      "  episode_reward_min: 486.3790291096828\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 12439\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1498.198\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.155942678451538\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009978404268622398\n",
      "        policy_loss: -0.05413065850734711\n",
      "        total_loss: 11.366377830505371\n",
      "        vf_explained_var: 0.9968752264976501\n",
      "        vf_loss: 11.405354499816895\n",
      "    load_time_ms: 2.409\n",
      "    num_steps_sampled: 2440000\n",
      "    num_steps_trained: 2420480\n",
      "    sample_time_ms: 599.322\n",
      "    update_time_ms: 6.278\n",
      "  iterations_since_restore: 610\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.800000000000004\n",
      "    gpu_util_percent0: 0.11\n",
      "    ram_util_percent: 65.3\n",
      "    vram_util_percent0: 0.16954732510288065\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44795909095034153\n",
      "    mean_inference_ms: 0.9718806180625493\n",
      "    mean_processing_ms: 0.23917345104364224\n",
      "  time_since_restore: 1287.3854620456696\n",
      "  time_this_iter_s: 2.144075632095337\n",
      "  time_total_s: 1287.3854620456696\n",
      "  timestamp: 1573083817\n",
      "  timesteps_since_restore: 2440000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2440000\n",
      "  training_iteration: 610\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1287 s, 610 iter, 2440000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-43-43\n",
      "  done: false\n",
      "  episode_len_mean: 199.46\n",
      "  episode_reward_max: 606.1084554521506\n",
      "  episode_reward_mean: 563.0122266845284\n",
      "  episode_reward_min: 477.4928744979149\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 12499\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1497.025\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1149442195892334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013895004987716675\n",
      "        policy_loss: -0.04468159005045891\n",
      "        total_loss: 12.147117614746094\n",
      "        vf_explained_var: 0.9966613054275513\n",
      "        vf_loss: 12.170693397521973\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 2452000\n",
      "    num_steps_trained: 2432384\n",
      "    sample_time_ms: 591.765\n",
      "    update_time_ms: 6.12\n",
      "  iterations_since_restore: 613\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.333333333333336\n",
      "    gpu_util_percent0: 0.12666666666666668\n",
      "    ram_util_percent: 65.1\n",
      "    vram_util_percent0: 0.1677914951989026\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44812625712987014\n",
      "    mean_inference_ms: 0.9724391369940002\n",
      "    mean_processing_ms: 0.23938830259517835\n",
      "  time_since_restore: 1293.749583721161\n",
      "  time_this_iter_s: 2.1028668880462646\n",
      "  time_total_s: 1293.749583721161\n",
      "  timestamp: 1573083823\n",
      "  timesteps_since_restore: 2452000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2452000\n",
      "  training_iteration: 613\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1293 s, 613 iter, 2452000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-43-50\n",
      "  done: false\n",
      "  episode_len_mean: 198.98\n",
      "  episode_reward_max: 619.1785222126639\n",
      "  episode_reward_mean: 562.5025097651046\n",
      "  episode_reward_min: 477.4928744979149\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 12560\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1502.868\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.070497512817383\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01197934802621603\n",
      "        policy_loss: -0.04409167543053627\n",
      "        total_loss: 3.501448154449463\n",
      "        vf_explained_var: 0.9990551471710205\n",
      "        vf_loss: 3.5273468494415283\n",
      "    load_time_ms: 2.131\n",
      "    num_steps_sampled: 2464000\n",
      "    num_steps_trained: 2444288\n",
      "    sample_time_ms: 592.925\n",
      "    update_time_ms: 6.06\n",
      "  iterations_since_restore: 616\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.300000000000004\n",
      "    gpu_util_percent0: 0.12\n",
      "    ram_util_percent: 65.2\n",
      "    vram_util_percent0: 0.16724279835390946\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4483949055272795\n",
      "    mean_inference_ms: 0.974502675573643\n",
      "    mean_processing_ms: 0.23970476223058274\n",
      "  time_since_restore: 1300.182912826538\n",
      "  time_this_iter_s: 2.078469753265381\n",
      "  time_total_s: 1300.182912826538\n",
      "  timestamp: 1573083830\n",
      "  timesteps_since_restore: 2464000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2464000\n",
      "  training_iteration: 616\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1300 s, 616 iter, 2464000 ts, 563 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-43-56\n",
      "  done: false\n",
      "  episode_len_mean: 199.19\n",
      "  episode_reward_max: 619.1785222126639\n",
      "  episode_reward_mean: 563.7068301586356\n",
      "  episode_reward_min: 503.54005588881677\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 12619\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1481.027\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.097417116165161\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011588018387556076\n",
      "        policy_loss: -0.04775000363588333\n",
      "        total_loss: 5.6583476066589355\n",
      "        vf_explained_var: 0.9983924627304077\n",
      "        vf_loss: 5.688498020172119\n",
      "    load_time_ms: 2.258\n",
      "    num_steps_sampled: 2476000\n",
      "    num_steps_trained: 2456192\n",
      "    sample_time_ms: 598.958\n",
      "    update_time_ms: 5.636\n",
      "  iterations_since_restore: 619\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.75\n",
      "    gpu_util_percent0: 0.135\n",
      "    ram_util_percent: 65.2\n",
      "    vram_util_percent0: 0.16724279835390946\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44822587887461773\n",
      "    mean_inference_ms: 0.9732262993193563\n",
      "    mean_processing_ms: 0.2394990829410312\n",
      "  time_since_restore: 1306.3175325393677\n",
      "  time_this_iter_s: 2.0402300357818604\n",
      "  time_total_s: 1306.3175325393677\n",
      "  timestamp: 1573083836\n",
      "  timesteps_since_restore: 2476000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2476000\n",
      "  training_iteration: 619\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1306 s, 619 iter, 2476000 ts, 564 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-44-02\n",
      "  done: false\n",
      "  episode_len_mean: 201.62\n",
      "  episode_reward_max: 608.3195033186306\n",
      "  episode_reward_mean: 569.3996838189197\n",
      "  episode_reward_min: 503.54005588881677\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 12680\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1465.762\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0602028369903564\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011940055526793003\n",
      "        policy_loss: -0.04980601370334625\n",
      "        total_loss: 5.002899646759033\n",
      "        vf_explained_var: 0.998623788356781\n",
      "        vf_loss: 5.034571647644043\n",
      "    load_time_ms: 2.167\n",
      "    num_steps_sampled: 2488000\n",
      "    num_steps_trained: 2468096\n",
      "    sample_time_ms: 600.382\n",
      "    update_time_ms: 5.614\n",
      "  iterations_since_restore: 622\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.63333333333333\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 65.2\n",
      "    vram_util_percent0: 0.16724279835390946\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44813944636048975\n",
      "    mean_inference_ms: 0.9724641831431831\n",
      "    mean_processing_ms: 0.2394109128990996\n",
      "  time_since_restore: 1312.5865280628204\n",
      "  time_this_iter_s: 2.088958263397217\n",
      "  time_total_s: 1312.5865280628204\n",
      "  timestamp: 1573083842\n",
      "  timesteps_since_restore: 2488000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2488000\n",
      "  training_iteration: 622\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1312 s, 622 iter, 2488000 ts, 569 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-44-08\n",
      "  done: false\n",
      "  episode_len_mean: 202.2\n",
      "  episode_reward_max: 626.3823843292984\n",
      "  episode_reward_mean: 571.0820989388131\n",
      "  episode_reward_min: 517.4549565279227\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 12738\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1444.217\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.08915376663208\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011905958876013756\n",
      "        policy_loss: -0.04568422585725784\n",
      "        total_loss: 2.4994351863861084\n",
      "        vf_explained_var: 0.9993040561676025\n",
      "        vf_loss: 2.5270369052886963\n",
      "    load_time_ms: 2.286\n",
      "    num_steps_sampled: 2500000\n",
      "    num_steps_trained: 2480000\n",
      "    sample_time_ms: 601.282\n",
      "    update_time_ms: 5.779\n",
      "  iterations_since_restore: 625\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.13333333333333\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 65.3\n",
      "    vram_util_percent0: 0.16724279835390946\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44828015252562203\n",
      "    mean_inference_ms: 0.9736674986073236\n",
      "    mean_processing_ms: 0.2395761927090348\n",
      "  time_since_restore: 1318.841347694397\n",
      "  time_this_iter_s: 2.072622299194336\n",
      "  time_total_s: 1318.841347694397\n",
      "  timestamp: 1573083848\n",
      "  timesteps_since_restore: 2500000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2500000\n",
      "  training_iteration: 625\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1318 s, 625 iter, 2500000 ts, 571 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-44-15\n",
      "  done: false\n",
      "  episode_len_mean: 200.7\n",
      "  episode_reward_max: 626.3823843292984\n",
      "  episode_reward_mean: 568.5728424202596\n",
      "  episode_reward_min: 530.7945742024373\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 12799\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1453.828\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.076620578765869\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014231224544346333\n",
      "        policy_loss: -0.044317346066236496\n",
      "        total_loss: 2.573639392852783\n",
      "        vf_explained_var: 0.9992965459823608\n",
      "        vf_loss: 2.5963430404663086\n",
      "    load_time_ms: 2.431\n",
      "    num_steps_sampled: 2512000\n",
      "    num_steps_trained: 2491904\n",
      "    sample_time_ms: 597.29\n",
      "    update_time_ms: 6.046\n",
      "  iterations_since_restore: 628\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.333333333333336\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 65.33333333333333\n",
      "    vram_util_percent0: 0.16724279835390946\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44831149882487337\n",
      "    mean_inference_ms: 0.9735931772425416\n",
      "    mean_processing_ms: 0.23941920369048686\n",
      "  time_since_restore: 1325.0706143379211\n",
      "  time_this_iter_s: 2.080822706222534\n",
      "  time_total_s: 1325.0706143379211\n",
      "  timestamp: 1573083855\n",
      "  timesteps_since_restore: 2512000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2512000\n",
      "  training_iteration: 628\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1325 s, 628 iter, 2512000 ts, 569 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-44-21\n",
      "  done: false\n",
      "  episode_len_mean: 200.03\n",
      "  episode_reward_max: 610.4374376340822\n",
      "  episode_reward_mean: 567.5201387529501\n",
      "  episode_reward_min: 511.914714667612\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 12857\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1446.869\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0637800693511963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01020969171077013\n",
      "        policy_loss: -0.046741049736738205\n",
      "        total_loss: 3.5391159057617188\n",
      "        vf_explained_var: 0.9990090131759644\n",
      "        vf_loss: 3.5703508853912354\n",
      "    load_time_ms: 2.308\n",
      "    num_steps_sampled: 2524000\n",
      "    num_steps_trained: 2503808\n",
      "    sample_time_ms: 593.931\n",
      "    update_time_ms: 5.923\n",
      "  iterations_since_restore: 631\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.63333333333333\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 65.26666666666667\n",
      "    vram_util_percent0: 0.16724279835390946\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4480207799331138\n",
      "    mean_inference_ms: 0.9734424424172552\n",
      "    mean_processing_ms: 0.23936729933997086\n",
      "  time_since_restore: 1331.18190574646\n",
      "  time_this_iter_s: 2.0573318004608154\n",
      "  time_total_s: 1331.18190574646\n",
      "  timestamp: 1573083861\n",
      "  timesteps_since_restore: 2524000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2524000\n",
      "  training_iteration: 631\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1331 s, 631 iter, 2524000 ts, 568 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-44-27\n",
      "  done: false\n",
      "  episode_len_mean: 200.9\n",
      "  episode_reward_max: 610.4374376340822\n",
      "  episode_reward_mean: 568.3000197004196\n",
      "  episode_reward_min: 483.7862000251861\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 12917\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1454.214\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0360429286956787\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012986565940082073\n",
      "        policy_loss: -0.04980459809303284\n",
      "        total_loss: 2.6714024543762207\n",
      "        vf_explained_var: 0.9992521405220032\n",
      "        vf_loss: 2.701483726501465\n",
      "    load_time_ms: 2.173\n",
      "    num_steps_sampled: 2536000\n",
      "    num_steps_trained: 2515712\n",
      "    sample_time_ms: 591.415\n",
      "    update_time_ms: 5.76\n",
      "  iterations_since_restore: 634\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.699999999999996\n",
      "    gpu_util_percent0: 0.05000000000000001\n",
      "    ram_util_percent: 64.96666666666667\n",
      "    vram_util_percent0: 0.1677914951989026\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4481168241748827\n",
      "    mean_inference_ms: 0.9729166698190166\n",
      "    mean_processing_ms: 0.2393494269555806\n",
      "  time_since_restore: 1337.4960567951202\n",
      "  time_this_iter_s: 2.124640941619873\n",
      "  time_total_s: 1337.4960567951202\n",
      "  timestamp: 1573083867\n",
      "  timesteps_since_restore: 2536000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2536000\n",
      "  training_iteration: 634\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1337 s, 634 iter, 2536000 ts, 568 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-44-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.34\n",
      "  episode_reward_max: 601.8227708968914\n",
      "  episode_reward_mean: 566.6720607518516\n",
      "  episode_reward_min: 505.3352845135728\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 12978\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1452.691\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0689191818237305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012859376147389412\n",
      "        policy_loss: -0.047758132219314575\n",
      "        total_loss: 4.899815082550049\n",
      "        vf_explained_var: 0.9986814856529236\n",
      "        vf_loss: 4.928042411804199\n",
      "    load_time_ms: 2.11\n",
      "    num_steps_sampled: 2548000\n",
      "    num_steps_trained: 2527616\n",
      "    sample_time_ms: 596.006\n",
      "    update_time_ms: 5.887\n",
      "  iterations_since_restore: 637\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.166666666666664\n",
      "    gpu_util_percent0: 0.06\n",
      "    ram_util_percent: 65.1\n",
      "    vram_util_percent0: 0.1677914951989026\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44849021758208396\n",
      "    mean_inference_ms: 0.9742112517766961\n",
      "    mean_processing_ms: 0.23953631595705838\n",
      "  time_since_restore: 1343.762326002121\n",
      "  time_this_iter_s: 2.0253539085388184\n",
      "  time_total_s: 1343.762326002121\n",
      "  timestamp: 1573083873\n",
      "  timesteps_since_restore: 2548000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2548000\n",
      "  training_iteration: 637\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1343 s, 637 iter, 2548000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-44-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.03\n",
      "  episode_reward_max: 603.2939672634498\n",
      "  episode_reward_mean: 566.5343026201822\n",
      "  episode_reward_min: 479.9407436149689\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 13038\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1486.051\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0432145595550537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010647786781191826\n",
      "        policy_loss: -0.05195469409227371\n",
      "        total_loss: 5.530993461608887\n",
      "        vf_explained_var: 0.9984802007675171\n",
      "        vf_loss: 5.566776752471924\n",
      "    load_time_ms: 2.22\n",
      "    num_steps_sampled: 2560000\n",
      "    num_steps_trained: 2539520\n",
      "    sample_time_ms: 611.053\n",
      "    update_time_ms: 5.972\n",
      "  iterations_since_restore: 640\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.666666666666664\n",
      "    gpu_util_percent0: 0.07666666666666667\n",
      "    ram_util_percent: 65.16666666666667\n",
      "    vram_util_percent0: 0.1688888888888889\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4483949598455394\n",
      "    mean_inference_ms: 0.9747082964552035\n",
      "    mean_processing_ms: 0.23966162388099016\n",
      "  time_since_restore: 1350.3844604492188\n",
      "  time_this_iter_s: 2.229459047317505\n",
      "  time_total_s: 1350.3844604492188\n",
      "  timestamp: 1573083880\n",
      "  timesteps_since_restore: 2560000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2560000\n",
      "  training_iteration: 640\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1350 s, 640 iter, 2560000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-44-47\n",
      "  done: false\n",
      "  episode_len_mean: 199.48\n",
      "  episode_reward_max: 602.6450345548933\n",
      "  episode_reward_mean: 565.3934190091237\n",
      "  episode_reward_min: 479.9407436149689\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 13099\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1514.7\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.074389696121216\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010696831159293652\n",
      "        policy_loss: -0.04657110944390297\n",
      "        total_loss: 14.405856132507324\n",
      "        vf_explained_var: 0.9960333108901978\n",
      "        vf_loss: 14.43618106842041\n",
      "    load_time_ms: 2.308\n",
      "    num_steps_sampled: 2572000\n",
      "    num_steps_trained: 2551424\n",
      "    sample_time_ms: 617.386\n",
      "    update_time_ms: 6.048\n",
      "  iterations_since_restore: 643\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.666666666666664\n",
      "    gpu_util_percent0: 0.13\n",
      "    ram_util_percent: 65.6\n",
      "    vram_util_percent0: 0.1688888888888889\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44849248747767034\n",
      "    mean_inference_ms: 0.9738050012179416\n",
      "    mean_processing_ms: 0.23974823650649627\n",
      "  time_since_restore: 1356.9799206256866\n",
      "  time_this_iter_s: 2.2117974758148193\n",
      "  time_total_s: 1356.9799206256866\n",
      "  timestamp: 1573083887\n",
      "  timesteps_since_restore: 2572000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2572000\n",
      "  training_iteration: 643\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1356 s, 643 iter, 2572000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-44-53\n",
      "  done: false\n",
      "  episode_len_mean: 198.92\n",
      "  episode_reward_max: 605.7260365185795\n",
      "  episode_reward_mean: 563.5214380616868\n",
      "  episode_reward_min: 430.34276987572\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 13158\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1503.733\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0620036125183105\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00925065390765667\n",
      "        policy_loss: -0.048228293657302856\n",
      "        total_loss: 16.715469360351562\n",
      "        vf_explained_var: 0.9954273104667664\n",
      "        vf_loss: 16.74964714050293\n",
      "    load_time_ms: 2.4\n",
      "    num_steps_sampled: 2584000\n",
      "    num_steps_trained: 2563328\n",
      "    sample_time_ms: 629.992\n",
      "    update_time_ms: 5.973\n",
      "  iterations_since_restore: 646\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.73333333333334\n",
      "    gpu_util_percent0: 0.26666666666666666\n",
      "    ram_util_percent: 65.4\n",
      "    vram_util_percent0: 0.1688888888888889\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44785632958068866\n",
      "    mean_inference_ms: 0.9735618140817703\n",
      "    mean_processing_ms: 0.23960147661980302\n",
      "  time_since_restore: 1363.355962753296\n",
      "  time_this_iter_s: 2.106393575668335\n",
      "  time_total_s: 1363.355962753296\n",
      "  timestamp: 1573083893\n",
      "  timesteps_since_restore: 2584000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2584000\n",
      "  training_iteration: 646\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1363 s, 646 iter, 2584000 ts, 564 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-44-59\n",
      "  done: false\n",
      "  episode_len_mean: 201.1\n",
      "  episode_reward_max: 606.2028174207683\n",
      "  episode_reward_mean: 568.6144150317037\n",
      "  episode_reward_min: 430.34276987572\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 13218\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1504.358\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.037496328353882\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013460933230817318\n",
      "        policy_loss: -0.04939047247171402\n",
      "        total_loss: 2.4883530139923096\n",
      "        vf_explained_var: 0.9993206858634949\n",
      "        vf_loss: 2.517299175262451\n",
      "    load_time_ms: 2.468\n",
      "    num_steps_sampled: 2596000\n",
      "    num_steps_trained: 2575232\n",
      "    sample_time_ms: 626.562\n",
      "    update_time_ms: 5.51\n",
      "  iterations_since_restore: 649\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.93333333333334\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 65.3\n",
      "    vram_util_percent0: 0.1688888888888889\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4477838693332068\n",
      "    mean_inference_ms: 0.9736818291234626\n",
      "    mean_processing_ms: 0.23951762894062045\n",
      "  time_since_restore: 1369.7368848323822\n",
      "  time_this_iter_s: 2.0853090286254883\n",
      "  time_total_s: 1369.7368848323822\n",
      "  timestamp: 1573083899\n",
      "  timesteps_since_restore: 2596000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2596000\n",
      "  training_iteration: 649\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1369 s, 649 iter, 2596000 ts, 569 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-45-06\n",
      "  done: false\n",
      "  episode_len_mean: 200.31\n",
      "  episode_reward_max: 606.2028174207683\n",
      "  episode_reward_mean: 567.0877142490309\n",
      "  episode_reward_min: 442.4952065518995\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 13278\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1480.092\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.079052448272705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012666244991123676\n",
      "        policy_loss: -0.053953833878040314\n",
      "        total_loss: 2.8891220092773438\n",
      "        vf_explained_var: 0.9992180466651917\n",
      "        vf_loss: 2.9238388538360596\n",
      "    load_time_ms: 2.597\n",
      "    num_steps_sampled: 2608000\n",
      "    num_steps_trained: 2587136\n",
      "    sample_time_ms: 633.223\n",
      "    update_time_ms: 5.264\n",
      "  iterations_since_restore: 652\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.800000000000004\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 65.73333333333333\n",
      "    vram_util_percent0: 0.1688888888888889\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44829474869359615\n",
      "    mean_inference_ms: 0.9753472093886151\n",
      "    mean_processing_ms: 0.23979033761844587\n",
      "  time_since_restore: 1376.1867725849152\n",
      "  time_this_iter_s: 2.1843082904815674\n",
      "  time_total_s: 1376.1867725849152\n",
      "  timestamp: 1573083906\n",
      "  timesteps_since_restore: 2608000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2608000\n",
      "  training_iteration: 652\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1376 s, 652 iter, 2608000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-45-12\n",
      "  done: false\n",
      "  episode_len_mean: 199.28\n",
      "  episode_reward_max: 596.975183409701\n",
      "  episode_reward_mean: 565.1367122946393\n",
      "  episode_reward_min: 442.4952065518995\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 13338\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1465.882\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.026942491531372\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011015528813004494\n",
      "        policy_loss: -0.046532753854990005\n",
      "        total_loss: 12.943127632141113\n",
      "        vf_explained_var: 0.9964690208435059\n",
      "        vf_loss: 12.972931861877441\n",
      "    load_time_ms: 2.625\n",
      "    num_steps_sampled: 2620000\n",
      "    num_steps_trained: 2599040\n",
      "    sample_time_ms: 624.123\n",
      "    update_time_ms: 5.245\n",
      "  iterations_since_restore: 655\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.75\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 65.6\n",
      "    vram_util_percent0: 0.1688888888888889\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4481801165095834\n",
      "    mean_inference_ms: 0.9745520493911292\n",
      "    mean_processing_ms: 0.2397303715439449\n",
      "  time_since_restore: 1382.4269828796387\n",
      "  time_this_iter_s: 2.0806901454925537\n",
      "  time_total_s: 1382.4269828796387\n",
      "  timestamp: 1573083912\n",
      "  timesteps_since_restore: 2620000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2620000\n",
      "  training_iteration: 655\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1382 s, 655 iter, 2620000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-45-19\n",
      "  done: false\n",
      "  episode_len_mean: 199.07\n",
      "  episode_reward_max: 603.574436987448\n",
      "  episode_reward_mean: 565.1024649068761\n",
      "  episode_reward_min: 439.239589564124\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 13398\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1453.1\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0778331756591797\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00877399928867817\n",
      "        policy_loss: -0.046677764505147934\n",
      "        total_loss: 8.018962860107422\n",
      "        vf_explained_var: 0.9977921843528748\n",
      "        vf_loss: 8.052314758300781\n",
      "    load_time_ms: 2.648\n",
      "    num_steps_sampled: 2632000\n",
      "    num_steps_trained: 2610944\n",
      "    sample_time_ms: 627.122\n",
      "    update_time_ms: 5.3\n",
      "  iterations_since_restore: 658\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.26666666666666\n",
      "    gpu_util_percent0: 0.060000000000000005\n",
      "    ram_util_percent: 66.0\n",
      "    vram_util_percent0: 0.1688888888888889\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44800000231792303\n",
      "    mean_inference_ms: 0.9733477852120243\n",
      "    mean_processing_ms: 0.23958062076124292\n",
      "  time_since_restore: 1388.73566365242\n",
      "  time_this_iter_s: 2.2471518516540527\n",
      "  time_total_s: 1388.73566365242\n",
      "  timestamp: 1573083919\n",
      "  timesteps_since_restore: 2632000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2632000\n",
      "  training_iteration: 658\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1388 s, 658 iter, 2632000 ts, 565 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-45-25\n",
      "  done: false\n",
      "  episode_len_mean: 199.35\n",
      "  episode_reward_max: 603.574436987448\n",
      "  episode_reward_mean: 567.0387338731605\n",
      "  episode_reward_min: 457.5367306846596\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 13458\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1497.318\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0056254863739014\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012259683571755886\n",
      "        policy_loss: -0.05137547105550766\n",
      "        total_loss: 1.9919945001602173\n",
      "        vf_explained_var: 0.9994467496871948\n",
      "        vf_loss: 2.0247509479522705\n",
      "    load_time_ms: 2.432\n",
      "    num_steps_sampled: 2644000\n",
      "    num_steps_trained: 2622848\n",
      "    sample_time_ms: 631.241\n",
      "    update_time_ms: 5.724\n",
      "  iterations_since_restore: 661\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.36666666666667\n",
      "    gpu_util_percent0: 0.06666666666666667\n",
      "    ram_util_percent: 66.26666666666667\n",
      "    vram_util_percent0: 0.1688888888888889\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44829342232235625\n",
      "    mean_inference_ms: 0.974315701172833\n",
      "    mean_processing_ms: 0.2397851431454057\n",
      "  time_since_restore: 1395.5680623054504\n",
      "  time_this_iter_s: 2.410938262939453\n",
      "  time_total_s: 1395.5680623054504\n",
      "  timestamp: 1573083925\n",
      "  timesteps_since_restore: 2644000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2644000\n",
      "  training_iteration: 661\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1395 s, 661 iter, 2644000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-45-32\n",
      "  done: false\n",
      "  episode_len_mean: 199.74\n",
      "  episode_reward_max: 612.1239846465647\n",
      "  episode_reward_mean: 568.1783459630414\n",
      "  episode_reward_min: 470.1038766263025\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 13518\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1524.327\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9801416397094727\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014138517901301384\n",
      "        policy_loss: -0.05344586819410324\n",
      "        total_loss: 2.170635938644409\n",
      "        vf_explained_var: 0.9993823170661926\n",
      "        vf_loss: 2.202608823776245\n",
      "    load_time_ms: 2.38\n",
      "    num_steps_sampled: 2656000\n",
      "    num_steps_trained: 2634752\n",
      "    sample_time_ms: 652.777\n",
      "    update_time_ms: 6.44\n",
      "  iterations_since_restore: 664\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.96666666666667\n",
      "    gpu_util_percent0: 0.06\n",
      "    ram_util_percent: 66.73333333333333\n",
      "    vram_util_percent0: 0.1688888888888889\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4484737990799498\n",
      "    mean_inference_ms: 0.9761796003875222\n",
      "    mean_processing_ms: 0.24009558258589522\n",
      "  time_since_restore: 1402.4160969257355\n",
      "  time_this_iter_s: 2.347278356552124\n",
      "  time_total_s: 1402.4160969257355\n",
      "  timestamp: 1573083932\n",
      "  timesteps_since_restore: 2656000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2656000\n",
      "  training_iteration: 664\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1402 s, 664 iter, 2656000 ts, 568 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-45-39\n",
      "  done: false\n",
      "  episode_len_mean: 199.28\n",
      "  episode_reward_max: 612.1239846465647\n",
      "  episode_reward_mean: 566.7089068860095\n",
      "  episode_reward_min: 470.1038766263025\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 13580\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1576.619\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9931658506393433\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013146902434527874\n",
      "        policy_loss: -0.04617411643266678\n",
      "        total_loss: 4.2473015785217285\n",
      "        vf_explained_var: 0.9988351464271545\n",
      "        vf_loss: 4.2735090255737305\n",
      "    load_time_ms: 2.45\n",
      "    num_steps_sampled: 2668000\n",
      "    num_steps_trained: 2646656\n",
      "    sample_time_ms: 679.912\n",
      "    update_time_ms: 6.489\n",
      "  iterations_since_restore: 667\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.199999999999996\n",
      "    gpu_util_percent0: 0.07333333333333335\n",
      "    ram_util_percent: 66.76666666666667\n",
      "    vram_util_percent0: 0.1688888888888889\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4486602265807848\n",
      "    mean_inference_ms: 0.9765030293915024\n",
      "    mean_processing_ms: 0.24034240278043095\n",
      "  time_since_restore: 1409.3672330379486\n",
      "  time_this_iter_s: 2.401101589202881\n",
      "  time_total_s: 1409.3672330379486\n",
      "  timestamp: 1573083939\n",
      "  timesteps_since_restore: 2668000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2668000\n",
      "  training_iteration: 667\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1409 s, 667 iter, 2668000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 199.22\n",
      "  episode_reward_max: 608.6530214773521\n",
      "  episode_reward_mean: 567.2936982210325\n",
      "  episode_reward_min: 499.31420641614585\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 13640\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1587.944\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.954565405845642\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012269612401723862\n",
      "        policy_loss: -0.04551934078335762\n",
      "        total_loss: 2.861525297164917\n",
      "        vf_explained_var: 0.9992226958274841\n",
      "        vf_loss: 2.8884098529815674\n",
      "    load_time_ms: 2.503\n",
      "    num_steps_sampled: 2680000\n",
      "    num_steps_trained: 2658560\n",
      "    sample_time_ms: 686.045\n",
      "    update_time_ms: 6.512\n",
      "  iterations_since_restore: 670\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.5\n",
      "    gpu_util_percent0: 0.13\n",
      "    ram_util_percent: 66.63333333333334\n",
      "    vram_util_percent0: 0.1688888888888889\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4487418267670925\n",
      "    mean_inference_ms: 0.9760969192418546\n",
      "    mean_processing_ms: 0.24011227424723716\n",
      "  time_since_restore: 1416.2242271900177\n",
      "  time_this_iter_s: 2.227980852127075\n",
      "  time_total_s: 1416.2242271900177\n",
      "  timestamp: 1573083946\n",
      "  timesteps_since_restore: 2680000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2680000\n",
      "  training_iteration: 670\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1416 s, 670 iter, 2680000 ts, 567 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-45-53\n",
      "  done: false\n",
      "  episode_len_mean: 198.81\n",
      "  episode_reward_max: 604.764855545899\n",
      "  episode_reward_mean: 565.8616084141901\n",
      "  episode_reward_min: 499.31420641614585\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 13700\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1591.264\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9741214513778687\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012341792695224285\n",
      "        policy_loss: -0.04704178497195244\n",
      "        total_loss: 2.1121182441711426\n",
      "        vf_explained_var: 0.9994110465049744\n",
      "        vf_loss: 2.140415906906128\n",
      "    load_time_ms: 2.43\n",
      "    num_steps_sampled: 2692000\n",
      "    num_steps_trained: 2670464\n",
      "    sample_time_ms: 667.876\n",
      "    update_time_ms: 5.993\n",
      "  iterations_since_restore: 673\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.666666666666664\n",
      "    gpu_util_percent0: 0.24333333333333332\n",
      "    ram_util_percent: 66.33333333333333\n",
      "    vram_util_percent0: 0.16724279835390946\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.44892997231101234\n",
      "    mean_inference_ms: 0.9786483357243414\n",
      "    mean_processing_ms: 0.2405742953466985\n",
      "  time_since_restore: 1422.976160287857\n",
      "  time_this_iter_s: 2.1455562114715576\n",
      "  time_total_s: 1422.976160287857\n",
      "  timestamp: 1573083953\n",
      "  timesteps_since_restore: 2692000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2692000\n",
      "  training_iteration: 673\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1422 s, 673 iter, 2692000 ts, 566 rew\n",
      "\n",
      "Result for PPO_Walker2d-v3_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-11-06_15-46-00\n",
      "  done: false\n",
      "  episode_len_mean: 199.13\n",
      "  episode_reward_max: 616.2596527366351\n",
      "  episode_reward_mean: 566.3505158971619\n",
      "  episode_reward_min: 418.762149878311\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 13758\n",
      "  experiment_id: d665416668f645709c816b98a5e03ab0\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    grad_time_ms: 1584.396\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.937386393547058\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011805261485278606\n",
      "        policy_loss: -0.04839255288243294\n",
      "        total_loss: 4.001163959503174\n",
      "        vf_explained_var: 0.9988892674446106\n",
      "        vf_loss: 4.031627178192139\n",
      "    load_time_ms: 2.259\n",
      "    num_steps_sampled: 2704000\n",
      "    num_steps_trained: 2682368\n",
      "    sample_time_ms: 645.865\n",
      "    update_time_ms: 5.645\n",
      "  iterations_since_restore: 676\n",
      "  node_ip: 192.168.0.12\n",
      "  num_healthy_workers: 15\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.0\n",
      "    gpu_util_percent0: 0.043333333333333335\n",
      "    ram_util_percent: 66.03333333333335\n",
      "    vram_util_percent0: 0.16724279835390946\n",
      "  pid: 7688\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.4487330402784896\n",
      "    mean_inference_ms: 0.9782702319028367\n",
      "    mean_processing_ms: 0.240290085739159\n",
      "  time_since_restore: 1429.5640742778778\n",
      "  time_this_iter_s: 2.2326297760009766\n",
      "  time_total_s: 1429.5640742778778\n",
      "  timestamp: 1573083960\n",
      "  timesteps_since_restore: 2704000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 2704000\n",
      "  training_iteration: 676\n",
      "  trial_id: 3983a1e6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 16/16 CPUs, 1/1 GPUs, 0.0/4.2 GiB heap, 0.0/1.42 GiB objects\n",
      "Memory usage on this node: 10.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_Walker2d-v3_0:\tRUNNING, [16 CPUs, 1 GPUs], [pid=7688], 1429 s, 676 iter, 2704000 ts, 566 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 1\n",
    "config[\"num_workers\"] = 15\n",
    "config[\"eager\"] = False\n",
    "config[\"model\"][\"fcnet_hiddens\"] = [64,64]\n",
    "config[\"lr\"] = 5e-5\n",
    "config[\"env\"] = \"Walker2d-v3\"\n",
    "#config[\"env\"] = \"Walker2DBulletEnv-v0\"\n",
    "config['observation_filter'] = 'MeanStdFilter'\n",
    "ray.init()\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config = config,\n",
    "    stop={\"timesteps_total\": 1e7},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2019-11-06 13:47:23,901\tINFO resource_spec.py:205 -- Starting Ray with 5.13 GiB memory available for workers and up to 2.57 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 6.3/15.7 GiB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 6.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m 2019-11-06 13:47:25,909\tINFO trainer.py:344 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m 2019-11-06 13:47:26,523\tINFO ars.py:184 -- Creating shared noise table.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m 2019-11-06 13:47:33,943\tINFO ars.py:189 -- Creating actors.\n",
      "\u001b[2m\u001b[36m(pid=3795)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3795)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3795)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3795)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3795)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3795)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3795)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3795)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3795)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3795)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3795)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3795)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3810)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3810)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3810)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3810)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3810)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3810)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3810)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3810)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3810)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3810)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3810)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3810)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3808)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3808)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3808)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3808)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3808)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3808)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3808)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3808)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3808)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3808)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3808)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3808)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3796)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3796)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3796)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3796)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3796)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3796)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3796)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3796)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3796)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3796)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3796)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3796)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3801)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3801)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3801)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3801)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3801)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3801)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3801)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3801)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3801)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3801)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3801)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3801)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3803)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3803)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3803)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3803)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3803)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3803)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3803)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3803)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3803)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3803)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3803)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3803)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3804)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3804)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3804)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3804)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3804)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3804)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3804)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3804)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3804)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3804)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3804)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3804)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3797)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3797)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3797)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3797)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3797)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3797)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3797)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3797)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3797)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3797)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3797)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3797)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3806)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3806)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3806)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3806)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3806)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3806)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3806)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3806)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3806)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3806)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3806)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3806)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3807)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3807)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3807)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3807)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3807)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3807)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3807)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3807)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3807)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3807)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3807)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3807)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3802)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3802)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3802)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3802)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3802)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3802)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3802)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3802)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3802)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3802)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3802)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3802)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3809)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3809)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3809)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3809)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3809)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3809)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3809)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3809)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3809)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3809)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3809)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3809)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3800)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3800)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3800)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3800)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3800)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3800)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3800)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3800)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3800)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3800)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3800)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3800)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3799)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3799)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3799)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3799)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3799)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3799)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3799)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3799)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3799)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3799)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3799)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3799)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3805)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3805)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3805)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3805)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3805)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3805)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3805)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3805)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3805)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3805)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=3805)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=3805)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-47-38\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 56\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 4.433424472808838\n",
      "    update_ratio: 0.8595952987670898\n",
      "    weights_norm: 0.000970443245023489\n",
      "    weights_std: 0.0029814280569553375\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 68.56666666666668\n",
      "    gpu_util_percent0: 0.01\n",
      "    ram_util_percent: 56.21666666666667\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 4.189875364303589\n",
      "  time_this_iter_s: 4.189875364303589\n",
      "  time_total_s: 4.189875364303589\n",
      "  timestamp: 1573076858\n",
      "  timesteps_since_restore: 3679\n",
      "  timesteps_this_iter: 3679\n",
      "  timesteps_total: 3679\n",
      "  training_iteration: 1\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 4 s, 1 iter, 3679 ts, nan rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   out=out, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-47-43\n",
      "  done: false\n",
      "  episode_len_mean: 147.0\n",
      "  episode_reward_mean: 235.87399037679037\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 616\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 12.06135368347168\n",
      "    update_ratio: 0.2905277907848358\n",
      "    weights_norm: 0.018113937228918076\n",
      "    weights_std: 0.012060892768204212\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 66.9\n",
      "    gpu_util_percent0: 0.01\n",
      "    ram_util_percent: 59.6\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 9.439728498458862\n",
      "  time_this_iter_s: 0.527585506439209\n",
      "  time_total_s: 9.439728498458862\n",
      "  timestamp: 1573076863\n",
      "  timesteps_since_restore: 58789\n",
      "  timesteps_this_iter: 6757\n",
      "  timesteps_total: 58789\n",
      "  training_iteration: 11\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 9 s, 11 iter, 58789 ts, 236 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-47-48\n",
      "  done: false\n",
      "  episode_len_mean: 134.0\n",
      "  episode_reward_mean: 257.35765228271487\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 1064\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 5.863678932189941\n",
      "    update_ratio: 0.13509181141853333\n",
      "    weights_norm: 0.03259904682636261\n",
      "    weights_std: 0.016117971390485764\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 69.4\n",
      "    gpu_util_percent0: 0.01\n",
      "    ram_util_percent: 59.3\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 14.473055124282837\n",
      "  time_this_iter_s: 0.6768283843994141\n",
      "  time_total_s: 14.473055124282837\n",
      "  timestamp: 1573076868\n",
      "  timesteps_since_restore: 124505\n",
      "  timesteps_this_iter: 8889\n",
      "  timesteps_total: 124505\n",
      "  training_iteration: 19\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 14 s, 19 iter, 124505 ts, 257 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-47-53\n",
      "  done: false\n",
      "  episode_len_mean: 142.0\n",
      "  episode_reward_mean: 256.23000310262046\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 1512\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 9.370576858520508\n",
      "    update_ratio: 0.13332854211330414\n",
      "    weights_norm: 0.054459553211927414\n",
      "    weights_std: 0.020899374037981033\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 71.1\n",
      "    gpu_util_percent0: 0.01\n",
      "    ram_util_percent: 59.2\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 19.671809196472168\n",
      "  time_this_iter_s: 0.746530294418335\n",
      "  time_total_s: 19.671809196472168\n",
      "  timestamp: 1573076873\n",
      "  timesteps_since_restore: 192253\n",
      "  timesteps_this_iter: 8934\n",
      "  timesteps_total: 192253\n",
      "  training_iteration: 27\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 19 s, 27 iter, 192253 ts, 256 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-47-59\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_mean: 260.48679580688474\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 1792\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 8.356194496154785\n",
      "    update_ratio: 0.11585142463445663\n",
      "    weights_norm: 0.0630381852388382\n",
      "    weights_std: 0.022690337151288986\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 47.9\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 59.0\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 25.154531478881836\n",
      "  time_this_iter_s: 1.4001057147979736\n",
      "  time_total_s: 25.154531478881836\n",
      "  timestamp: 1573076879\n",
      "  timesteps_since_restore: 251540\n",
      "  timesteps_this_iter: 14040\n",
      "  timesteps_total: 251540\n",
      "  training_iteration: 32\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 25 s, 32 iter, 251540 ts, 260 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-48-06\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_mean: 268.8959465026855\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 2016\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 12.871139526367188\n",
      "    update_ratio: 0.13660843670368195\n",
      "    weights_norm: 0.07297901064157486\n",
      "    weights_std: 0.024930167943239212\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 67.76666666666667\n",
      "    gpu_util_percent0: 0.04\n",
      "    ram_util_percent: 58.96666666666667\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 32.17485570907593\n",
      "  time_this_iter_s: 2.2077207565307617\n",
      "  time_total_s: 32.17485570907593\n",
      "  timestamp: 1573076886\n",
      "  timesteps_since_restore: 337928\n",
      "  timesteps_this_iter: 27695\n",
      "  timesteps_total: 337928\n",
      "  training_iteration: 36\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 32 s, 36 iter, 337928 ts, 269 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-48-11\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_mean: 416.7290412902832\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 2128\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 7.6695990562438965\n",
      "    update_ratio: 0.0988566055893898\n",
      "    weights_norm: 0.0817597284913063\n",
      "    weights_std: 0.026421990245580673\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 64.025\n",
      "    gpu_util_percent0: 0.05\n",
      "    ram_util_percent: 59.0\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 37.64621901512146\n",
      "  time_this_iter_s: 2.8312904834747314\n",
      "  time_total_s: 37.64621901512146\n",
      "  timestamp: 1573076891\n",
      "  timesteps_since_restore: 407793\n",
      "  timesteps_this_iter: 38288\n",
      "  timesteps_total: 407793\n",
      "  training_iteration: 38\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.2/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 37 s, 38 iter, 407793 ts, 417 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-48-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_mean: 490.7888023376465\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 2240\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 6.381701946258545\n",
      "    update_ratio: 0.08652044087648392\n",
      "    weights_norm: 0.08751143515110016\n",
      "    weights_std: 0.027433788403868675\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 62.239999999999995\n",
      "    gpu_util_percent0: 0.12999999999999998\n",
      "    ram_util_percent: 59.0\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 44.48006510734558\n",
      "  time_this_iter_s: 3.8187315464019775\n",
      "  time_total_s: 44.48006510734558\n",
      "  timestamp: 1573076898\n",
      "  timesteps_since_restore: 496125\n",
      "  timesteps_this_iter: 42966\n",
      "  timesteps_total: 496125\n",
      "  training_iteration: 40\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 44 s, 40 iter, 496125 ts, 491 rew\n",
      "\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-48-25\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_mean: 564.844164276123\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 2352\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 12.289180755615234\n",
      "    update_ratio: 0.11627662926912308\n",
      "    weights_norm: 0.09490855038166046\n",
      "    weights_std: 0.02849319577217102\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 74.4\n",
      "    gpu_util_percent0: 0.135\n",
      "    ram_util_percent: 59.65\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 51.2081241607666\n",
      "  time_this_iter_s: 3.3343868255615234\n",
      "  time_total_s: 51.2081241607666\n",
      "  timestamp: 1573076905\n",
      "  timesteps_since_restore: 587049\n",
      "  timesteps_this_iter: 45602\n",
      "  timesteps_total: 587049\n",
      "  training_iteration: 42\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 51 s, 42 iter, 587049 ts, 565 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-48-32\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_mean: 712.2532623291015\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 2464\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 11.710687637329102\n",
      "    update_ratio: 0.10948748141527176\n",
      "    weights_norm: 0.10054796189069748\n",
      "    weights_std: 0.029489120468497276\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 75.84\n",
      "    gpu_util_percent0: 0.015999999999999997\n",
      "    ram_util_percent: 59.379999999999995\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 58.0736780166626\n",
      "  time_this_iter_s: 3.2495381832122803\n",
      "  time_total_s: 58.0736780166626\n",
      "  timestamp: 1573076912\n",
      "  timesteps_since_restore: 683401\n",
      "  timesteps_this_iter: 47416\n",
      "  timesteps_total: 683401\n",
      "  training_iteration: 44\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 58 s, 44 iter, 683401 ts, 712 rew\n",
      "\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-48-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_mean: 857.7398493448894\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 2576\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 10.294916152954102\n",
      "    update_ratio: 0.09837035089731216\n",
      "    weights_norm: 0.11258545517921448\n",
      "    weights_std: 0.031262774020433426\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 68.14\n",
      "    gpu_util_percent0: 0.008\n",
      "    ram_util_percent: 59.1\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 65.29834318161011\n",
      "  time_this_iter_s: 3.4545137882232666\n",
      "  time_total_s: 65.29834318161011\n",
      "  timestamp: 1573076919\n",
      "  timesteps_since_restore: 780489\n",
      "  timesteps_this_iter: 44776\n",
      "  timesteps_total: 780489\n",
      "  training_iteration: 46\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 65 s, 46 iter, 780489 ts, 858 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-48-46\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_mean: 928.986848449707\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 2688\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 9.082413673400879\n",
      "    update_ratio: 0.0892644003033638\n",
      "    weights_norm: 0.1155020073056221\n",
      "    weights_std: 0.03184479847550392\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 71.2\n",
      "    gpu_util_percent0: 0.021666666666666667\n",
      "    ram_util_percent: 59.35\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 72.49820446968079\n",
      "  time_this_iter_s: 3.948939561843872\n",
      "  time_total_s: 72.49820446968079\n",
      "  timestamp: 1573076926\n",
      "  timesteps_since_restore: 880059\n",
      "  timesteps_this_iter: 51919\n",
      "  timesteps_total: 880059\n",
      "  training_iteration: 48\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 72 s, 48 iter, 880059 ts, 929 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-48-53\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_mean: 928.986848449707\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 2800\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 7.377631187438965\n",
      "    update_ratio: 0.07958327233791351\n",
      "    weights_norm: 0.11896730214357376\n",
      "    weights_std: 0.03225873410701752\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 83.97500000000001\n",
      "    gpu_util_percent0: 0.01\n",
      "    ram_util_percent: 59.675000000000004\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 79.48622989654541\n",
      "  time_this_iter_s: 3.4463980197906494\n",
      "  time_total_s: 79.48622989654541\n",
      "  timestamp: 1573076933\n",
      "  timesteps_since_restore: 986669\n",
      "  timesteps_this_iter: 54700\n",
      "  timesteps_total: 986669\n",
      "  training_iteration: 50\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.3/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 79 s, 50 iter, 986669 ts, 929 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-49-01\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_mean: 994.7611724853516\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 2912\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 7.11041784286499\n",
      "    update_ratio: 0.07659531384706497\n",
      "    weights_norm: 0.12377060204744339\n",
      "    weights_std: 0.03292367234826088\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 82.8\n",
      "    gpu_util_percent0: 0.096\n",
      "    ram_util_percent: 59.96\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 87.16011953353882\n",
      "  time_this_iter_s: 3.5354013442993164\n",
      "  time_total_s: 87.16011953353882\n",
      "  timestamp: 1573076941\n",
      "  timesteps_since_restore: 1095574\n",
      "  timesteps_this_iter: 55143\n",
      "  timesteps_total: 1095574\n",
      "  training_iteration: 52\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 87 s, 52 iter, 1095574 ts, 995 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-49-09\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_mean: 994.8370391845704\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 3024\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 8.253084182739258\n",
      "    update_ratio: 0.08048346638679504\n",
      "    weights_norm: 0.13139179348945618\n",
      "    weights_std: 0.03398692235350609\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 69.75\n",
      "    gpu_util_percent0: 0.08333333333333336\n",
      "    ram_util_percent: 60.166666666666664\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 94.86082339286804\n",
      "  time_this_iter_s: 3.9039430618286133\n",
      "  time_total_s: 94.86082339286804\n",
      "  timestamp: 1573076949\n",
      "  timesteps_since_restore: 1204333\n",
      "  timesteps_this_iter: 54497\n",
      "  timesteps_total: 1204333\n",
      "  training_iteration: 54\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 94 s, 54 iter, 1204333 ts, 995 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-49-17\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_mean: 994.9011474609375\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 3136\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 7.69484806060791\n",
      "    update_ratio: 0.07642999291419983\n",
      "    weights_norm: 0.13475145399570465\n",
      "    weights_std: 0.034488365054130554\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 72.34285714285714\n",
      "    gpu_util_percent0: 0.038571428571428576\n",
      "    ram_util_percent: 60.21428571428571\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 102.86238312721252\n",
      "  time_this_iter_s: 4.440135478973389\n",
      "  time_total_s: 102.86238312721252\n",
      "  timestamp: 1573076957\n",
      "  timesteps_since_restore: 1312084\n",
      "  timesteps_this_iter: 53500\n",
      "  timesteps_total: 1312084\n",
      "  training_iteration: 56\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 102 s, 56 iter, 1312084 ts, 995 rew\n",
      "\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-49-24\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_mean: 994.9380859375\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 3248\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 10.783525466918945\n",
      "    update_ratio: 0.08913681656122208\n",
      "    weights_norm: 0.1363988220691681\n",
      "    weights_std: 0.03480809926986694\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 80.22\n",
      "    gpu_util_percent0: 0.032\n",
      "    ram_util_percent: 60.3\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 110.48844957351685\n",
      "  time_this_iter_s: 3.603461265563965\n",
      "  time_total_s: 110.48844957351685\n",
      "  timestamp: 1573076964\n",
      "  timesteps_since_restore: 1417782\n",
      "  timesteps_this_iter: 52383\n",
      "  timesteps_total: 1417782\n",
      "  training_iteration: 58\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.5/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 110 s, 58 iter, 1417782 ts, 995 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-49-32\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_mean: 994.8447998046875\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 3360\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 9.842838287353516\n",
      "    update_ratio: 0.08554870635271072\n",
      "    weights_norm: 0.13648894429206848\n",
      "    weights_std: 0.03494599461555481\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 82.9\n",
      "    gpu_util_percent0: 0.01\n",
      "    ram_util_percent: 60.32000000000001\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 117.86971783638\n",
      "  time_this_iter_s: 3.47053861618042\n",
      "  time_total_s: 117.86971783638\n",
      "  timestamp: 1573076972\n",
      "  timesteps_since_restore: 1527214\n",
      "  timesteps_this_iter: 56000\n",
      "  timesteps_total: 1527214\n",
      "  training_iteration: 60\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.5/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 117 s, 60 iter, 1527214 ts, 995 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-49-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_mean: 995.0714172363281\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 3472\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 7.504450798034668\n",
      "    update_ratio: 0.07353977113962173\n",
      "    weights_norm: 0.13784946501255035\n",
      "    weights_std: 0.03517348691821098\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 75.85999999999999\n",
      "    gpu_util_percent0: 0.02\n",
      "    ram_util_percent: 60.21999999999999\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 125.57223057746887\n",
      "  time_this_iter_s: 3.79689884185791\n",
      "  time_total_s: 125.57223057746887\n",
      "  timestamp: 1573076979\n",
      "  timesteps_since_restore: 1638285\n",
      "  timesteps_this_iter: 56000\n",
      "  timesteps_total: 1638285\n",
      "  training_iteration: 62\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 125 s, 62 iter, 1638285 ts, 995 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-49-47\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_mean: 995.034701538086\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 3584\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 7.10622501373291\n",
      "    update_ratio: 0.07146120071411133\n",
      "    weights_norm: 0.13867710530757904\n",
      "    weights_std: 0.03541344404220581\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 76.7\n",
      "    gpu_util_percent0: 0.004\n",
      "    ram_util_percent: 59.9\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 132.8106255531311\n",
      "  time_this_iter_s: 3.8737876415252686\n",
      "  time_total_s: 132.8106255531311\n",
      "  timestamp: 1573076987\n",
      "  timesteps_since_restore: 1747365\n",
      "  timesteps_this_iter: 54624\n",
      "  timesteps_total: 1747365\n",
      "  training_iteration: 64\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 132 s, 64 iter, 1747365 ts, 995 rew\n",
      "\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-49-55\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_mean: 995.3017730712891\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 3696\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 11.874409675598145\n",
      "    update_ratio: 0.09050846099853516\n",
      "    weights_norm: 0.15085239708423615\n",
      "    weights_std: 0.03689726069569588\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 75.1\n",
      "    gpu_util_percent0: 0.052000000000000005\n",
      "    ram_util_percent: 60.0\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 140.90836715698242\n",
      "  time_this_iter_s: 3.9182634353637695\n",
      "  time_total_s: 140.90836715698242\n",
      "  timestamp: 1573076995\n",
      "  timesteps_since_restore: 1856927\n",
      "  timesteps_this_iter: 56000\n",
      "  timesteps_total: 1856927\n",
      "  training_iteration: 66\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 140 s, 66 iter, 1856927 ts, 995 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-50-02\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_mean: 995.3017730712891\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 3808\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 8.183688163757324\n",
      "    update_ratio: 0.07328919321298599\n",
      "    weights_norm: 0.15361274778842926\n",
      "    weights_std: 0.03735161945223808\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 83.12\n",
      "    gpu_util_percent0: 0.072\n",
      "    ram_util_percent: 59.9\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 147.90420794487\n",
      "  time_this_iter_s: 3.5094311237335205\n",
      "  time_total_s: 147.90420794487\n",
      "  timestamp: 1573077002\n",
      "  timesteps_since_restore: 1967815\n",
      "  timesteps_this_iter: 56000\n",
      "  timesteps_total: 1967815\n",
      "  training_iteration: 68\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 15/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tRUNNING, [15 CPUs, 0 GPUs], [pid=3798], 147 s, 68 iter, 1967815 ts, 995 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/ray/rllib/agents/ars/ars.py:289: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   episode_len_mean=eval_lengths.mean(),\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m /home/sgillen/miniconda3/envs/ray/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=3798)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 13:50:06,252\tINFO tune.py:274 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n",
      "2019-11-06 13:50:06,255\tWARNING experiment_analysis.py:34 -- pandas not installed. Run `pip install pandas` for Analysis utilities.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for ARS_Walker2d-v3_0_seed=1:\n",
      "  date: 2019-11-06_13-50-06\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_mean: 995.4634368896484\n",
      "  experiment_id: 77c6d434963a43d5b139012b9cde7644\n",
      "  hostname: sgdt\n",
      "  info:\n",
      "    episodes_so_far: 3864\n",
      "    episodes_this_iter: 56\n",
      "    grad_norm: 9.00768756866455\n",
      "    update_ratio: 0.07657606154680252\n",
      "    weights_norm: 0.155051127076149\n",
      "    weights_std: 0.03750545158982277\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.12\n",
      "  perf:\n",
      "    cpu_util_percent: 81.92\n",
      "    gpu_util_percent0: 0.008\n",
      "    ram_util_percent: 59.9\n",
      "    vram_util_percent0: 0.12757201646090535\n",
      "  pid: 3798\n",
      "  time_since_restore: 151.83171033859253\n",
      "  time_this_iter_s: 3.927502393722534\n",
      "  time_total_s: 151.83171033859253\n",
      "  timestamp: 1573077006\n",
      "  timesteps_since_restore: 2023176\n",
      "  timesteps_this_iter: 55361\n",
      "  timesteps_total: 2023176\n",
      "  training_iteration: 69\n",
      "  trial_id: 04919da6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/5.13 GiB heap, 0.0/1.76 GiB objects\n",
      "Memory usage on this node: 9.4/15.7 GiB\n",
      "Result logdir: /home/sgillen/ray_results/ARS\n",
      "Number of trials: 1 ({'TERMINATED': 1})\n",
      "TERMINATED trials:\n",
      " - ARS_Walker2d-v3_0_seed=1:\tTERMINATED, [15 CPUs, 0 GPUs], [pid=3798], 151 s, 69 iter, 2023176 ts, 995 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "import ray.rllib.agents.ars as ars\n",
    "import pybullet_envs\n",
    "\n",
    "\n",
    "config = ars.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"num_workers\"] = 14\n",
    "config[\"eager\"] = False\n",
    "config[\"model\"][\"fcnet_hiddens\"] = []\n",
    "config[\"lr\"] = 5e-5\n",
    "config[\"env\"] = \"Walker2d-v3\"\n",
    "#config[\"env\"] = \"Walker2DBulletEnv-v0\"\n",
    "\n",
    "config['seed'] = tune.grid_search([1])\n",
    "\n",
    "ray.init()\n",
    "analysis = tune.run(\n",
    "    \"ARS\",\n",
    "    config = config,\n",
    "    stop={\"timesteps_total\": 2e6},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ray(3.6)",
   "language": "python",
   "name": "ray"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
